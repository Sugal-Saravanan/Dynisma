{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3404bdda-dc65-478f-9e9c-4846c27e7f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No --config provided or not found. Using in-memory default config.\n",
      "ðŸ”§ Creating residuals...\n",
      "â†©ï¸  Skip residual (exists): Dataset01_Ski_CrossbeamYawNotPerforming_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset02_Matrix_Rocker4EncoderNotWorking_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset03_Wushu_YawTrapezoidNormal_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset04_Wushu_YawWaveletSqueak_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset05_Wushu_LaneChanges_ModelBump_residual.csv\n",
      "âŒ Failed to read Dataset07_Demo_Spa_GT.csv: No columns to parse from file\n",
      "â†©ï¸  Skip residual (exists): Dataset08_Demo_Jiggler_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset09_Demo_VerticalChirp_residual.csv\n",
      "âŒ Failed to read Dataset10_Demo_MillbrookHills.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:391: RuntimeWarning: invalid value encountered in divide\n",
      "  out = np.where(den > 0, num / den, np.nan)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = hybrid\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_is_anomaly\"] = hlabels.astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_thr\"] = hthr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:405: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:406: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:407: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # NEW: plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset01_Ski_CrossbeamYawNotPerforming_residual] iso=6081 | ae=12194 | lof=1724 | lstm=213 | hyb=18005 | vote3+=217 | any=14514\n",
      "âŒ Skipped Dataset02_Matrix_Rocker4EncoderNotWorking_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:391: RuntimeWarning: invalid value encountered in divide\n",
      "  out = np.where(den > 0, num / den, np.nan)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = hybrid\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_is_anomaly\"] = hlabels.astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_thr\"] = hthr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:405: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:406: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:407: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # NEW: plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset03_Wushu_YawTrapezoidNormal_residual] iso=4353 | ae=16691 | lof=2735 | lstm=141 | hyb=32491 | vote3+=363 | any=19849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:391: RuntimeWarning: invalid value encountered in divide\n",
      "  out = np.where(den > 0, num / den, np.nan)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = hybrid\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_is_anomaly\"] = hlabels.astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_thr\"] = hthr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:405: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:406: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:407: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # NEW: plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset04_Wushu_YawWaveletSqueak_residual] iso=5912 | ae=5593 | lof=1179 | lstm=722 | hyb=7239 | vote3+=655 | any=7004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:391: RuntimeWarning: invalid value encountered in divide\n",
      "  out = np.where(den > 0, num / den, np.nan)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = hybrid\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_is_anomaly\"] = hlabels.astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"hybrid_thr\"] = hthr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:405: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:406: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:407: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # NEW: plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\3673355693.py:1217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset05_Wushu_LaneChanges_ModelBump_residual] iso=7951 | ae=7923 | lof=1177 | lstm=335 | hyb=11666 | vote3+=118 | any=11004\n",
      "âŒ Skipped Dataset08_Demo_Jiggler_residual: No residuals found.\n",
      "âŒ Skipped Dataset09_Demo_VerticalChirp_residual: No residuals found.\n",
      "âœ… Saved row-level: ./Anomaly_detection/code/outputs/combined_anomaly_results.csv\n",
      "âœ… Saved summary:   ./Anomaly_detection/code/outputs/model_comparison_summary.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_plot.png\n",
      "âœ… Saved episodes+reason: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons.csv\n",
      "âœ… Saved episodes+scores: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons_and_scores.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset01_Ski_CrossbeamYawNotPerforming_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset03_Wushu_YawTrapezoidNormal_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset04_Wushu_YawWaveletSqueak_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset05_Wushu_LaneChanges_ModelBump_residual.png\n",
      "âœ… Saved sensor table: ./Anomaly_detection/code/outputs/voted_outputs\\sensor_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Ops report saved: ./Anomaly_detection/code/outputs/ops_report.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Anomaly Detection Product (single script) â€” Extended:\n",
    "- Residual creation (Demand - Measured -> Residual)\n",
    "- Feature engineering (reuse if already present)\n",
    "- Scaling once -> shared across models\n",
    "- Models: IsolationForest, LOF, Dense AE, LSTM AE\n",
    "- Dynamic thresholds (MAD) for all scores\n",
    "- Voting (3+) + episodes (merged runs)\n",
    "- NEW: Plain voting bar (vote_any = >=1 model)\n",
    "- NEW: Hybrid scoring (weighted + robust-normalized score + MAD threshold)\n",
    "- Episode explanations (primary signal, suspected sensor)\n",
    "- Hardware mapping & root-cause scoring (lag/saturation/drift/vibe)\n",
    "- Sensor ranking, clustering & heatmap\n",
    "- Multi-page PDF Ops Report (now includes Hybrid + Plain voting bars)\n",
    "- Config-driven (JSON) OR safe defaults (no args)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utils\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in str(name))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Residual creation (optional)\n",
    "# =========================================================\n",
    "def create_residuals_for_folder(\n",
    "    in_folder: str,\n",
    "    out_folder: str,\n",
    "    demand_token: str = \"Demand\",\n",
    "    measured_token: str = \"Measured\",\n",
    "    residual_token: str = \"Residual\",\n",
    "    skip_if_exists: bool = True,\n",
    "    suffix: str = \"_residual\",\n",
    "    logger=print,\n",
    ") -> None:\n",
    "    ensure_dir(out_folder)\n",
    "    for file in os.listdir(in_folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(in_folder, file)\n",
    "        out_name = file.replace(\".csv\", f\"{suffix}.csv\")  # fixed\n",
    "        out_path = os.path.join(out_folder, out_name)\n",
    "\n",
    "        if skip_if_exists and os.path.exists(out_path):\n",
    "            logger(f\"â†©ï¸  Skip residual (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(in_path)\n",
    "        except Exception as e:\n",
    "            logger(f\"âŒ Failed to read {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        made_any = False\n",
    "        for col in cols:\n",
    "            if demand_token in col:\n",
    "                measured_col = col.replace(demand_token, measured_token)\n",
    "                if measured_col in df.columns:\n",
    "                    residual_col = col.replace(demand_token, residual_token)\n",
    "                    df[residual_col] = df[col] - df[measured_col]\n",
    "                    made_any = True\n",
    "\n",
    "        if not made_any:\n",
    "            logger(f\"âš ï¸  No Demand/Measured pairs found in {file}.\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        logger(f\"âœ… Residual CSV saved: {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Scaling + robust threshold (MAD)\n",
    "# =========================================================\n",
    "def scale_features(X: pd.DataFrame, use_float32: bool = True):\n",
    "    \"\"\"\n",
    "    Standardize features once and share across models.\n",
    "    Returns (scaler, X_scaled np.array, X_tensor torch.tensor)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    if use_float32:\n",
    "        X_scaled = X_scaled.astype(\"float32\")\n",
    "    X_tensor = torch.from_numpy(X_scaled)\n",
    "    return scaler, X_scaled, X_tensor\n",
    "\n",
    "\n",
    "def robust_threshold(\n",
    "    values: np.ndarray,\n",
    "    k: float = 3.5,\n",
    "    tail: str = \"high\",\n",
    "    min_anoms: int = 5,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    MAD-based threshold: median Â± k * 1.4826 * MAD\n",
    "    tail = 'high' (right tail) or 'low' (left tail)\n",
    "    Returns: (threshold, labels) labels aligned to 'values' (1=anomaly)\n",
    "    \"\"\"\n",
    "    v = np.asarray(values)\n",
    "    mask = ~np.isnan(v)\n",
    "    v = v[mask]\n",
    "    if v.size == 0:\n",
    "        return (np.inf if tail == \"high\" else -np.inf), np.zeros_like(values, dtype=int)\n",
    "\n",
    "    med = np.median(v)\n",
    "    mad = np.median(np.abs(v - med)) + 1e-12\n",
    "    if tail == \"high\":\n",
    "        thr = med + k * 1.4826 * mad\n",
    "        labels = (values > thr).astype(int)\n",
    "    else:\n",
    "        thr = med - k * 1.4826 * mad\n",
    "        labels = (values < thr).astype(int)\n",
    "\n",
    "    # relax if too strict on large arrays\n",
    "    if labels.sum() < min_anoms and v.size >= 100:\n",
    "        for k_relax in (3.0, 2.5, 2.0):\n",
    "            if tail == \"high\":\n",
    "                thr = med + k_relax * 1.4826 * mad\n",
    "                labels = (values > thr).astype(int)\n",
    "            else:\n",
    "                thr = med - k_relax * 1.4826 * mad\n",
    "                labels = (values < thr).astype(int)\n",
    "            if labels.sum() >= min_anoms:\n",
    "                break\n",
    "\n",
    "    return thr, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Feature Engineering\n",
    "# =========================================================\n",
    "def prepare_features(\n",
    "    df: pd.DataFrame,\n",
    "    residual_cols: List[str],\n",
    "    window: int = 5,\n",
    "    max_features: int = 500,\n",
    "    logger=print,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Create or reuse features: residual, delta, rolling mean/std\n",
    "    Returns: X, feature_cols, stats (reused vs generated)\n",
    "    \"\"\"\n",
    "    already_done = any(f\"{residual_cols[0]}_delta\" in df.columns for _ in residual_cols)\n",
    "    stats = {\"reused\": 0, \"generated\": 0}\n",
    "\n",
    "    if already_done:\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if any(k in c for k in [\"Residual\", \"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "        ]\n",
    "        X = df[feature_cols].dropna()\n",
    "        stats[\"reused\"] = len(feature_cols)\n",
    "        logger(f\"ðŸ” Reusing {len(feature_cols)} engineered features.\")\n",
    "        return X, feature_cols, stats\n",
    "\n",
    "    # Generate\n",
    "    for col in residual_cols:\n",
    "        df[f\"{col}_delta\"] = df[col].diff()\n",
    "        df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
    "        df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in residual_cols:\n",
    "        feature_cols += [\n",
    "            col,\n",
    "            f\"{col}_delta\",\n",
    "            f\"{col}_rolling_mean_{window}\",\n",
    "            f\"{col}_rolling_std_{window}\",\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].dropna()\n",
    "    stats[\"generated\"] = len(feature_cols)\n",
    "    logger(f\"ðŸ› ï¸  Generated {len(feature_cols)} features (window={window}).\")\n",
    "\n",
    "    if X.shape[1] > max_features:\n",
    "        logger(f\"âŒ Too many features ({X.shape[1]} > {max_features}). Skipping file.\")\n",
    "        return pd.DataFrame(), [], stats\n",
    "\n",
    "    return X, feature_cols, stats\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Models\n",
    "# =========================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 32), nn.ReLU(), nn.Linear(32, 8))\n",
    "        self.decoder = nn.Sequential(nn.Linear(8, 32), nn.ReLU(), nn.Linear(32, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def dense_autoencoder_detect(\n",
    "    X_tensor: torch.Tensor, k: float, ae_epochs: int, ae_lr: float\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    model = Autoencoder(X_tensor.shape[1])\n",
    "    opt = optim.Adam(model.parameters(), lr=ae_lr)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    for _ in range(ae_epochs):\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = crit(out, X_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec = model(X_tensor)\n",
    "        errors = torch.mean((X_tensor - rec) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "    return labels.astype(int), errors, thr\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.encoder(x)  # [1, B, H]\n",
    "        repeated = h.repeat(x.size(1), 1, 1).transpose(0, 1)  # [B, T, H]\n",
    "        decoded, _ = self.decoder(repeated)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def make_sequences(X: np.ndarray, seq_len: int) -> Tuple[np.ndarray, List[int]]:\n",
    "    seqs, idxs = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seqs.append(X[i:i+seq_len])\n",
    "        idxs.append(i + seq_len - 1)\n",
    "    return np.array(sesqs := seqs), idxs  # keep identical shape to previous code; sesqs to avoid linter\n",
    "\n",
    "\n",
    "def lstm_autoencoder_detect(\n",
    "    X_scaled: np.ndarray,\n",
    "    k: float,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int,\n",
    "    patience: int,\n",
    "    max_sequences: int,\n",
    "    downsample: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[int], float]:\n",
    "    try:\n",
    "        Xds = X_scaled[::downsample]\n",
    "        if len(Xds) < seq_len:\n",
    "            return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "        Xseq, idxs = make_sequences(Xds, seq_len)\n",
    "        if len(Xseq) > max_sequences:\n",
    "            Xseq, idxs = Xseq[:max_sequences], idxs[:max_sequences]\n",
    "\n",
    "        Xt = torch.tensor(Xseq, dtype=torch.float32)\n",
    "        model = LSTMAutoencoder(Xt.shape[2], hidden_dim)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best, wait = float(\"inf\"), 0\n",
    "        for _ in range(100):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(Xt)\n",
    "            loss = crit(out, Xt)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if loss.item() < best:\n",
    "                best, wait = loss.item(), 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(Xt)\n",
    "            errors = torch.mean((Xt - out) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "        thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "        return labels.astype(int), errors, idxs, thr\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ LSTM memory error: {e}\")\n",
    "        return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "\n",
    "def isolation_forest_detect(X_scaled: np.ndarray, k: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    iso = IsolationForest(contamination=\"auto\", n_estimators=300, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "def lof_detect(X_scaled: np.ndarray, k: float, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=\"auto\")\n",
    "    _ = lof.fit_predict(X_scaled)  # populates negative_outlier_factor_\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# NEW: Hybrid scoring utilities\n",
    "# =========================================================\n",
    "def _robust_z_pos(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Right-tail robust z-score (>=0 when above median).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    z = np.where(np.isnan(z), np.nan, z)\n",
    "    return np.maximum(z, 0.0)  # only right tail counts as anomalous\n",
    "\n",
    "\n",
    "def _percentile01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map to [0,1] by robust percentiles (2â€“98). Values outside clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    lo = np.nanpercentile(x, 2)\n",
    "    hi = np.nanpercentile(x, 98)\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    y = (x - lo) / rng\n",
    "    return np.clip(y, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hybrid_score(df: pd.DataFrame, cfg: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Combine model scores into a single 'hybrid_score' using robust normalization\n",
    "    and weights. Higher = more anomalous.\n",
    "    \"\"\"\n",
    "    if not cfg.get(\"hybrid\", {}).get(\"enabled\", False):\n",
    "        return np.full(len(df), np.nan)\n",
    "\n",
    "    wmap = cfg[\"hybrid\"][\"weights\"]\n",
    "    method = cfg[\"hybrid\"][\"method\"]\n",
    "\n",
    "    # Which score columns to use (must be \"higher = more anomalous\" already)\n",
    "    components = [c for c in [\"iso_score\", \"lof_score\", \"ae_error\", \"lstm_error\"] if c in df.columns and c in wmap]\n",
    "    if not components:\n",
    "        return np.full(len(df), np.nan)\n",
    "\n",
    "    normers = []\n",
    "    for c in components:\n",
    "        arr = df[c].to_numpy()\n",
    "        if method == \"robust_z\":\n",
    "            norm = _robust_z_pos(arr)\n",
    "            norm = np.clip(norm, 0, 10.0) / 10.0  # compress tails, map ~[0,1]\n",
    "        else:  # \"percentile\"\n",
    "            norm = _percentile01(arr)\n",
    "        normers.append((c, norm, float(wmap.get(c, 0.0))))\n",
    "\n",
    "    # Weighted average ignoring NaNs\n",
    "    num = np.zeros(len(df), dtype=float)\n",
    "    den = np.zeros(len(df), dtype=float)\n",
    "    for _, norm, w in normers:\n",
    "        m = ~np.isnan(norm)\n",
    "        num[m] += w * norm[m]\n",
    "        den[m] += w\n",
    "    out = np.where(den > 0, num / den, np.nan)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Voting, episodes, explanations\n",
    "# =========================================================\n",
    "def generate_votes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"agreement_all_4\"] = (\n",
    "        (df.get(\"ae_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lof_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lstm_is_anomaly\", 0) == 1)\n",
    "    ).astype(int)\n",
    "    df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
    "    df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
    "    df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # NEW: plain voting (>=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_voted_rows(df: pd.DataFrame, rule: str = \"vote_3plus\") -> pd.DataFrame:\n",
    "    if rule == \"vote_3plus\":\n",
    "        mask = df[\"vote_3plus\"] == 1\n",
    "    elif rule == \"agreement_all_4\":\n",
    "        mask = df[\"agreement_all_4\"] == 1\n",
    "    elif rule == \"any\":\n",
    "        mask = (\n",
    "            (df[\"ae_is_anomaly\"] == 1)\n",
    "            | (df[\"is_anomaly\"] == 1)\n",
    "            | (df[\"lof_is_anomaly\"] == 1)\n",
    "            | (df[\"lstm_is_anomaly\"] == 1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown rule: {rule}\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _group_runs(idxs: np.ndarray, min_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "    runs, start, prev = [], int(idxs[0]), int(idxs[0])\n",
    "    for i in idxs[1:]:\n",
    "        if int(i) - prev <= min_gap:\n",
    "            prev = int(i)\n",
    "            continue\n",
    "        runs.append((start, prev))\n",
    "        start = int(i); prev = int(i)\n",
    "    runs.append((start, prev))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def summarize_episodes(voted_df: pd.DataFrame, min_gap: int = 1) -> pd.DataFrame:\n",
    "    if voted_df.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\", \"start_idx\", \"end_idx\", \"length\", \"n_models_mean\"])\n",
    "\n",
    "    idxs = voted_df.index.to_numpy()\n",
    "    runs = _group_runs(idxs, min_gap=min_gap)\n",
    "\n",
    "    rows = []\n",
    "    for start, end in runs:\n",
    "        chunk = voted_df.loc[start:end]\n",
    "        row = {\n",
    "            \"source_file\": chunk[\"source_file\"].iloc[0] if \"source_file\" in chunk else \"\",\n",
    "            \"start_idx\": start,\n",
    "            \"end_idx\": end,\n",
    "            \"length\": int(end - start + 1),\n",
    "            \"n_models_mean\": float(\n",
    "                chunk[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1).mean()\n",
    "            ),\n",
    "        }\n",
    "        for c in [\"iso_score\", \"ae_error\", \"lof_score\", \"lstm_error\", \"hybrid_score\"]:\n",
    "            if c in chunk.columns:\n",
    "                row[f\"{c}_max\"] = float(chunk[c].max())\n",
    "                row[f\"{c}_mean\"] = float(chunk[c].mean())\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _base_residual_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def _models_string(chunk: pd.DataFrame) -> str:\n",
    "    model_cols = [c for c in [\"is_anomaly\", \"ae_is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\", \"hybrid_is_anomaly\"] if c in chunk.columns]\n",
    "    if not model_cols:\n",
    "        return \"no-model-flags\"\n",
    "    means = chunk[model_cols].mean()\n",
    "    active = [m.replace(\"_is_anomaly\", \"\").upper() for m, v in means.items() if v >= 0.5]\n",
    "    return \", \".join(active) if active else \"weak/isolated flags\"\n",
    "\n",
    "\n",
    "def attach_episode_reasons(\n",
    "    combined_df: pd.DataFrame, episodes_df: pd.DataFrame, top_k: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "\n",
    "    base_res = _base_residual_columns(combined_df)\n",
    "    if not base_res:\n",
    "        episodes_df[\"primary_signal\"] = \"\"\n",
    "        episodes_df[\"reason\"] = \"no residual columns present\"\n",
    "        episodes_df[\"suspected_sensor\"] = \"\"\n",
    "        return episodes_df\n",
    "\n",
    "    out = []\n",
    "    for _, epi in episodes_df.iterrows():\n",
    "        start, end = int(epi[\"start_idx\"]), int(epi[\"end_idx\"])\n",
    "        mask = combined_df[\"source_file\"] == epi[\"source_file\"] if \"source_file\" in combined_df.columns else slice(None)\n",
    "        chunk = combined_df.loc[mask].loc[start:end]\n",
    "\n",
    "        if chunk.empty:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"empty slice\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats = []\n",
    "        for col in base_res:\n",
    "            if col in chunk.columns:\n",
    "                stats.append((col, float(chunk[col].abs().max())))\n",
    "        if not stats:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"no residual stats\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_signal, primary_val = stats[:top_k][0]\n",
    "        models_str = _models_string(chunk)\n",
    "        measured_col = primary_signal.replace(\"Residual\", \"Measured\")\n",
    "        suspected = measured_col if (measured_col in combined_df.columns) else \"unknown-measured-sensor\"\n",
    "\n",
    "        epi[\"primary_signal\"] = primary_signal\n",
    "        epi[\"reason\"] = f\"max |{primary_signal}| = {primary_val:.3f}; models: {models_str}\"\n",
    "        epi[\"suspected_sensor\"] = suspected\n",
    "        out.append(epi)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hardware mapping + root cause scoring\n",
    "# =========================================================\n",
    "HARDWARE_MAP = [\n",
    "    (\"Force_\",         \"Actuator/LoadCell\",  \"Force didnâ€™t follow demand â†’ friction/lag/saturation/load-cell drift likely\"),\n",
    "    (\"Encoder_\",       \"Encoders/Alignment\", \"Pose/velocity mismatch â†’ quantization/missing counts/misalignment\"),\n",
    "    (\"Accelerometer_\", \"IMU/Accelerometer\",  \"Vibration bursts â†’ mounting/looseness/thermal drift\"),\n",
    "    (\"State_\",         \"Control/Timing\",     \"Requested vs achieved state diverged â†’ scheduler limits/controller windup\"),\n",
    "]\n",
    "\n",
    "def map_signal_to_hardware(primary_signal: str):\n",
    "    for needle, hw, why in HARDWARE_MAP:\n",
    "        if needle in primary_signal:\n",
    "            return hw, why\n",
    "    return \"Unknown\", \"No mapping rule matched\"\n",
    "\n",
    "\n",
    "def enrich_hardware_mapping(episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    episodes_df = episodes_df.copy()\n",
    "    episodes_df[\"hardware_class\"] = \"\"\n",
    "    episodes_df[\"hardware_why\"] = \"\"\n",
    "    for i, r in episodes_df.iterrows():\n",
    "        hw, why = map_signal_to_hardware(r.get(\"primary_signal\", \"\"))\n",
    "        episodes_df.at[i, \"hardware_class\"] = hw\n",
    "        episodes_df.at[i, \"hardware_why\"]   = why\n",
    "    return episodes_df\n",
    "\n",
    "\n",
    "def _paired_columns(primary_signal: str, cfg: dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "    resid_tok  = cfg[\"signals\"][\"residual_token\"]\n",
    "    demand_tok = cfg[\"signals\"][\"demand_token\"]\n",
    "    measured_tok = cfg[\"signals\"][\"measured_token\"]\n",
    "    if resid_tok not in primary_signal:\n",
    "        return None, None\n",
    "    demand_col   = primary_signal.replace(resid_tok, demand_tok)\n",
    "    measured_col = primary_signal.replace(resid_tok, measured_tok)\n",
    "    return demand_col, measured_col\n",
    "\n",
    "\n",
    "def _nan_ok(arr: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_correlation_lag(x: np.ndarray, y: np.ndarray, sample_rate_hz: Optional[float]) -> Tuple[float, int]:\n",
    "    x = _nan_ok(x); y = _nan_ok(y)\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return (np.nan, 0)\n",
    "    x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    x = np.nan_to_num(x);  y = np.nan_to_num(y)\n",
    "    corr = np.correlate(x, y, mode=\"full\")\n",
    "    lags = np.arange(-len(x)+1, len(x))\n",
    "    k = int(np.argmax(corr))\n",
    "    lag_samples = int(lags[k])\n",
    "    lag_seconds = lag_samples / sample_rate_hz if sample_rate_hz and sample_rate_hz > 0 else np.nan\n",
    "    return (lag_seconds, lag_samples)\n",
    "\n",
    "\n",
    "def _saturation_score(demand: np.ndarray, residual: np.ndarray, cfg: dict) -> float:\n",
    "    if len(demand) == 0 or len(residual) == 0:\n",
    "        return 0.0\n",
    "    p_dem = np.nanpercentile(demand, cfg[\"scores\"][\"saturation_pct\"])\n",
    "    p_res = np.nanpercentile(np.abs(residual), cfg[\"scores\"][\"resid_prominence_pct\"])\n",
    "    near_limit = demand >= p_dem\n",
    "    large_res  = np.abs(residual) >= p_res\n",
    "    both = np.logical_and(near_limit, large_res)\n",
    "    return float(np.nansum(both)) / max(1, len(demand))\n",
    "\n",
    "\n",
    "def _drift_score(residual: np.ndarray) -> float:\n",
    "    residual = _nan_ok(residual)\n",
    "    mu = float(np.nanmean(residual))\n",
    "    sd = float(np.nanstd(residual)) + 1e-9\n",
    "    return abs(mu) / sd\n",
    "\n",
    "\n",
    "def _vibration_score(signal: np.ndarray, sample_rate_hz: Optional[float]) -> float:\n",
    "    if not sample_rate_hz or sample_rate_hz <= 0 or len(signal) < 8:\n",
    "        return np.nan\n",
    "    sig = np.nan_to_num(signal - np.nanmean(signal))\n",
    "    fft = np.fft.rfft(sig)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1.0 / sample_rate_hz)\n",
    "    if len(freqs) == 0:\n",
    "        return np.nan\n",
    "    cutoff = 0.25 * (sample_rate_hz / 2.0)  # > Nyquist/4\n",
    "    mask_hi = freqs >= cutoff\n",
    "    num = float(np.nansum(power[mask_hi]))\n",
    "    den = float(np.nansum(power) + 1e-12)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def score_episodes(combined_df: pd.DataFrame, episodes_df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds: lag_seconds, lag_samples, saturation_score, drift_score, vibe_score\n",
    "    \"\"\"\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    out = episodes_df.copy()\n",
    "    sr = cfg[\"signals\"][\"sample_rate_hz\"]\n",
    "    min_len = cfg[\"scores\"][\"min_window_len\"]\n",
    "\n",
    "    if \"primary_signal\" not in out.columns:\n",
    "        out[\"primary_signal\"] = \"\"\n",
    "\n",
    "    for i, r in out.iterrows():\n",
    "        start, end = int(r[\"start_idx\"]), int(r[\"end_idx\"])\n",
    "        if end - start + 1 < min_len:\n",
    "            out.at[i, \"lag_seconds\"] = np.nan\n",
    "            out.at[i, \"lag_samples\"] = 0\n",
    "            out.at[i, \"saturation_score\"] = 0.0\n",
    "            out.at[i, \"drift_score\"] = 0.0\n",
    "            out.at[i, \"vibe_score\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        if \"source_file\" in combined_df.columns and \"source_file\" in out.columns and \"source_file\" in r:\n",
    "            chunk = combined_df.loc[(combined_df[\"source_file\"] == r[\"source_file\"])].loc[start:end]\n",
    "        else:\n",
    "            chunk = combined_df.loc[start:end]\n",
    "\n",
    "        primary = r.get(\"primary_signal\", \"\")\n",
    "        demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "\n",
    "        resid = chunk[primary].values if (primary in chunk.columns) else np.array([])\n",
    "        dem   = chunk[demand_col].values if (demand_col and demand_col in chunk.columns) else np.array([])\n",
    "        meas  = chunk[measured_col].values if (measured_col and measured_col in chunk.columns) else np.array([])\n",
    "\n",
    "        lag_s, lag_k = _cross_correlation_lag(dem, meas, sr) if (len(dem) and len(meas)) else (np.nan, 0)\n",
    "        sat_sc = _saturation_score(dem, resid, cfg) if (len(dem) and len(resid)) else 0.0\n",
    "        dr_sc  = _drift_score(resid) if len(resid) else 0.0\n",
    "        if \"Accelerometer_\" in primary and primary in chunk.columns:\n",
    "            vibe_sc = _vibration_score(chunk[primary].values, sr)\n",
    "        else:\n",
    "            vibe_sc = _vibration_score(resid, sr)\n",
    "\n",
    "        out.at[i, \"lag_seconds\"]       = lag_s\n",
    "        out.at[i, \"lag_samples\"]       = int(lag_k)\n",
    "        out.at[i, \"saturation_score\"]  = float(sat_sc)\n",
    "        out.at[i, \"drift_score\"]       = float(dr_sc)\n",
    "        out.at[i, \"vibe_score\"]        = float(vibe_sc) if vibe_sc == vibe_sc else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Plotting helpers (per-file voted overlays)\n",
    "# =========================================================\n",
    "def _pick_residual(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand = [c for c in df.columns if \"Residual\" in c and not any(t in c for t in [\"_delta\", \"_rolling_\"])]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def plot_voted_for_file(\n",
    "    df_file: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    rule: str,\n",
    "    min_gap: int,\n",
    "    figsize: Tuple[int, int] = (12, 5),\n",
    ") -> Optional[str]:\n",
    "    ensure_dir(out_dir)\n",
    "    residual_col = _pick_residual(df_file)\n",
    "    if residual_col is None:\n",
    "        print(\"âš ï¸ No residual column to plot.\")\n",
    "        return None\n",
    "\n",
    "    voted_rows = extract_voted_rows(df_file, rule=rule)\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=min_gap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_file.index, df_file[residual_col], label=residual_col, alpha=0.85)\n",
    "\n",
    "    if not voted_rows.empty:\n",
    "        plt.scatter(voted_rows.index, voted_rows[residual_col], s=12, label=f\"Voted anomalies ({rule})\")\n",
    "\n",
    "    if not episodes.empty:\n",
    "        for _, r in episodes.iterrows():\n",
    "            plt.axvspan(r[\"start_idx\"], r[\"end_idx\"], alpha=0.15, label=\"Episode\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uniq, seen = [], set()\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in seen:\n",
    "                uniq.append((h, l)); seen.add(l)\n",
    "        handles, labels = zip(*uniq)\n",
    "        plt.legend(handles, labels)\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    sf = df_file[\"source_file\"].iloc[0] if \"source_file\" in df_file.columns else \"file\"\n",
    "    plt.title(f\"{sf} â€” Residual with voted anomalies & episodes\")\n",
    "    plt.xlabel(\"Index\"); plt.ylabel(residual_col)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"voted_plot_{safe_name(sf)}.png\")\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_all_files(combined_df: pd.DataFrame, out_dir: str, rule: str, min_gap: int, max_files: Optional[int] = None):\n",
    "    paths = []\n",
    "    if \"source_file\" not in combined_df.columns:\n",
    "        print(\"âš ï¸ combined_df missing 'source_file'.\")\n",
    "        return paths\n",
    "    groups = list(combined_df.groupby(\"source_file\"))\n",
    "    if max_files is not None:\n",
    "        groups = groups[:max_files]\n",
    "    for fname, df_file in groups:\n",
    "        p = plot_voted_for_file(df_file, out_dir=out_dir, rule=rule, min_gap=min_gap)\n",
    "        if p:\n",
    "            paths.append(p); print(f\"ðŸ–¼ï¸ Saved: {p}\")\n",
    "    if not paths:\n",
    "        print(\"âš ï¸ No plots produced.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Sensor attribution, clustering & heatmap\n",
    "# =========================================================\n",
    "def _residual_cols_base(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_sensor_table(\n",
    "    combined: pd.DataFrame,\n",
    "    voted_rows: pd.DataFrame,\n",
    "    episodes_with_reasons: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    base_res = _residual_cols_base(combined)\n",
    "    if not base_res:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(combined)\n",
    "    voted_mask = pd.Series(False, index=combined.index)\n",
    "    if not voted_rows.empty:\n",
    "        voted_mask.loc[voted_rows.index] = True\n",
    "\n",
    "    rows = []\n",
    "    expected_keys = [\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"anomaly_rate_hybrid\",    # NEW\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_vote_any\",  # NEW\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "        \"episodes_as_primary\",\n",
    "    ]\n",
    "\n",
    "    for col in base_res:\n",
    "        stats = {\"sensor\": col}\n",
    "\n",
    "        # Model rates (per total rows)\n",
    "        stats[\"anomaly_rate_is\"]   = float(combined[\"is_anomaly\"].sum())   / max(total_rows, 1) if \"is_anomaly\"   in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_ae\"]   = float(combined[\"ae_is_anomaly\"].sum())/ max(total_rows, 1) if \"ae_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lof\"]  = float(combined[\"lof_is_anomaly\"].sum())/max(total_rows, 1) if \"lof_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lstm\"] = float(combined[\"lstm_is_anomaly\"].fillna(0).sum())/max(total_rows, 1) if \"lstm_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_hybrid\"] = float(combined[\"hybrid_is_anomaly\"].sum())/max(total_rows, 1) if \"hybrid_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote3p\"] = float(combined[\"vote_3plus\"].sum())/max(total_rows, 1) if \"vote_3plus\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote_any\"] = float(combined[\"vote_any\"].sum())/max(total_rows, 1) if \"vote_any\" in combined.columns else 0.0\n",
    "\n",
    "        # Mean/Max |residual| during voted anomalies\n",
    "        if col in combined.columns and voted_mask.any():\n",
    "            vals = combined.loc[voted_mask, col].abs()\n",
    "            stats[\"mean_abs_resid_voted\"] = float(vals.mean()) if not vals.empty else 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = float(vals.max())  if not vals.empty else 0.0\n",
    "        else:\n",
    "            stats[\"mean_abs_resid_voted\"] = 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = 0.0\n",
    "\n",
    "        # Episodes where this residual was primary\n",
    "        if episodes_with_reasons is not None and not episodes_with_reasons.empty and \"primary_signal\" in episodes_with_reasons.columns:\n",
    "            stats[\"episodes_as_primary\"] = int((episodes_with_reasons[\"primary_signal\"] == col).sum())\n",
    "        else:\n",
    "            stats[\"episodes_as_primary\"] = 0\n",
    "\n",
    "        for k in expected_keys:\n",
    "            stats.setdefault(k, 0.0)\n",
    "\n",
    "        rows.append(stats)\n",
    "\n",
    "    sensor_df = pd.DataFrame(rows)\n",
    "    for c in sensor_df.columns:\n",
    "        if c != \"sensor\":\n",
    "            sensor_df[c] = sensor_df[c].fillna(0.0)\n",
    "    return sensor_df\n",
    "\n",
    "\n",
    "def cluster_sensors(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    n_clusters: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    if sensor_df.empty or \"sensor\" not in sensor_df.columns:\n",
    "        return sensor_df, np.empty((0, 2)), np.empty((0, 2))\n",
    "\n",
    "    features = sensor_df.drop(columns=[\"sensor\"]).to_numpy(dtype=np.float32)\n",
    "    if features.shape[0] < n_clusters:\n",
    "        n_clusters = max(1, features.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(features)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(Z)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    Z2 = pca.fit_transform(Z)\n",
    "    centers2 = pca.transform(km.cluster_centers_)\n",
    "\n",
    "    out = sensor_df.copy()\n",
    "    out[\"cluster\"] = labels\n",
    "\n",
    "    return out, Z2, centers2\n",
    "\n",
    "\n",
    "def plot_sensor_bar_top(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metric: str = \"episodes_as_primary\",\n",
    "    top_n: int = 15,\n",
    "    title: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty or metric not in sensor_df.columns:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    df = sensor_df.sort_values(metric, ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(df)), df[metric])\n",
    "    plt.xticks(range(len(df)), [s.replace(\"Force_\", \"F_\") for s in df[\"sensor\"]], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title or f\"Top {top_n} sensors by {metric}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"top_sensors_{metric}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_clusters_scatter(\n",
    "    sensor_df_with_cluster: pd.DataFrame,\n",
    "    Z2: np.ndarray,\n",
    "    centers2: np.ndarray,\n",
    "    out_dir: str,\n",
    "    title: str = \"Sensor clusters (PCA of features)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df_with_cluster.empty or Z2.size == 0:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    clusters = sorted(sensor_df_with_cluster[\"cluster\"].unique().tolist())\n",
    "    for cl in clusters:\n",
    "        mask = sensor_df_with_cluster[\"cluster\"] == cl\n",
    "        pts = Z2[mask.values]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"cluster {cl}\", alpha=0.8, s=36)\n",
    "\n",
    "    if centers2.size:\n",
    "        plt.scatter(centers2[:, 0], centers2[:, 1], marker=\"X\", s=120, label=\"centers\")\n",
    "\n",
    "    try:\n",
    "        top_lab = sensor_df_with_cluster.sort_values(\"episodes_as_primary\", ascending=False).head(10).index\n",
    "        for idx in top_lab:\n",
    "            plt.text(Z2[idx, 0], Z2[idx, 1], sensor_df_with_cluster.loc[idx, \"sensor\"], fontsize=8)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_clusters_pca.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_heatmap(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    title: str = \"Sensor anomaly fingerprint (rates & magnitudes)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty:\n",
    "        return None\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    desired = [\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_vote_any\",  # NEW\n",
    "        \"anomaly_rate_hybrid\",    # NEW\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "    ]\n",
    "    if metrics is None:\n",
    "        metrics = desired\n",
    "\n",
    "    available = [m for m in metrics if m in sensor_df.columns]\n",
    "    if not available:\n",
    "        print(\"âš ï¸ No requested heatmap metrics are present in sensor_df. Skipping heatmap.\")\n",
    "        return None\n",
    "    if len(available) < len(metrics):\n",
    "        missing = [m for m in metrics if m not in sensor_df.columns]\n",
    "        print(f\"â„¹ï¸ Skipping missing metrics in heatmap: {missing}\")\n",
    "    metrics = available\n",
    "\n",
    "    key_rank = \"episodes_as_primary\" if \"episodes_as_primary\" in sensor_df.columns else metrics[0]\n",
    "    keep = sensor_df.sort_values(key_rank, ascending=False).head(25)\n",
    "\n",
    "    M = keep[metrics].to_numpy(dtype=np.float32)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(M, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.yticks(range(len(keep)), keep[\"sensor\"])\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_fingerprint_heatmap.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Report\n",
    "# =========================================================\n",
    "def _overlay_episode_plot(df: pd.DataFrame, episode_row: pd.Series, cfg: dict, ax=None):\n",
    "    start, end = int(episode_row[\"start_idx\"]), int(episode_row[\"end_idx\"])\n",
    "    primary = episode_row.get(\"primary_signal\", \"\")\n",
    "    demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    t = np.arange(start, end + 1)\n",
    "    if primary in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, primary].values, label=f\"{primary}\", alpha=0.85)\n",
    "    if demand_col and demand_col in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, demand_col].values, label=f\"{demand_col}\", alpha=0.8)\n",
    "    if measured_col and measured_col in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, measured_col].values, label=f\"{measured_col}\", alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_title(f\"Episode {start}â€“{end}\\nprimary={primary}\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def build_ops_report(\n",
    "    combined: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    sensor_df: pd.DataFrame,\n",
    "    episodes_scored: pd.DataFrame,\n",
    "    cfg: dict,\n",
    "    out_pdf_path: str\n",
    "):\n",
    "    ensure_dir(os.path.dirname(out_pdf_path))\n",
    "    with PdfPages(out_pdf_path) as pdf:\n",
    "\n",
    "        # Page 1 â€” Anomalies counts by model (dynamic columns incl. Hybrid + Plain voting)\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"] if c in summary.columns]\n",
    "        summary[plot_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomalies per Model per File\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 2 â€” Top sensors by episodes_as_primary\n",
    "        p1 = plot_sensor_bar_top(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"], metric=\"episodes_as_primary\", top_n=15,\n",
    "                                 title=\"Top sensors by episodes_as_primary\")\n",
    "        if p1 and os.path.exists(p1):\n",
    "            img = plt.imread(p1)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 3 â€” Sensor heatmap (if created)\n",
    "        p2 = plot_sensor_heatmap(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"])\n",
    "        if p2 and os.path.exists(p2):\n",
    "            img = plt.imread(p2)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Pages 4+ â€” Example episode overlays\n",
    "        if not episodes_scored.empty:\n",
    "            candidates = episodes_scored.copy()\n",
    "            if \"n_models_mean\" in candidates.columns:\n",
    "                candidates = candidates.sort_values([\"n_models_mean\"], ascending=False)\n",
    "            n_show = min(cfg[\"report\"][\"top_n_episodes\"], len(candidates))\n",
    "            for _, epi in candidates.head(n_show).iterrows():\n",
    "                plt.figure(figsize=(11, 5))\n",
    "                if \"source_file\" in combined.columns and \"source_file\" in epi:\n",
    "                    sub = combined.loc[combined[\"source_file\"] == epi[\"source_file\"]]\n",
    "                else:\n",
    "                    sub = combined\n",
    "                _overlay_episode_plot(sub, epi, cfg, ax=plt.gca())\n",
    "                hw = epi.get(\"hardware_class\", \"Unknown\")\n",
    "                why = epi.get(\"hardware_why\", \"\")\n",
    "                lag_s = epi.get(\"lag_seconds\", np.nan)\n",
    "                sat   = epi.get(\"saturation_score\", np.nan)\n",
    "                drift = epi.get(\"drift_score\", np.nan)\n",
    "                vibe  = epi.get(\"vibe_score\", np.nan)\n",
    "                txt = (\n",
    "                    f\"hardware: {hw}\\n\"\n",
    "                    f\"why: {why}\\n\"\n",
    "                    f\"lag_seconds: {lag_s:.4f}  |  saturation: {sat:.3f}  |  drift: {drift:.3f}  |  vibe: {vibe:.3f}\"\n",
    "                )\n",
    "                plt.gcf().text(0.02, 0.02, txt, ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(); plt.close()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Config (defaults or JSON)\n",
    "# =========================================================\n",
    "def default_config() -> dict:\n",
    "    return {\n",
    "        \"io\": {\n",
    "            \"input_folder\": \"./Datasets/Datasets\",\n",
    "            \"residual_folder\": \"./Anomaly_detection/residual_created/\",\n",
    "            \"output_folder\": \"./Anomaly_detection/code/outputs/\"\n",
    "        },\n",
    "        \"residuals\": {\n",
    "            \"enabled\": True,\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\",\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"suffix\": \"_residual\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"window\": 5,\n",
    "            \"max_features\": 500\n",
    "        },\n",
    "        \"threshold\": {\n",
    "            \"k\": 3.5\n",
    "        },\n",
    "        \"ae\": {\n",
    "            \"epochs\": 50,\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"seq_len\": 5,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"patience\": 5,\n",
    "            \"max_sequences\": 3000,\n",
    "            \"downsample\": 5\n",
    "        },\n",
    "        \"lof\": {\n",
    "            \"n_neighbors\": 20\n",
    "        },\n",
    "        \"hybrid\": {  # NEW\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"robust_z\",   # \"robust_z\" | \"percentile\"\n",
    "            \"weights\": {\n",
    "                \"iso_score\": 0.25,\n",
    "                \"lof_score\": 0.25,\n",
    "                \"ae_error\": 0.25,\n",
    "                \"lstm_error\": 0.25\n",
    "            }\n",
    "        },\n",
    "        \"voting\": {\n",
    "            \"rule\": \"vote_3plus\",    # \"vote_3plus\" | \"agreement_all_4\" | \"any\"\n",
    "            \"min_gap\": 1\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_files\": None\n",
    "        },\n",
    "        \"runtime\": {\n",
    "            \"use_float32\": True\n",
    "        },\n",
    "        \"signals\": {\n",
    "            \"sample_rate_hz\": 100.0,     # set None if unknown\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\"\n",
    "        },\n",
    "        \"scores\": {\n",
    "            \"saturation_pct\": 95.0,\n",
    "            \"resid_prominence_pct\": 95.0,\n",
    "            \"min_window_len\": 5\n",
    "        },\n",
    "        \"report\": {\n",
    "            \"enabled\": True,\n",
    "            \"top_n_episodes\": 3\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_config_from_path_or_default(path: Optional[str]) -> dict:\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    print(\"â„¹ï¸  No --config provided or not found. Using in-memory default config.\")\n",
    "    return default_config()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Per-file processing & Pipeline\n",
    "# =========================================================\n",
    "def process_file(file_path: str, cfg: Dict, logger=print) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "\n",
    "    residual_cols = [c for c in df.columns if \"Residual\" in c]\n",
    "    if not residual_cols:\n",
    "        logger(f\"âŒ Skipped {file_name}: No residuals found.\")\n",
    "        return None\n",
    "\n",
    "    X, feature_cols, fe_stats = prepare_features(\n",
    "        df, residual_cols,\n",
    "        window=cfg[\"features\"][\"window\"],\n",
    "        max_features=cfg[\"features\"][\"max_features\"],\n",
    "        logger=logger,\n",
    "    )\n",
    "    if X is None or len(feature_cols) == 0 or X.empty:\n",
    "        logger(f\"âŒ Skipped {file_name}: invalid or empty features\")\n",
    "        return None\n",
    "\n",
    "    scaler, X_scaled, X_tensor = scale_features(X, use_float32=cfg[\"runtime\"][\"use_float32\"])\n",
    "\n",
    "    iso_labels, iso_scores, iso_thr = isolation_forest_detect(X_scaled, k=cfg[\"threshold\"][\"k\"])\n",
    "    df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
    "    df.loc[X.index, \"iso_score\"] = iso_scores\n",
    "    df.loc[X.index, \"iso_thr\"] = iso_thr\n",
    "\n",
    "    ae_labels, ae_errors, ae_thr = dense_autoencoder_detect(\n",
    "        X_tensor, k=cfg[\"threshold\"][\"k\"], ae_epochs=cfg[\"ae\"][\"epochs\"], ae_lr=cfg[\"ae\"][\"lr\"]\n",
    "    )\n",
    "    df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
    "    df.loc[X.index, \"ae_error\"] = ae_errors\n",
    "    df.loc[X.index, \"ae_thr\"] = ae_thr\n",
    "\n",
    "    lof_labels, lof_scores, lof_thr = lof_detect(\n",
    "        X_scaled, k=cfg[\"threshold\"][\"k\"], n_neighbors=cfg[\"lof\"][\"n_neighbors\"]\n",
    "    )\n",
    "    df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
    "    df.loc[X.index, \"lof_score\"] = lof_scores\n",
    "    df.loc[X.index, \"lof_thr\"] = lof_thr\n",
    "\n",
    "    lstm_labels, lstm_errors, lstm_idx, lstm_thr = lstm_autoencoder_detect(\n",
    "        X_scaled,\n",
    "        k=cfg[\"threshold\"][\"k\"],\n",
    "        seq_len=cfg[\"lstm\"][\"seq_len\"],\n",
    "        hidden_dim=cfg[\"lstm\"][\"hidden_dim\"],\n",
    "        patience=cfg[\"lstm\"][\"patience\"],\n",
    "        max_sequences=cfg[\"lstm\"][\"max_sequences\"],\n",
    "        downsample=cfg[\"lstm\"][\"downsample\"],\n",
    "    )\n",
    "    if len(lstm_idx) > 0:\n",
    "        df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
    "        df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
    "        df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
    "    else:\n",
    "        df[\"lstm_is_anomaly\"] = 0\n",
    "        df[\"lstm_error\"] = np.nan\n",
    "        df[\"lstm_thr\"] = np.nan\n",
    "\n",
    "    # --- NEW: Hybrid score (weighted fusion)\n",
    "    hybrid = compute_hybrid_score(df, cfg)\n",
    "    df[\"hybrid_score\"] = hybrid\n",
    "    hs = df.loc[X.index, \"hybrid_score\"].to_numpy()\n",
    "    if np.isnan(hs).all():\n",
    "        df.loc[X.index, \"hybrid_is_anomaly\"] = 0\n",
    "        df.loc[X.index, \"hybrid_thr\"] = np.nan\n",
    "    else:\n",
    "        hthr, hlabels = robust_threshold(hs, k=cfg[\"threshold\"][\"k\"], tail=\"high\")\n",
    "        df.loc[X.index, \"hybrid_is_anomaly\"] = hlabels.astype(int)\n",
    "        df.loc[X.index, \"hybrid_thr\"] = hthr\n",
    "\n",
    "    df = generate_votes(df)  # includes vote_3plus + vote_any\n",
    "    df[\"source_file\"] = file_name\n",
    "    df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
    "    df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n",
    "\n",
    "    logger(\n",
    "        f\"[{file_name}] iso={int(df['is_anomaly'].sum())} | \"\n",
    "        f\"ae={int(df['ae_is_anomaly'].sum())} | \"\n",
    "        f\"lof={int(df['lof_is_anomaly'].sum())} | \"\n",
    "        f\"lstm={int(df['lstm_is_anomaly'].fillna(0).sum())} | \"\n",
    "        f\"hyb={int(df['hybrid_is_anomaly'].sum())} | \"\n",
    "        f\"vote3+={int(df['vote_3plus'].sum())} | any={int(df['vote_any'].sum())}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Dict):\n",
    "    logger = print\n",
    "\n",
    "    # A) residuals (optional)\n",
    "    if cfg[\"residuals\"][\"enabled\"]:\n",
    "        logger(\"ðŸ”§ Creating residuals...\")\n",
    "        create_residuals_for_folder(\n",
    "            in_folder=cfg[\"io\"][\"input_folder\"],\n",
    "            out_folder=cfg[\"io\"][\"residual_folder\"],\n",
    "            demand_token=cfg[\"residuals\"][\"demand_token\"],\n",
    "            measured_token=cfg[\"residuals\"][\"measured_token\"],\n",
    "            residual_token=cfg[\"residuals\"][\"residual_token\"],\n",
    "            skip_if_exists=True,\n",
    "            suffix=cfg[\"residuals\"][\"suffix\"],\n",
    "            logger=logger,\n",
    "        )\n",
    "        data_folder = cfg[\"io\"][\"residual_folder\"]\n",
    "    else:\n",
    "        data_folder = cfg[\"io\"][\"input_folder\"]\n",
    "\n",
    "    # B) per-file\n",
    "    all_dfs = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            out = process_file(os.path.join(data_folder, file), cfg, logger=logger)\n",
    "            if out is not None:\n",
    "                all_dfs.append(out)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger(\"âŒ No files processed.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    ensure_dir(cfg[\"io\"][\"output_folder\"])\n",
    "\n",
    "    combined_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"combined_anomaly_results.csv\")\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "\n",
    "    cols = [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"]\n",
    "    cols = [c for c in cols if c in combined.columns]\n",
    "    summary = combined.groupby(\"source_file\")[cols].sum()\n",
    "    summary[\"total_anomalies\"] = summary.sum(axis=1)\n",
    "    summary_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_summary.csv\")\n",
    "    summary.to_csv(summary_path)\n",
    "\n",
    "    logger(f\"âœ… Saved row-level: {combined_path}\")\n",
    "    logger(f\"âœ… Saved summary:   {summary_path}\")\n",
    "\n",
    "    # C) Counts plot (dynamic cols incl. Hybrid + Plain voting)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"] if c in summary.columns]\n",
    "    summary[plot_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "    plt.title(\"Anomalies per Model per File\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_plot.png\")\n",
    "    plt.savefig(bar_path); plt.close()\n",
    "    logger(f\"ðŸ–¼ï¸ Saved: {bar_path}\")\n",
    "\n",
    "    # D) Voted rows + episodes + reasons\n",
    "    voted_rows = extract_voted_rows(combined, rule=cfg[\"voting\"][\"rule\"])\n",
    "    voted_dir = os.path.join(cfg[\"io\"][\"output_folder\"], \"voted_outputs\")\n",
    "    ensure_dir(voted_dir)\n",
    "    voted_rows_path = os.path.join(voted_dir, \"voted_anomalies_rows.csv\")\n",
    "    voted_rows.to_csv(voted_rows_path, index=False)\n",
    "\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=cfg[\"voting\"][\"min_gap\"])\n",
    "    episodes_path = os.path.join(voted_dir, \"voted_anomaly_episodes.csv\")\n",
    "    episodes.to_csv(episodes_path, index=False)\n",
    "\n",
    "    episodes_with_reasons = attach_episode_reasons(combined, episodes, top_k=1)\n",
    "    episodes_with_reasons = enrich_hardware_mapping(episodes_with_reasons)\n",
    "    episodes_scored = score_episodes(combined, episodes_with_reasons, cfg)\n",
    "\n",
    "    episodes_reason_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons.csv\")\n",
    "    episodes_scored_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons_and_scores.csv\")\n",
    "    episodes_with_reasons.to_csv(episodes_reason_path, index=False)\n",
    "    episodes_scored.to_csv(episodes_scored_path, index=False)\n",
    "    logger(f\"âœ… Saved episodes+reason: {episodes_reason_path}\")\n",
    "    logger(f\"âœ… Saved episodes+scores: {episodes_scored_path}\")\n",
    "\n",
    "    # E) Per-file plots with voted overlays (optional)\n",
    "    if cfg[\"plots\"][\"enabled\"]:\n",
    "        _ = plot_all_files(\n",
    "            combined_df=combined,\n",
    "            out_dir=voted_dir,\n",
    "            rule=cfg[\"voting\"][\"rule\"],\n",
    "            min_gap=cfg[\"voting\"][\"min_gap\"],\n",
    "            max_files=cfg[\"plots\"][\"max_files\"],\n",
    "        )\n",
    "\n",
    "    # F) Sensor table + clustering visuals\n",
    "    sensor_df = build_sensor_table(combined, voted_rows, episodes_with_reasons=episodes_with_reasons)\n",
    "    sensor_df_path = os.path.join(voted_dir, \"sensor_table.csv\")\n",
    "    sensor_df.to_csv(sensor_df_path, index=False)\n",
    "    logger(f\"âœ… Saved sensor table: {sensor_df_path}\")\n",
    "\n",
    "    clustered, Z2, centers2 = cluster_sensors(sensor_df, n_clusters=3, random_state=42)\n",
    "    _ = plot_sensor_clusters_scatter(clustered, Z2, centers2, out_dir=voted_dir)\n",
    "    _ = plot_sensor_heatmap(sensor_df, out_dir=voted_dir)\n",
    "    _ = plot_sensor_bar_top(sensor_df, out_dir=voted_dir, metric=\"episodes_as_primary\", top_n=15)\n",
    "\n",
    "    # G) PDF report\n",
    "    if cfg.get(\"report\", {}).get(\"enabled\", True):\n",
    "        pdf_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"ops_report.pdf\")\n",
    "        build_ops_report(\n",
    "            combined=combined,\n",
    "            summary=summary,\n",
    "            sensor_df=sensor_df,\n",
    "            episodes_scored=episodes_scored,\n",
    "            cfg=cfg,\n",
    "            out_pdf_path=pdf_path\n",
    "        )\n",
    "        logger(f\"ðŸ“„ Ops report saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Entrypoint\n",
    "# =========================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Anomaly Detection Product\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to config JSON\")\n",
    "    args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "    cfg = load_config_from_path_or_default(args.config)\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8535695-fd3a-4af0-9d80-136ebb5c8f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No --config provided or not found. Using in-memory default config.\n",
      "ðŸ”§ Creating residuals...\n",
      "â†©ï¸  Skip residual (exists): Dataset01_Ski_CrossbeamYawNotPerforming_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset02_Matrix_Rocker4EncoderNotWorking_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset03_Wushu_YawTrapezoidNormal_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset04_Wushu_YawWaveletSqueak_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset05_Wushu_LaneChanges_ModelBump_residual.csv\n",
      "âŒ Failed to read Dataset07_Demo_Spa_GT.csv: No columns to parse from file\n",
      "â†©ï¸  Skip residual (exists): Dataset08_Demo_Jiggler_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset09_Demo_VerticalChirp_residual.csv\n",
      "âŒ Failed to read Dataset10_Demo_MillbrookHills.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:410: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:416: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset01_Ski_CrossbeamYawNotPerforming_residual] iso=6081 | ae=12267 | lof=1724 | lstm=221 | hyb=1802 | vote3+=216 | any=14816\n",
      "âŒ Skipped Dataset02_Matrix_Rocker4EncoderNotWorking_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:410: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:416: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset03_Wushu_YawTrapezoidNormal_residual] iso=4353 | ae=16319 | lof=2735 | lstm=138 | hyb=3815 | vote3+=338 | any=20169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:410: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:416: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset04_Wushu_YawWaveletSqueak_residual] iso=5912 | ae=5620 | lof=1179 | lstm=728 | hyb=190 | vote3+=662 | any=7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:410: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:416: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # plain voting (>=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_26456\\1082362259.py:1247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset05_Wushu_LaneChanges_ModelBump_residual] iso=7951 | ae=8558 | lof=1177 | lstm=334 | hyb=1312 | vote3+=118 | any=11125\n",
      "âŒ Skipped Dataset08_Demo_Jiggler_residual: No residuals found.\n",
      "âŒ Skipped Dataset09_Demo_VerticalChirp_residual: No residuals found.\n",
      "âœ… Saved row-level: ./Anomaly_detection/code/outputs/combined_anomaly_results.csv\n",
      "âœ… Saved summary:   ./Anomaly_detection/code/outputs/model_comparison_summary.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_plot.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_rate_plot.png\n",
      "âœ… Saved episodes+reason: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons.csv\n",
      "âœ… Saved episodes+scores: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons_and_scores.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset01_Ski_CrossbeamYawNotPerforming_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset03_Wushu_YawTrapezoidNormal_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset04_Wushu_YawWaveletSqueak_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset05_Wushu_LaneChanges_ModelBump_residual.png\n",
      "âœ… Saved sensor table: ./Anomaly_detection/code/outputs/voted_outputs\\sensor_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Ops report saved: ./Anomaly_detection/code/outputs/ops_report.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Anomaly Detection Product (single script) â€” Extended & Calibrated\n",
    "- Residual creation (Demand - Measured -> Residual)\n",
    "- Feature engineering (reuse if already present)\n",
    "- Scaling once -> shared across models\n",
    "- Models: IsolationForest, LOF, Dense AE, LSTM AE\n",
    "- Dynamic thresholds (MAD) for all scores\n",
    "- Voting (3+) + episodes (merged runs) + vote_any (>=1 model)\n",
    "- Hybrid scoring (weighted fusion) with MAD or quantile threshold + fallback\n",
    "- Episode explanations (primary signal, suspected sensor)\n",
    "- Hardware mapping & root-cause scoring (lag/saturation/drift/vibe)\n",
    "- Sensor ranking, clustering & heatmap\n",
    "- Multi-page PDF Ops Report (dynamic first page bars)\n",
    "- Adds anomaly RATE plot (percent of rows)\n",
    "- Config-driven (JSON) OR safe defaults (no args)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utils\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in str(name))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Residual creation (optional)\n",
    "# =========================================================\n",
    "def create_residuals_for_folder(\n",
    "    in_folder: str,\n",
    "    out_folder: str,\n",
    "    demand_token: str = \"Demand\",\n",
    "    measured_token: str = \"Measured\",\n",
    "    residual_token: str = \"Residual\",\n",
    "    skip_if_exists: bool = True,\n",
    "    suffix: str = \"_residual\",\n",
    "    logger=print,\n",
    ") -> None:\n",
    "    ensure_dir(out_folder)\n",
    "    for file in os.listdir(in_folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(in_folder, file)\n",
    "        out_name = file.replace(\".csv\", f\"{suffix}.csv\")\n",
    "        out_path = os.path.join(out_folder, out_name)\n",
    "\n",
    "        if skip_if_exists and os.path.exists(out_path):\n",
    "            logger(f\"â†©ï¸  Skip residual (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(in_path)\n",
    "        except Exception as e:\n",
    "            logger(f\"âŒ Failed to read {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        made_any = False\n",
    "        for col in cols:\n",
    "            if demand_token in col:\n",
    "                measured_col = col.replace(demand_token, measured_token)\n",
    "                if measured_col in df.columns:\n",
    "                    residual_col = col.replace(demand_token, residual_token)\n",
    "                    df[residual_col] = df[col] - df[measured_col]\n",
    "                    made_any = True\n",
    "\n",
    "        if not made_any:\n",
    "            logger(f\"âš ï¸  No Demand/Measured pairs found in {file}.\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        logger(f\"âœ… Residual CSV saved: {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Scaling + robust threshold (MAD)\n",
    "# =========================================================\n",
    "def scale_features(X: pd.DataFrame, use_float32: bool = True):\n",
    "    \"\"\"\n",
    "    Standardize features once and share across models.\n",
    "    Returns (scaler, X_scaled np.array, X_tensor torch.tensor)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    if use_float32:\n",
    "        X_scaled = X_scaled.astype(\"float32\")\n",
    "    X_tensor = torch.from_numpy(X_scaled)\n",
    "    return scaler, X_scaled, X_tensor\n",
    "\n",
    "\n",
    "def robust_threshold(\n",
    "    values: np.ndarray,\n",
    "    k: float = 3.5,\n",
    "    tail: str = \"high\",\n",
    "    min_anoms: int = 5,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    MAD-based threshold: median Â± k * 1.4826 * MAD\n",
    "    tail = 'high' (right tail) or 'low' (left tail)\n",
    "    Returns: (threshold, labels) labels aligned to 'values' (1=anomaly)\n",
    "    \"\"\"\n",
    "    v = np.asarray(values)\n",
    "    mask = ~np.isnan(v)\n",
    "    v = v[mask]\n",
    "    if v.size == 0:\n",
    "        return (np.inf if tail == \"high\" else -np.inf), np.zeros_like(values, dtype=int)\n",
    "\n",
    "    med = np.median(v)\n",
    "    mad = np.median(np.abs(v - med)) + 1e-12\n",
    "    if tail == \"high\":\n",
    "        thr = med + k * 1.4826 * mad\n",
    "        labels = (values > thr).astype(int)\n",
    "    else:\n",
    "        thr = med - k * 1.4826 * mad\n",
    "        labels = (values < thr).astype(int)\n",
    "\n",
    "    # relax if too strict on large arrays\n",
    "    if labels.sum() < min_anoms and v.size >= 100:\n",
    "        for k_relax in (3.0, 2.5, 2.0):\n",
    "            if tail == \"high\":\n",
    "                thr = med + k_relax * 1.4826 * mad\n",
    "                labels = (values > thr).astype(int)\n",
    "            else:\n",
    "                thr = med - k_relax * 1.4826 * mad\n",
    "                labels = (values < thr).astype(int)\n",
    "            if labels.sum() >= min_anoms:\n",
    "                break\n",
    "\n",
    "    return thr, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Feature Engineering\n",
    "# =========================================================\n",
    "def prepare_features(\n",
    "    df: pd.DataFrame,\n",
    "    residual_cols: List[str],\n",
    "    window: int = 5,\n",
    "    max_features: int = 500,\n",
    "    logger=print,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Create or reuse features: residual, delta, rolling mean/std\n",
    "    Returns: X, feature_cols, stats (reused vs generated)\n",
    "    \"\"\"\n",
    "    already_done = any(f\"{residual_cols[0]}_delta\" in df.columns for _ in residual_cols)\n",
    "    stats = {\"reused\": 0, \"generated\": 0}\n",
    "\n",
    "    if already_done:\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if any(k in c for k in [\"Residual\", \"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "        ]\n",
    "        X = df[feature_cols].dropna()\n",
    "        stats[\"reused\"] = len(feature_cols)\n",
    "        logger(f\"ðŸ” Reusing {len(feature_cols)} engineered features.\")\n",
    "        return X, feature_cols, stats\n",
    "\n",
    "    # Generate\n",
    "    for col in residual_cols:\n",
    "        df[f\"{col}_delta\"] = df[col].diff()\n",
    "        df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
    "        df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in residual_cols:\n",
    "        feature_cols += [\n",
    "            col,\n",
    "            f\"{col}_delta\",\n",
    "            f\"{col}_rolling_mean_{window}\",\n",
    "            f\"{col}_rolling_std_{window}\",\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].dropna()\n",
    "    stats[\"generated\"] = len(feature_cols)\n",
    "    logger(f\"ðŸ› ï¸  Generated {len(feature_cols)} features (window={window}).\")\n",
    "\n",
    "    if X.shape[1] > max_features:\n",
    "        logger(f\"âŒ Too many features ({X.shape[1]} > {max_features}). Skipping file.\")\n",
    "        return pd.DataFrame(), [], stats\n",
    "\n",
    "    return X, feature_cols, stats\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Models\n",
    "# =========================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 32), nn.ReLU(), nn.Linear(32, 8))\n",
    "        self.decoder = nn.Sequential(nn.Linear(8, 32), nn.ReLU(), nn.Linear(32, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def dense_autoencoder_detect(\n",
    "    X_tensor: torch.Tensor, k: float, ae_epochs: int, ae_lr: float\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    model = Autoencoder(X_tensor.shape[1])\n",
    "    opt = optim.Adam(model.parameters(), lr=ae_lr)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    for _ in range(ae_epochs):\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = crit(out, X_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec = model(X_tensor)\n",
    "        errors = torch.mean((X_tensor - rec) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "    return labels.astype(int), errors, thr\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.encoder(x)  # [1, B, H]\n",
    "        repeated = h.repeat(x.size(1), 1, 1).transpose(0, 1)  # [B, T, H]\n",
    "        decoded, _ = self.decoder(repeated)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def make_sequences(X: np.ndarray, seq_len: int) -> Tuple[np.ndarray, List[int]]:\n",
    "    seqs, idxs = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seqs.append(X[i:i+seq_len])\n",
    "        idxs.append(i + seq_len - 1)\n",
    "    return np.array(seqs), idxs\n",
    "\n",
    "\n",
    "def lstm_autoencoder_detect(\n",
    "    X_scaled: np.ndarray,\n",
    "    k: float,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int,\n",
    "    patience: int,\n",
    "    max_sequences: int,\n",
    "    downsample: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[int], float]:\n",
    "    try:\n",
    "        Xds = X_scaled[::downsample]\n",
    "        if len(Xds) < seq_len:\n",
    "            return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "        Xseq, idxs = make_sequences(Xds, seq_len)\n",
    "        if len(Xseq) > max_sequences:\n",
    "            Xseq, idxs = Xseq[:max_sequences], idxs[:max_sequences]\n",
    "\n",
    "        Xt = torch.tensor(Xseq, dtype=torch.float32)\n",
    "        model = LSTMAutoencoder(Xt.shape[2], hidden_dim)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best, wait = float(\"inf\"), 0\n",
    "        for _ in range(100):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(Xt)\n",
    "            loss = crit(out, Xt)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if loss.item() < best:\n",
    "                best, wait = loss.item(), 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(Xt)\n",
    "            errors = torch.mean((Xt - out) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "        thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "        return labels.astype(int), errors, idxs, thr\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ LSTM memory error: {e}\")\n",
    "        return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "\n",
    "def isolation_forest_detect(X_scaled: np.ndarray, k: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    iso = IsolationForest(contamination=\"auto\", n_estimators=300, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "def lof_detect(X_scaled: np.ndarray, k: float, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=\"auto\")\n",
    "    _ = lof.fit_predict(X_scaled)  # populates negative_outlier_factor_\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hybrid scoring utilities\n",
    "# =========================================================\n",
    "def _robust_z_pos(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Right-tail robust z-score (>=0 when above median).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    z = np.where(np.isnan(z), np.nan, z)\n",
    "    return np.maximum(z, 0.0)  # only right tail counts as anomalous\n",
    "\n",
    "\n",
    "def _percentile01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map to [0,1] by robust percentiles (2â€“98). Values outside clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    lo = np.nanpercentile(x, 2)\n",
    "    hi = np.nanpercentile(x, 98)\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    y = (x - lo) / rng\n",
    "    return np.clip(y, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hybrid_score_on_mask(df: pd.DataFrame, cfg: dict, mask_idx) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute hybrid only on valid rows (mask_idx). Returns array the size of df,\n",
    "    NaN elsewhere. Requires >= min_components present.\n",
    "    \"\"\"\n",
    "    out = np.full(len(df), np.nan)\n",
    "    if not cfg.get(\"hybrid\", {}).get(\"enabled\", False):\n",
    "        return out\n",
    "\n",
    "    # Weights guard\n",
    "    wmap = cfg.get(\"hybrid\", {}).get(\"weights\")\n",
    "    if not isinstance(wmap, dict) or not wmap:\n",
    "        wmap = {\"iso_score\": 0.25, \"lof_score\": 0.25, \"ae_error\": 0.25, \"lstm_error\": 0.25}\n",
    "\n",
    "    method = cfg[\"hybrid\"].get(\"method\", \"robust_z\")\n",
    "    min_components = int(cfg[\"hybrid\"].get(\"min_components\", 2))\n",
    "\n",
    "    use = df.loc[mask_idx]  # restrict to valid feature rows\n",
    "\n",
    "    comps = [c for c in [\"iso_score\", \"lof_score\", \"ae_error\", \"lstm_error\"] if c in use.columns and c in wmap]\n",
    "    if not comps:\n",
    "        return out\n",
    "\n",
    "    parts = []\n",
    "    for c in comps:\n",
    "        arr = use[c].to_numpy(dtype=float)\n",
    "        if method == \"robust_z\":\n",
    "            norm = _robust_z_pos(arr)\n",
    "            norm = np.clip(norm, 0, 10.0) / 10.0  # compress extreme tails to ~[0,1]\n",
    "        else:\n",
    "            norm = _percentile01(arr)\n",
    "        parts.append((norm, float(wmap[c])))\n",
    "\n",
    "    num = np.zeros(len(use), dtype=float)\n",
    "    den = np.zeros(len(use), dtype=float)\n",
    "    present = np.zeros(len(use), dtype=int)\n",
    "\n",
    "    for norm, w in parts:\n",
    "        m = ~np.isnan(norm)\n",
    "        num[m] += w * norm[m]\n",
    "        den[m] += w\n",
    "        present[m] += 1\n",
    "\n",
    "    hybrid_local = np.where((den > 0) & (present >= min_components), num / den, np.nan)\n",
    "    out[np.asarray(mask_idx)] = hybrid_local\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Voting, episodes, explanations\n",
    "# =========================================================\n",
    "def generate_votes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"agreement_all_4\"] = (\n",
    "        (df.get(\"ae_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lof_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lstm_is_anomaly\", 0) == 1)\n",
    "    ).astype(int)\n",
    "    df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
    "    df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
    "    df[\"vote_any\"] = (df[\"num_votes\"] >= 1).astype(int)  # plain voting (>=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_voted_rows(df: pd.DataFrame, rule: str = \"vote_3plus\") -> pd.DataFrame:\n",
    "    if rule == \"vote_3plus\":\n",
    "        mask = df[\"vote_3plus\"] == 1\n",
    "    elif rule == \"agreement_all_4\":\n",
    "        mask = df[\"agreement_all_4\"] == 1\n",
    "    elif rule == \"any\":\n",
    "        mask = (\n",
    "            (df[\"ae_is_anomaly\"] == 1)\n",
    "            | (df[\"is_anomaly\"] == 1)\n",
    "            | (df[\"lof_is_anomaly\"] == 1)\n",
    "            | (df[\"lstm_is_anomaly\"] == 1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown rule: {rule}\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _group_runs(idxs: np.ndarray, min_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "    runs, start, prev = [], int(idxs[0]), int(idxs[0])\n",
    "    for i in idxs[1:]:\n",
    "        if int(i) - prev <= min_gap:\n",
    "            prev = int(i)\n",
    "            continue\n",
    "        runs.append((start, prev))\n",
    "        start = int(i); prev = int(i)\n",
    "    runs.append((start, prev))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def summarize_episodes(voted_df: pd.DataFrame, min_gap: int = 1) -> pd.DataFrame:\n",
    "    if voted_df.empty:\n",
    "        return pd.DataFrame(columns=[\"source_file\", \"start_idx\", \"end_idx\", \"length\", \"n_models_mean\"])\n",
    "\n",
    "    idxs = voted_df.index.to_numpy()\n",
    "    runs = _group_runs(idxs, min_gap=min_gap)\n",
    "\n",
    "    rows = []\n",
    "    for start, end in runs:\n",
    "        chunk = voted_df.loc[start:end]\n",
    "        row = {\n",
    "            \"source_file\": chunk[\"source_file\"].iloc[0] if \"source_file\" in chunk else \"\",\n",
    "            \"start_idx\": start,\n",
    "            \"end_idx\": end,\n",
    "            \"length\": int(end - start + 1),\n",
    "            \"n_models_mean\": float(\n",
    "                chunk[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1).mean()\n",
    "            ),\n",
    "        }\n",
    "        for c in [\"iso_score\", \"ae_error\", \"lof_score\", \"lstm_error\", \"hybrid_score\"]:\n",
    "            if c in chunk.columns:\n",
    "                row[f\"{c}_max\"] = float(chunk[c].max())\n",
    "                row[f\"{c}_mean\"] = float(chunk[c].mean())\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _base_residual_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def _models_string(chunk: pd.DataFrame) -> str:\n",
    "    model_cols = [c for c in [\"is_anomaly\", \"ae_is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\", \"hybrid_is_anomaly\"] if c in chunk.columns]\n",
    "    if not model_cols:\n",
    "        return \"no-model-flags\"\n",
    "    means = chunk[model_cols].mean()\n",
    "    active = [m.replace(\"_is_anomaly\", \"\").upper() for m, v in means.items() if v >= 0.5]\n",
    "    return \", \".join(active) if active else \"weak/isolated flags\"\n",
    "\n",
    "\n",
    "def attach_episode_reasons(\n",
    "    combined_df: pd.DataFrame, episodes_df: pd.DataFrame, top_k: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "\n",
    "    base_res = _base_residual_columns(combined_df)\n",
    "    if not base_res:\n",
    "        episodes_df[\"primary_signal\"] = \"\"\n",
    "        episodes_df[\"reason\"] = \"no residual columns present\"\n",
    "        episodes_df[\"suspected_sensor\"] = \"\"\n",
    "        return episodes_df\n",
    "\n",
    "    out = []\n",
    "    for _, epi in episodes_df.iterrows():\n",
    "        start, end = int(epi[\"start_idx\"]), int(epi[\"end_idx\"])\n",
    "        mask = combined_df[\"source_file\"] == epi[\"source_file\"] if \"source_file\" in combined_df.columns else slice(None)\n",
    "        chunk = combined_df.loc[mask].loc[start:end]\n",
    "\n",
    "        if chunk.empty:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"empty slice\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats = []\n",
    "        for col in base_res:\n",
    "            if col in chunk.columns:\n",
    "                stats.append((col, float(chunk[col].abs().max())))\n",
    "        if not stats:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"no residual stats\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_signal, primary_val = stats[:top_k][0]\n",
    "        models_str = _models_string(chunk)\n",
    "        measured_col = primary_signal.replace(\"Residual\", \"Measured\")\n",
    "        suspected = measured_col if (measured_col in combined_df.columns) else \"unknown-measured-sensor\"\n",
    "\n",
    "        epi[\"primary_signal\"] = primary_signal\n",
    "        epi[\"reason\"] = f\"max |{primary_signal}| = {primary_val:.3f}; models: {models_str}\"\n",
    "        epi[\"suspected_sensor\"] = suspected\n",
    "        out.append(epi)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hardware mapping + root cause scoring\n",
    "# =========================================================\n",
    "HARDWARE_MAP = [\n",
    "    (\"Force_\",         \"Actuator/LoadCell\",  \"Force didnâ€™t follow demand â†’ friction/lag/saturation/load-cell drift likely\"),\n",
    "    (\"Encoder_\",       \"Encoders/Alignment\", \"Pose/velocity mismatch â†’ quantization/missing counts/misalignment\"),\n",
    "    (\"Accelerometer_\", \"IMU/Accelerometer\",  \"Vibration bursts â†’ mounting/looseness/thermal drift\"),\n",
    "    (\"State_\",         \"Control/Timing\",     \"Requested vs achieved state diverged â†’ scheduler limits/controller windup\"),\n",
    "]\n",
    "\n",
    "def map_signal_to_hardware(primary_signal: str):\n",
    "    for needle, hw, why in HARDWARE_MAP:\n",
    "        if needle in primary_signal:\n",
    "            return hw, why\n",
    "    return \"Unknown\", \"No mapping rule matched\"\n",
    "\n",
    "\n",
    "def enrich_hardware_mapping(episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    episodes_df = episodes_df.copy()\n",
    "    episodes_df[\"hardware_class\"] = \"\"\n",
    "    episodes_df[\"hardware_why\"] = \"\"\n",
    "    for i, r in episodes_df.iterrows():\n",
    "        hw, why = map_signal_to_hardware(r.get(\"primary_signal\", \"\"))\n",
    "        episodes_df.at[i, \"hardware_class\"] = hw\n",
    "        episodes_df.at[i, \"hardware_why\"]   = why\n",
    "    return episodes_df\n",
    "\n",
    "\n",
    "def _paired_columns(primary_signal: str, cfg: dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "    resid_tok  = cfg[\"signals\"][\"residual_token\"]\n",
    "    demand_tok = cfg[\"signals\"][\"demand_token\"]\n",
    "    measured_tok = cfg[\"signals\"][\"measured_token\"]\n",
    "    if resid_tok not in primary_signal:\n",
    "        return None, None\n",
    "    demand_col   = primary_signal.replace(resid_tok, demand_tok)\n",
    "    measured_col = primary_signal.replace(resid_tok, measured_tok)\n",
    "    return demand_col, measured_col\n",
    "\n",
    "\n",
    "def _nan_ok(arr: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_correlation_lag(x: np.ndarray, y: np.ndarray, sample_rate_hz: Optional[float]) -> Tuple[float, int]:\n",
    "    x = _nan_ok(x); y = _nan_ok(y)\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return (np.nan, 0)\n",
    "    x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    x = np.nan_to_num(x);  y = np.nan_to_num(y)\n",
    "    corr = np.correlate(x, y, mode=\"full\")\n",
    "    lags = np.arange(-len(x)+1, len(x))\n",
    "    k = int(np.argmax(corr))\n",
    "    lag_samples = int(lags[k])\n",
    "    lag_seconds = lag_samples / sample_rate_hz if sample_rate_hz and sample_rate_hz > 0 else np.nan\n",
    "    return (lag_seconds, lag_samples)\n",
    "\n",
    "\n",
    "def _saturation_score(demand: np.ndarray, residual: np.ndarray, cfg: dict) -> float:\n",
    "    if len(demand) == 0 or len(residual) == 0:\n",
    "        return 0.0\n",
    "    p_dem = np.nanpercentile(demand, cfg[\"scores\"][\"saturation_pct\"])\n",
    "    p_res = np.nanpercentile(np.abs(residual), cfg[\"scores\"][\"resid_prominence_pct\"])\n",
    "    near_limit = demand >= p_dem\n",
    "    large_res  = np.abs(residual) >= p_res\n",
    "    both = np.logical_and(near_limit, large_res)\n",
    "    return float(np.nansum(both)) / max(1, len(demand))\n",
    "\n",
    "\n",
    "def _drift_score(residual: np.ndarray) -> float:\n",
    "    residual = _nan_ok(residual)\n",
    "    mu = float(np.nanmean(residual))\n",
    "    sd = float(np.nanstd(residual)) + 1e-9\n",
    "    return abs(mu) / sd\n",
    "\n",
    "\n",
    "def _vibration_score(signal: np.ndarray, sample_rate_hz: Optional[float]) -> float:\n",
    "    if not sample_rate_hz or sample_rate_hz <= 0 or len(signal) < 8:\n",
    "        return np.nan\n",
    "    sig = np.nan_to_num(signal - np.nanmean(signal))\n",
    "    fft = np.fft.rfft(sig)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1.0 / sample_rate_hz)\n",
    "    if len(freqs) == 0:\n",
    "        return np.nan\n",
    "    cutoff = 0.25 * (sample_rate_hz / 2.0)  # > Nyquist/4\n",
    "    mask_hi = freqs >= cutoff\n",
    "    num = float(np.nansum(power[mask_hi]))\n",
    "    den = float(np.nansum(power) + 1e-12)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def score_episodes(combined_df: pd.DataFrame, episodes_df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds: lag_seconds, lag_samples, saturation_score, drift_score, vibe_score\n",
    "    \"\"\"\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    out = episodes_df.copy()\n",
    "    sr = cfg[\"signals\"][\"sample_rate_hz\"]\n",
    "    min_len = cfg[\"scores\"][\"min_window_len\"]\n",
    "\n",
    "    if \"primary_signal\" not in out.columns:\n",
    "        out[\"primary_signal\"] = \"\"\n",
    "\n",
    "    for i, r in out.iterrows():\n",
    "        start, end = int(r[\"start_idx\"]), int(r[\"end_idx\"])\n",
    "        if end - start + 1 < min_len:\n",
    "            out.at[i, \"lag_seconds\"] = np.nan\n",
    "            out.at[i, \"lag_samples\"] = 0\n",
    "            out.at[i, \"saturation_score\"] = 0.0\n",
    "            out.at[i, \"drift_score\"] = 0.0\n",
    "            out.at[i, \"vibe_score\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        if \"source_file\" in combined_df.columns and \"source_file\" in out.columns and \"source_file\" in r:\n",
    "            chunk = combined_df.loc[(combined_df[\"source_file\"] == r[\"source_file\"])].loc[start:end]\n",
    "        else:\n",
    "            chunk = combined_df.loc[start:end]\n",
    "\n",
    "        primary = r.get(\"primary_signal\", \"\")\n",
    "        demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "\n",
    "        resid = chunk[primary].values if (primary in chunk.columns) else np.array([])\n",
    "        dem   = chunk[demand_col].values if (demand_col and demand_col in chunk.columns) else np.array([])\n",
    "        meas  = chunk[measured_col].values if (measured_col and measured_col in chunk.columns) else np.array([])\n",
    "\n",
    "        lag_s, lag_k = _cross_correlation_lag(dem, meas, sr) if (len(dem) and len(meas)) else (np.nan, 0)\n",
    "        sat_sc = _saturation_score(dem, resid, cfg) if (len(dem) and len(resid)) else 0.0\n",
    "        dr_sc  = _drift_score(resid) if len(resid) else 0.0\n",
    "        if \"Accelerometer_\" in primary and primary in chunk.columns:\n",
    "            vibe_sc = _vibration_score(chunk[primary].values, sr)\n",
    "        else:\n",
    "            vibe_sc = _vibration_score(resid, sr)\n",
    "\n",
    "        out.at[i, \"lag_seconds\"]       = lag_s\n",
    "        out.at[i, \"lag_samples\"]       = int(lag_k)\n",
    "        out.at[i, \"saturation_score\"]  = float(sat_sc)\n",
    "        out.at[i, \"drift_score\"]       = float(dr_sc)\n",
    "        out.at[i, \"vibe_score\"]        = float(vibe_sc) if vibe_sc == vibe_sc else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Plotting helpers (per-file voted overlays)\n",
    "# =========================================================\n",
    "def _pick_residual(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand = [c for c in df.columns if \"Residual\" in c and not any(t in c for t in [\"_delta\", \"_rolling_\"])]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def plot_voted_for_file(\n",
    "    df_file: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    rule: str,\n",
    "    min_gap: int,\n",
    "    figsize: Tuple[int, int] = (12, 5),\n",
    ") -> Optional[str]:\n",
    "    ensure_dir(out_dir)\n",
    "    residual_col = _pick_residual(df_file)\n",
    "    if residual_col is None:\n",
    "        print(\"âš ï¸ No residual column to plot.\")\n",
    "        return None\n",
    "\n",
    "    voted_rows = extract_voted_rows(df_file, rule=rule)\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=min_gap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_file.index, df_file[residual_col], label=residual_col, alpha=0.85)\n",
    "\n",
    "    if not voted_rows.empty:\n",
    "        plt.scatter(voted_rows.index, voted_rows[residual_col], s=12, label=f\"Voted anomalies ({rule})\")\n",
    "\n",
    "    if not episodes.empty:\n",
    "        for _, r in episodes.iterrows():\n",
    "            plt.axvspan(r[\"start_idx\"], r[\"end_idx\"], alpha=0.15, label=\"Episode\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uniq, seen = [], set()\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in seen:\n",
    "                uniq.append((h, l)); seen.add(l)\n",
    "        handles, labels = zip(*uniq)\n",
    "        plt.legend(handles, labels)\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    sf = df_file[\"source_file\"].iloc[0] if \"source_file\" in df_file.columns else \"file\"\n",
    "    plt.title(f\"{sf} â€” Residual with voted anomalies & episodes\")\n",
    "    plt.xlabel(\"Index\"); plt.ylabel(residual_col)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"voted_plot_{safe_name(sf)}.png\")\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_all_files(combined_df: pd.DataFrame, out_dir: str, rule: str, min_gap: int, max_files: Optional[int] = None):\n",
    "    paths = []\n",
    "    if \"source_file\" not in combined_df.columns:\n",
    "        print(\"âš ï¸ combined_df missing 'source_file'.\")\n",
    "        return paths\n",
    "    groups = list(combined_df.groupby(\"source_file\"))\n",
    "    if max_files is not None:\n",
    "        groups = groups[:max_files]\n",
    "    for fname, df_file in groups:\n",
    "        p = plot_voted_for_file(df_file, out_dir=out_dir, rule=rule, min_gap=min_gap)\n",
    "        if p:\n",
    "            paths.append(p); print(f\"ðŸ–¼ï¸ Saved: {p}\")\n",
    "    if not paths:\n",
    "        print(\"âš ï¸ No plots produced.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Sensor attribution, clustering & heatmap\n",
    "# =========================================================\n",
    "def _residual_cols_base(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_sensor_table(\n",
    "    combined: pd.DataFrame,\n",
    "    voted_rows: pd.DataFrame,\n",
    "    episodes_with_reasons: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    base_res = _residual_cols_base(combined)\n",
    "    if not base_res:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(combined)\n",
    "    voted_mask = pd.Series(False, index=combined.index)\n",
    "    if not voted_rows.empty:\n",
    "        voted_mask.loc[voted_rows.index] = True\n",
    "\n",
    "    rows = []\n",
    "    expected_keys = [\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_vote_any\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "        \"episodes_as_primary\",\n",
    "    ]\n",
    "\n",
    "    for col in base_res:\n",
    "        stats = {\"sensor\": col}\n",
    "\n",
    "        # Model rates (per total rows)\n",
    "        stats[\"anomaly_rate_is\"]   = float(combined[\"is_anomaly\"].sum())   / max(total_rows, 1) if \"is_anomaly\"   in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_ae\"]   = float(combined[\"ae_is_anomaly\"].sum())/ max(total_rows, 1) if \"ae_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lof\"]  = float(combined[\"lof_is_anomaly\"].sum())/max(total_rows, 1) if \"lof_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lstm\"] = float(combined[\"lstm_is_anomaly\"].fillna(0).sum())/max(total_rows, 1) if \"lstm_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_hybrid\"] = float(combined[\"hybrid_is_anomaly\"].sum())/max(total_rows, 1) if \"hybrid_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote3p\"] = float(combined[\"vote_3plus\"].sum())/max(total_rows, 1) if \"vote_3plus\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote_any\"] = float(combined[\"vote_any\"].sum())/max(total_rows, 1) if \"vote_any\" in combined.columns else 0.0\n",
    "\n",
    "        # Mean/Max |residual| during voted anomalies\n",
    "        if col in combined.columns and voted_mask.any():\n",
    "            vals = combined.loc[voted_mask, col].abs()\n",
    "            stats[\"mean_abs_resid_voted\"] = float(vals.mean()) if not vals.empty else 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = float(vals.max())  if not vals.empty else 0.0\n",
    "        else:\n",
    "            stats[\"mean_abs_resid_voted\"] = 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = 0.0\n",
    "\n",
    "        # Episodes where this residual was primary\n",
    "        if episodes_with_reasons is not None and not episodes_with_reasons.empty and \"primary_signal\" in episodes_with_reasons.columns:\n",
    "            stats[\"episodes_as_primary\"] = int((episodes_with_reasons[\"primary_signal\"] == col).sum())\n",
    "        else:\n",
    "            stats[\"episodes_as_primary\"] = 0\n",
    "\n",
    "        for k in expected_keys:\n",
    "            stats.setdefault(k, 0.0)\n",
    "\n",
    "        rows.append(stats)\n",
    "\n",
    "    sensor_df = pd.DataFrame(rows)\n",
    "    for c in sensor_df.columns:\n",
    "        if c != \"sensor\":\n",
    "            sensor_df[c] = sensor_df[c].fillna(0.0)\n",
    "    return sensor_df\n",
    "\n",
    "\n",
    "def cluster_sensors(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    n_clusters: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    if sensor_df.empty or \"sensor\" not in sensor_df.columns:\n",
    "        return sensor_df, np.empty((0, 2)), np.empty((0, 2))\n",
    "\n",
    "    features = sensor_df.drop(columns=[\"sensor\"]).to_numpy(dtype=np.float32)\n",
    "    if features.shape[0] < n_clusters:\n",
    "        n_clusters = max(1, features.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(features)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(Z)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    Z2 = pca.fit_transform(Z)\n",
    "    centers2 = pca.transform(km.cluster_centers_)\n",
    "\n",
    "    out = sensor_df.copy()\n",
    "    out[\"cluster\"] = labels\n",
    "\n",
    "    return out, Z2, centers2\n",
    "\n",
    "\n",
    "def plot_sensor_bar_top(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metric: str = \"episodes_as_primary\",\n",
    "    top_n: int = 15,\n",
    "    title: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty or metric not in sensor_df.columns:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    df = sensor_df.sort_values(metric, ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(df)), df[metric])\n",
    "    plt.xticks(range(len(df)), [s.replace(\"Force_\", \"F_\") for s in df[\"sensor\"]], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title or f\"Top {top_n} sensors by {metric}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"top_sensors_{metric}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_clusters_scatter(\n",
    "    sensor_df_with_cluster: pd.DataFrame,\n",
    "    Z2: np.ndarray,\n",
    "    centers2: np.ndarray,\n",
    "    out_dir: str,\n",
    "    title: str = \"Sensor clusters (PCA of features)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df_with_cluster.empty or Z2.size == 0:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    clusters = sorted(sensor_df_with_cluster[\"cluster\"].unique().tolist())\n",
    "    for cl in clusters:\n",
    "        mask = sensor_df_with_cluster[\"cluster\"] == cl\n",
    "        pts = Z2[mask.values]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"cluster {cl}\", alpha=0.8, s=36)\n",
    "\n",
    "    if centers2.size:\n",
    "        plt.scatter(centers2[:, 0], centers2[:, 1], marker=\"X\", s=120, label=\"centers\")\n",
    "\n",
    "    try:\n",
    "        top_lab = sensor_df_with_cluster.sort_values(\"episodes_as_primary\", ascending=False).head(10).index\n",
    "        for idx in top_lab:\n",
    "            plt.text(Z2[idx, 0], Z2[idx, 1], sensor_df_with_cluster.loc[idx, \"sensor\"], fontsize=8)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_clusters_pca.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_heatmap(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    title: str = \"Sensor anomaly fingerprint (rates & magnitudes)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty:\n",
    "        return None\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    desired = [\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_vote_any\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "    ]\n",
    "    if metrics is None:\n",
    "        metrics = desired\n",
    "\n",
    "    available = [m for m in metrics if m in sensor_df.columns]\n",
    "    if not available:\n",
    "        print(\"âš ï¸ No requested heatmap metrics are present in sensor_df. Skipping heatmap.\")\n",
    "        return None\n",
    "    if len(available) < len(metrics):\n",
    "        missing = [m for m in metrics if m not in sensor_df.columns]\n",
    "        print(f\"â„¹ï¸ Skipping missing metrics in heatmap: {missing}\")\n",
    "    metrics = available\n",
    "\n",
    "    key_rank = \"episodes_as_primary\" if \"episodes_as_primary\" in sensor_df.columns else metrics[0]\n",
    "    keep = sensor_df.sort_values(key_rank, ascending=False).head(25)\n",
    "\n",
    "    M = keep[metrics].to_numpy(dtype=np.float32)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(M, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.yticks(range(len(keep)), keep[\"sensor\"])\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_fingerprint_heatmap.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Report\n",
    "# =========================================================\n",
    "def _overlay_episode_plot(df: pd.DataFrame, episode_row: pd.Series, cfg: dict, ax=None):\n",
    "    start, end = int(episode_row[\"start_idx\"]), int(episode_row[\"end_idx\"])\n",
    "    primary = episode_row.get(\"primary_signal\", \"\")\n",
    "    demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    t = np.arange(start, end + 1)\n",
    "    if primary in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, primary].values, label=f\"{primary}\", alpha=0.85)\n",
    "    if demand_col and demand_col in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, demand_col].values, label=f\"{demand_col}\", alpha=0.8)\n",
    "    if measured_col and measured_col in df.columns:\n",
    "        ax.plot(t, df.loc[start:end, measured_col].values, label=f\"{measured_col}\", alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_title(f\"Episode {start}â€“{end}\\nprimary={primary}\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def build_ops_report(\n",
    "    combined: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    sensor_df: pd.DataFrame,\n",
    "    episodes_scored: pd.DataFrame,\n",
    "    cfg: dict,\n",
    "    out_pdf_path: str\n",
    "):\n",
    "    ensure_dir(os.path.dirname(out_pdf_path))\n",
    "    with PdfPages(out_pdf_path) as pdf:\n",
    "\n",
    "        # Page 1 â€” Anomalies counts by model (dynamic columns)\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"] if c in summary.columns]\n",
    "        summary[plot_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomalies per Model per File\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 2 â€” Top sensors by episodes_as_primary\n",
    "        p1 = plot_sensor_bar_top(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"], metric=\"episodes_as_primary\", top_n=15,\n",
    "                                 title=\"Top sensors by episodes_as_primary\")\n",
    "        if p1 and os.path.exists(p1):\n",
    "            img = plt.imread(p1)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 3 â€” Sensor heatmap (if created)\n",
    "        p2 = plot_sensor_heatmap(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"])\n",
    "        if p2 and os.path.exists(p2):\n",
    "            img = plt.imread(p2)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Pages 4+ â€” Example episode overlays\n",
    "        if not episodes_scored.empty:\n",
    "            candidates = episodes_scored.copy()\n",
    "            if \"n_models_mean\" in candidates.columns:\n",
    "                candidates = candidates.sort_values([\"n_models_mean\"], ascending=False)\n",
    "            n_show = min(cfg[\"report\"][\"top_n_episodes\"], len(candidates))\n",
    "            for _, epi in candidates.head(n_show).iterrows():\n",
    "                plt.figure(figsize=(11, 5))\n",
    "                if \"source_file\" in combined.columns and \"source_file\" in epi:\n",
    "                    sub = combined.loc[combined[\"source_file\"] == epi[\"source_file\"]]\n",
    "                else:\n",
    "                    sub = combined\n",
    "                _overlay_episode_plot(sub, epi, cfg, ax=plt.gca())\n",
    "                hw = epi.get(\"hardware_class\", \"Unknown\")\n",
    "                why = epi.get(\"hardware_why\", \"\")\n",
    "                lag_s = epi.get(\"lag_seconds\", np.nan)\n",
    "                sat   = epi.get(\"saturation_score\", np.nan)\n",
    "                drift = epi.get(\"drift_score\", np.nan)\n",
    "                vibe  = epi.get(\"vibe_score\", np.nan)\n",
    "                txt = (\n",
    "                    f\"hardware: {hw}\\n\"\n",
    "                    f\"why: {why}\\n\"\n",
    "                    f\"lag_seconds: {lag_s:.4f}  |  saturation: {sat:.3f}  |  drift: {drift:.3f}  |  vibe: {vibe:.3f}\"\n",
    "                )\n",
    "                plt.gcf().text(0.02, 0.02, txt, ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(); plt.close()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Config (defaults or JSON)\n",
    "# =========================================================\n",
    "def default_config() -> dict:\n",
    "    return {\n",
    "        \"io\": {\n",
    "            \"input_folder\": \"./Datasets/Datasets\",\n",
    "            \"residual_folder\": \"./Anomaly_detection/residual_created/\",\n",
    "            \"output_folder\": \"./Anomaly_detection/code/outputs/\"\n",
    "        },\n",
    "        \"residuals\": {\n",
    "            \"enabled\": True,\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\",\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"suffix\": \"_residual\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"window\": 5,\n",
    "            \"max_features\": 500\n",
    "        },\n",
    "        \"threshold\": {\n",
    "            \"k\": 3.5\n",
    "        },\n",
    "        \"ae\": {\n",
    "            \"epochs\": 50,\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"seq_len\": 5,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"patience\": 5,\n",
    "            \"max_sequences\": 3000,\n",
    "            \"downsample\": 5\n",
    "        },\n",
    "        \"lof\": {\n",
    "            \"n_neighbors\": 20\n",
    "        },\n",
    "        \"hybrid\": {  # Hybrid scoring config\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"robust_z\",      # \"robust_z\" | \"percentile\"\n",
    "            \"min_components\": 2,       # require at least N model scores present\n",
    "            \"weights\": {               # relative importance (doesn't need to sum to 1)\n",
    "                \"iso_score\": 0.20,\n",
    "                \"lof_score\": 0.20,\n",
    "                \"ae_error\": 0.30,\n",
    "                \"lstm_error\": 0.30\n",
    "            }\n",
    "        },\n",
    "        \"hybrid_threshold\": {          # How to threshold hybrid_score\n",
    "            \"mode\": \"quantile\",        # \"mad\" or \"quantile\"\n",
    "            \"k\": 3.5,                  # used only if mode=\"mad\"\n",
    "            \"quantile\": 0.99           # top 1% as anomalies (fallback if MAD degenerates)\n",
    "        },\n",
    "        \"voting\": {\n",
    "            \"rule\": \"vote_3plus\",    # \"vote_3plus\" | \"agreement_all_4\" | \"any\"\n",
    "            \"min_gap\": 1\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_files\": None,\n",
    "            \"emit_rate_plot\": True     # also write an anomaly RATE bar chart (% rows)\n",
    "        },\n",
    "        \"runtime\": {\n",
    "            \"use_float32\": True\n",
    "        },\n",
    "        \"signals\": {\n",
    "            \"sample_rate_hz\": 100.0,     # set None if unknown\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\"\n",
    "        },\n",
    "        \"scores\": {\n",
    "            \"saturation_pct\": 95.0,\n",
    "            \"resid_prominence_pct\": 95.0,\n",
    "            \"min_window_len\": 5\n",
    "        },\n",
    "        \"report\": {\n",
    "            \"enabled\": True,\n",
    "            \"top_n_episodes\": 3\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_config_from_path_or_default(path: Optional[str]) -> dict:\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    print(\"â„¹ï¸  No --config provided or not found. Using in-memory default config.\")\n",
    "    return default_config()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Per-file processing & Pipeline\n",
    "# =========================================================\n",
    "def process_file(file_path: str, cfg: Dict, logger=print) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "\n",
    "    residual_cols = [c for c in df.columns if \"Residual\" in c]\n",
    "    if not residual_cols:\n",
    "        logger(f\"âŒ Skipped {file_name}: No residuals found.\")\n",
    "        return None\n",
    "\n",
    "    X, feature_cols, fe_stats = prepare_features(\n",
    "        df, residual_cols,\n",
    "        window=cfg[\"features\"][\"window\"],\n",
    "        max_features=cfg[\"features\"][\"max_features\"],\n",
    "        logger=logger,\n",
    "    )\n",
    "    if X is None or len(feature_cols) == 0 or X.empty:\n",
    "        logger(f\"âŒ Skipped {file_name}: invalid or empty features\")\n",
    "        return None\n",
    "\n",
    "    _, X_scaled, X_tensor = scale_features(X, use_float32=cfg[\"runtime\"][\"use_float32\"])\n",
    "\n",
    "    iso_labels, iso_scores, iso_thr = isolation_forest_detect(X_scaled, k=cfg[\"threshold\"][\"k\"])\n",
    "    df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
    "    df.loc[X.index, \"iso_score\"] = iso_scores\n",
    "    df.loc[X.index, \"iso_thr\"] = iso_thr\n",
    "\n",
    "    ae_labels, ae_errors, ae_thr = dense_autoencoder_detect(\n",
    "        X_tensor, k=cfg[\"threshold\"][\"k\"], ae_epochs=cfg[\"ae\"][\"epochs\"], ae_lr=cfg[\"ae\"][\"lr\"]\n",
    "    )\n",
    "    df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
    "    df.loc[X.index, \"ae_error\"] = ae_errors\n",
    "    df.loc[X.index, \"ae_thr\"] = ae_thr\n",
    "\n",
    "    lof_labels, lof_scores, lof_thr = lof_detect(\n",
    "        X_scaled, k=cfg[\"threshold\"][\"k\"], n_neighbors=cfg[\"lof\"][\"n_neighbors\"]\n",
    "    )\n",
    "    df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
    "    df.loc[X.index, \"lof_score\"] = lof_scores\n",
    "    df.loc[X.index, \"lof_thr\"] = lof_thr\n",
    "\n",
    "    lstm_labels, lstm_errors, lstm_idx, lstm_thr = lstm_autoencoder_detect(\n",
    "        X_scaled,\n",
    "        k=cfg[\"threshold\"][\"k\"],\n",
    "        seq_len=cfg[\"lstm\"][\"seq_len\"],\n",
    "        hidden_dim=cfg[\"lstm\"][\"hidden_dim\"],\n",
    "        patience=cfg[\"lstm\"][\"patience\"],\n",
    "        max_sequences=cfg[\"lstm\"][\"max_sequences\"],\n",
    "        downsample=cfg[\"lstm\"][\"downsample\"],\n",
    "    )\n",
    "    if len(lstm_idx) > 0:\n",
    "        df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
    "        df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
    "        df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
    "    else:\n",
    "        df[\"lstm_is_anomaly\"] = 0\n",
    "        df[\"lstm_error\"] = np.nan\n",
    "        df[\"lstm_thr\"] = np.nan\n",
    "\n",
    "    # --- Hybrid score (weighted fusion on valid rows)\n",
    "    mask_idx = X.index\n",
    "    df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
    "\n",
    "    hs = df.loc[mask_idx, \"hybrid_score\"].to_numpy()\n",
    "    if np.isnan(hs).all():\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = 0\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = np.nan\n",
    "    else:\n",
    "        mode = cfg.get(\"hybrid_threshold\", {}).get(\"mode\", \"mad\")\n",
    "        if mode == \"quantile\":\n",
    "            q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "            thr = np.nanpercentile(hs, 100 * q)\n",
    "            labels = (hs > thr).astype(int)\n",
    "        else:\n",
    "            thr, labels = robust_threshold(hs, k=cfg[\"hybrid_threshold\"].get(\"k\", 3.5), tail=\"high\")\n",
    "            # Fallback if too many positives (MAD degenerate)\n",
    "            if np.nanmean(labels) > 0.5:\n",
    "                q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "                thr = np.nanpercentile(hs, 100 * q)\n",
    "                labels = (hs > thr).astype(int)\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
    "\n",
    "    df = generate_votes(df)  # adds vote_3plus + vote_any\n",
    "    df[\"source_file\"] = file_name\n",
    "    df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
    "    df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n",
    "\n",
    "    logger(\n",
    "        f\"[{file_name}] iso={int(df['is_anomaly'].sum())} | \"\n",
    "        f\"ae={int(df['ae_is_anomaly'].sum())} | \"\n",
    "        f\"lof={int(df['lof_is_anomaly'].sum())} | \"\n",
    "        f\"lstm={int(df['lstm_is_anomaly'].fillna(0).sum())} | \"\n",
    "        f\"hyb={int(df['hybrid_is_anomaly'].sum())} | \"\n",
    "        f\"vote3+={int(df['vote_3plus'].sum())} | any={int(df['vote_any'].sum())}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Dict):\n",
    "    logger = print\n",
    "\n",
    "    # A) residuals (optional)\n",
    "    if cfg[\"residuals\"][\"enabled\"]:\n",
    "        logger(\"ðŸ”§ Creating residuals...\")\n",
    "        create_residuals_for_folder(\n",
    "            in_folder=cfg[\"io\"][\"input_folder\"],\n",
    "            out_folder=cfg[\"io\"][\"residual_folder\"],\n",
    "            demand_token=cfg[\"residuals\"][\"demand_token\"],\n",
    "            measured_token=cfg[\"residuals\"][\"measured_token\"],\n",
    "            residual_token=cfg[\"residuals\"][\"residual_token\"],\n",
    "            skip_if_exists=True,\n",
    "            suffix=cfg[\"residuals\"][\"suffix\"],\n",
    "            logger=logger,\n",
    "        )\n",
    "        data_folder = cfg[\"io\"][\"residual_folder\"]\n",
    "    else:\n",
    "        data_folder = cfg[\"io\"][\"input_folder\"]\n",
    "\n",
    "    # B) per-file\n",
    "    all_dfs = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            out = process_file(os.path.join(data_folder, file), cfg, logger=logger)\n",
    "            if out is not None:\n",
    "                all_dfs.append(out)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger(\"âŒ No files processed.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    ensure_dir(cfg[\"io\"][\"output_folder\"])\n",
    "\n",
    "    combined_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"combined_anomaly_results.csv\")\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "\n",
    "    # Summary (counts)\n",
    "    cols = [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"]\n",
    "    cols = [c for c in cols if c in combined.columns]\n",
    "    summary = combined.groupby(\"source_file\")[cols].sum()\n",
    "    summary[\"total_anomalies\"] = summary.sum(axis=1)\n",
    "    summary_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_summary.csv\")\n",
    "    summary.to_csv(summary_path)\n",
    "\n",
    "    logger(f\"âœ… Saved row-level: {combined_path}\")\n",
    "    logger(f\"âœ… Saved summary:   {summary_path}\")\n",
    "\n",
    "    # C) Counts plot (dynamic cols incl. Hybrid + Plain voting)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\",\"vote_any\"] if c in summary.columns]\n",
    "    summary[plot_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "    plt.title(\"Anomalies per Model per File\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_plot.png\")\n",
    "    plt.savefig(bar_path); plt.close()\n",
    "    logger(f\"ðŸ–¼ï¸ Saved: {bar_path}\")\n",
    "\n",
    "    # C2) Rate plot (percent of rows) for apples-to-apples comparison\n",
    "    if cfg.get(\"plots\", {}).get(\"emit_rate_plot\", True):\n",
    "        sizes = combined.groupby(\"source_file\").size().rename(\"n_rows\")\n",
    "        summary_rates = summary.div(sizes, axis=0) * 100.0\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        rate_cols = [c for c in plot_cols if c in summary_rates.columns]\n",
    "        summary_rates[rate_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "        plt.title(\"Anomaly RATE per Model per File (%)\")\n",
    "        plt.ylabel(\"Percent of rows (%)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        rate_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_rate_plot.png\")\n",
    "        plt.savefig(rate_path); plt.close()\n",
    "        logger(f\"ðŸ–¼ï¸ Saved: {rate_path}\")\n",
    "\n",
    "    # D) Voted rows + episodes + reasons\n",
    "    voted_rows = extract_voted_rows(combined, rule=cfg[\"voting\"][\"rule\"])\n",
    "    voted_dir = os.path.join(cfg[\"io\"][\"output_folder\"], \"voted_outputs\")\n",
    "    ensure_dir(voted_dir)\n",
    "    voted_rows_path = os.path.join(voted_dir, \"voted_anomalies_rows.csv\")\n",
    "    voted_rows.to_csv(voted_rows_path, index=False)\n",
    "\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=cfg[\"voting\"][\"min_gap\"])\n",
    "    episodes_path = os.path.join(voted_dir, \"voted_anomaly_episodes.csv\")\n",
    "    episodes.to_csv(episodes_path, index=False)\n",
    "\n",
    "    episodes_with_reasons = attach_episode_reasons(combined, episodes, top_k=1)\n",
    "    episodes_with_reasons = enrich_hardware_mapping(episodes_with_reasons)\n",
    "    episodes_scored = score_episodes(combined, episodes_with_reasons, cfg)\n",
    "\n",
    "    episodes_reason_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons.csv\")\n",
    "    episodes_scored_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons_and_scores.csv\")\n",
    "    episodes_with_reasons.to_csv(episodes_reason_path, index=False)\n",
    "    episodes_scored.to_csv(episodes_scored_path, index=False)\n",
    "    logger(f\"âœ… Saved episodes+reason: {episodes_reason_path}\")\n",
    "    logger(f\"âœ… Saved episodes+scores: {episodes_scored_path}\")\n",
    "\n",
    "    # E) Per-file plots with voted overlays (optional)\n",
    "    if cfg[\"plots\"][\"enabled\"]:\n",
    "        _ = plot_all_files(\n",
    "            combined_df=combined,\n",
    "            out_dir=voted_dir,\n",
    "            rule=cfg[\"voting\"][\"rule\"],\n",
    "            min_gap=cfg[\"voting\"][\"min_gap\"],\n",
    "            max_files=cfg[\"plots\"][\"max_files\"],\n",
    "        )\n",
    "\n",
    "    # F) Sensor table + clustering visuals\n",
    "    sensor_df = build_sensor_table(combined, voted_rows, episodes_with_reasons=episodes_with_reasons)\n",
    "    sensor_df_path = os.path.join(voted_dir, \"sensor_table.csv\")\n",
    "    sensor_df.to_csv(sensor_df_path, index=False)\n",
    "    logger(f\"âœ… Saved sensor table: {sensor_df_path}\")\n",
    "\n",
    "    clustered, Z2, centers2 = cluster_sensors(sensor_df, n_clusters=3, random_state=42)\n",
    "    _ = plot_sensor_clusters_scatter(clustered, Z2, centers2, out_dir=voted_dir)\n",
    "    _ = plot_sensor_heatmap(sensor_df, out_dir=voted_dir)\n",
    "    _ = plot_sensor_bar_top(sensor_df, out_dir=voted_dir, metric=\"episodes_as_primary\", top_n=15)\n",
    "\n",
    "    # G) PDF report\n",
    "    if cfg.get(\"report\", {}).get(\"enabled\", True):\n",
    "        pdf_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"ops_report.pdf\")\n",
    "        build_ops_report(\n",
    "            combined=combined,\n",
    "            summary=summary,\n",
    "            sensor_df=sensor_df,\n",
    "            episodes_scored=episodes_scored,\n",
    "            cfg=cfg,\n",
    "            out_pdf_path=pdf_path\n",
    "        )\n",
    "        logger(f\"ðŸ“„ Ops report saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Entrypoint\n",
    "# =========================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Anomaly Detection Product\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to config JSON\")\n",
    "    args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "    cfg = load_config_from_path_or_default(args.config)\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8483dee-87eb-4fd1-b36d-f479e7b30f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No --config provided or not found. Using in-memory default config.\n",
      "ðŸ”§ Creating residuals...\n",
      "â†©ï¸  Skip residual (exists): Dataset01_Ski_CrossbeamYawNotPerforming_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset02_Matrix_Rocker4EncoderNotWorking_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset03_Wushu_YawTrapezoidNormal_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset04_Wushu_YawWaveletSqueak_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset05_Wushu_LaneChanges_ModelBump_residual.csv\n",
      "âŒ Failed to read Dataset07_Demo_Spa_GT.csv: No columns to parse from file\n",
      "â†©ï¸  Skip residual (exists): Dataset08_Demo_Jiggler_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset09_Demo_VerticalChirp_residual.csv\n",
      "âŒ Failed to read Dataset10_Demo_MillbrookHills.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1269: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1273: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1274: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset01_Ski_CrossbeamYawNotPerforming_residual] iso=6081 | ae=11638 | lof=1724 | lstm=218 | hyb=1802 | vote3+=217\n",
      "âŒ Skipped Dataset02_Matrix_Rocker4EncoderNotWorking_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1269: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1273: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1274: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset03_Wushu_YawTrapezoidNormal_residual] iso=4353 | ae=15526 | lof=2735 | lstm=142 | hyb=3815 | vote3+=355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1269: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1273: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1274: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset04_Wushu_YawWaveletSqueak_residual] iso=5912 | ae=5678 | lof=1179 | lstm=721 | hyb=190 | vote3+=682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1269: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1273: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1274: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\426484469.py:1275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset05_Wushu_LaneChanges_ModelBump_residual] iso=7951 | ae=8057 | lof=1177 | lstm=336 | hyb=1312 | vote3+=119\n",
      "âŒ Skipped Dataset08_Demo_Jiggler_residual: No residuals found.\n",
      "âŒ Skipped Dataset09_Demo_VerticalChirp_residual: No residuals found.\n",
      "âœ… Saved row-level: ./Anomaly_detection/code/outputs/combined_anomaly_results.csv\n",
      "âœ… Saved summary:   ./Anomaly_detection/code/outputs/model_comparison_summary.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_plot.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_rate_plot.png\n",
      "âœ… Saved episodes+reason: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons.csv\n",
      "âœ… Saved episodes+scores: ./Anomaly_detection/code/outputs/voted_outputs\\voted_anomaly_episodes_with_reasons_and_scores.csv\n",
      "\n",
      "EPISODES PER FILE (after voting & scoring):\n",
      "source_file\n",
      "Dataset01_Ski_CrossbeamYawNotPerforming_residual    136\n",
      "Dataset03_Wushu_YawTrapezoidNormal_residual         200\n",
      "Dataset04_Wushu_YawWaveletSqueak_residual           292\n",
      "Dataset05_Wushu_LaneChanges_ModelBump_residual       58\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset01_Ski_CrossbeamYawNotPerforming_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset03_Wushu_YawTrapezoidNormal_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset04_Wushu_YawWaveletSqueak_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/voted_outputs\\voted_plot_Dataset05_Wushu_LaneChanges_ModelBump_residual.png\n",
      "âœ… Saved sensor table: ./Anomaly_detection/code/outputs/voted_outputs\\sensor_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Ops report saved: ./Anomaly_detection/code/outputs/ops_report.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Anomaly Detection Product (single script) â€” Hybrid + Episode fix (no vote_any)\n",
    "- Residual creation (Demand - Measured -> Residual)\n",
    "- Feature engineering (reuse if already present)\n",
    "- Scaling once -> shared across models\n",
    "- Models: IsolationForest, LOF, Dense AE, LSTM AE\n",
    "- Dynamic thresholds (MAD); Hybrid: MAD or quantile with fallback\n",
    "- Voting (3+) + episodes (merged runs, grouped per file âœ…)\n",
    "- Robust overlay plots with context padding (fixes blank episode pages âœ…)\n",
    "- Episode explanations + hardware mapping + root-cause scoring\n",
    "- Sensor ranking, clustering & heatmap\n",
    "- Multi-page PDF Ops Report (dynamic first page)\n",
    "- Emits a RATE plot (% rows) for fair cross-file comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utils\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in str(name))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Residual creation (optional)\n",
    "# =========================================================\n",
    "def create_residuals_for_folder(\n",
    "    in_folder: str,\n",
    "    out_folder: str,\n",
    "    demand_token: str = \"Demand\",\n",
    "    measured_token: str = \"Measured\",\n",
    "    residual_token: str = \"Residual\",\n",
    "    skip_if_exists: bool = True,\n",
    "    suffix: str = \"_residual\",\n",
    "    logger=print,\n",
    ") -> None:\n",
    "    ensure_dir(out_folder)\n",
    "    for file in os.listdir(in_folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(in_folder, file)\n",
    "        out_name = file.replace(\".csv\", f\"{suffix}.csv\")\n",
    "        out_path = os.path.join(out_folder, out_name)\n",
    "\n",
    "        if skip_if_exists and os.path.exists(out_path):\n",
    "            logger(f\"â†©ï¸  Skip residual (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(in_path)\n",
    "        except Exception as e:\n",
    "            logger(f\"âŒ Failed to read {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        made_any = False\n",
    "        for col in cols:\n",
    "            if demand_token in col:\n",
    "                measured_col = col.replace(demand_token, measured_token)\n",
    "                if measured_col in df.columns:\n",
    "                    residual_col = col.replace(demand_token, residual_token)\n",
    "                    df[residual_col] = df[col] - df[measured_col]\n",
    "                    made_any = True\n",
    "\n",
    "        if not made_any:\n",
    "            logger(f\"âš ï¸  No Demand/Measured pairs found in {file}.\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        logger(f\"âœ… Residual CSV saved: {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Scaling + robust threshold (MAD)\n",
    "# =========================================================\n",
    "def scale_features(X: pd.DataFrame, use_float32: bool = True):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    if use_float32:\n",
    "        X_scaled = X_scaled.astype(\"float32\")\n",
    "    X_tensor = torch.from_numpy(X_scaled)\n",
    "    return scaler, X_scaled, X_tensor\n",
    "\n",
    "\n",
    "def robust_threshold(\n",
    "    values: np.ndarray,\n",
    "    k: float = 3.5,\n",
    "    tail: str = \"high\",\n",
    "    min_anoms: int = 5,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    v = np.asarray(values)\n",
    "    mask = ~np.isnan(v)\n",
    "    v = v[mask]\n",
    "    if v.size == 0:\n",
    "        return (np.inf if tail == \"high\" else -np.inf), np.zeros_like(values, dtype=int)\n",
    "\n",
    "    med = np.median(v)\n",
    "    mad = np.median(np.abs(v - med)) + 1e-12\n",
    "    if tail == \"high\":\n",
    "        thr = med + k * 1.4826 * mad\n",
    "        labels = (values > thr).astype(int)\n",
    "    else:\n",
    "        thr = med - k * 1.4826 * mad\n",
    "        labels = (values < thr).astype(int)\n",
    "\n",
    "    # relax if too strict on large arrays\n",
    "    if labels.sum() < min_anoms and v.size >= 100:\n",
    "        for k_relax in (3.0, 2.5, 2.0):\n",
    "            if tail == \"high\":\n",
    "                thr = med + k_relax * 1.4826 * mad\n",
    "                labels = (values > thr).astype(int)\n",
    "            else:\n",
    "                thr = med - k_relax * 1.4826 * mad\n",
    "                labels = (values < thr).astype(int)\n",
    "            if labels.sum() >= min_anoms:\n",
    "                break\n",
    "\n",
    "    return thr, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Feature Engineering\n",
    "# =========================================================\n",
    "def prepare_features(\n",
    "    df: pd.DataFrame,\n",
    "    residual_cols: List[str],\n",
    "    window: int = 5,\n",
    "    max_features: int = 500,\n",
    "    logger=print,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:\n",
    "    already_done = any(f\"{residual_cols[0]}_delta\" in df.columns for _ in residual_cols)\n",
    "    stats = {\"reused\": 0, \"generated\": 0}\n",
    "\n",
    "    if already_done:\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if any(k in c for k in [\"Residual\", \"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "        ]\n",
    "        X = df[feature_cols].dropna()\n",
    "        stats[\"reused\"] = len(feature_cols)\n",
    "        logger(f\"ðŸ” Reusing {len(feature_cols)} engineered features.\")\n",
    "        return X, feature_cols, stats\n",
    "\n",
    "    # Generate\n",
    "    for col in residual_cols:\n",
    "        df[f\"{col}_delta\"] = df[col].diff()\n",
    "        df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
    "        df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in residual_cols:\n",
    "        feature_cols += [\n",
    "            col,\n",
    "            f\"{col}_delta\",\n",
    "            f\"{col}_rolling_mean_{window}\",\n",
    "            f\"{col}_rolling_std_{window}\",\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].dropna()\n",
    "    stats[\"generated\"] = len(feature_cols)\n",
    "    logger(f\"ðŸ› ï¸  Generated {len(feature_cols)} features (window={window}).\")\n",
    "\n",
    "    if X.shape[1] > max_features:\n",
    "        logger(f\"âŒ Too many features ({X.shape[1]} > {max_features}). Skipping file.\")\n",
    "        return pd.DataFrame(), [], stats\n",
    "\n",
    "    return X, feature_cols, stats\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Models\n",
    "# =========================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 32), nn.ReLU(), nn.Linear(32, 8))\n",
    "        self.decoder = nn.Sequential(nn.Linear(8, 32), nn.ReLU(), nn.Linear(32, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def dense_autoencoder_detect(\n",
    "    X_tensor: torch.Tensor, k: float, ae_epochs: int, ae_lr: float\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    model = Autoencoder(X_tensor.shape[1])\n",
    "    opt = optim.Adam(model.parameters(), lr=ae_lr)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    for _ in range(ae_epochs):\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = crit(out, X_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec = model(X_tensor)\n",
    "        errors = torch.mean((X_tensor - rec) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "    return labels.astype(int), errors, thr\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.encoder(x)  # [1, B, H]\n",
    "        repeated = h.repeat(x.size(1), 1, 1).transpose(0, 1)  # [B, T, H]\n",
    "        decoded, _ = self.decoder(repeated)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def make_sequences(X: np.ndarray, seq_len: int) -> Tuple[np.ndarray, List[int]]:\n",
    "    seqs, idxs = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seqs.append(X[i:i+seq_len])\n",
    "        idxs.append(i + seq_len - 1)\n",
    "    return np.array(seqs), idxs\n",
    "\n",
    "\n",
    "def lstm_autoencoder_detect(\n",
    "    X_scaled: np.ndarray,\n",
    "    k: float,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int,\n",
    "    patience: int,\n",
    "    max_sequences: int,\n",
    "    downsample: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[int], float]:\n",
    "    try:\n",
    "        Xds = X_scaled[::downsample]\n",
    "        if len(Xds) < seq_len:\n",
    "            return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "        Xseq, idxs = make_sequences(Xds, seq_len)\n",
    "        if len(Xseq) > max_sequences:\n",
    "            Xseq, idxs = Xseq[:max_sequences], idxs[:max_sequences]\n",
    "\n",
    "        Xt = torch.tensor(Xseq, dtype=torch.float32)\n",
    "        model = LSTMAutoencoder(Xt.shape[2], hidden_dim)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best, wait = float(\"inf\"), 0\n",
    "        for _ in range(100):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(Xt)\n",
    "            loss = crit(out, Xt)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if loss.item() < best:\n",
    "                best, wait = loss.item(), 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(Xt)\n",
    "            errors = torch.mean((Xt - out) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "        thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "        return labels.astype(int), errors, idxs, thr\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ LSTM memory error: {e}\")\n",
    "        return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "\n",
    "def isolation_forest_detect(X_scaled: np.ndarray, k: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    iso = IsolationForest(contamination=\"auto\", n_estimators=300, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "def lof_detect(X_scaled: np.ndarray, k: float, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=\"auto\")\n",
    "    _ = lof.fit_predict(X_scaled)  # populates negative_outlier_factor_\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hybrid scoring utilities\n",
    "# =========================================================\n",
    "def _robust_z_pos(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Right-tail robust z-score (>=0 when above median).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    z = np.where(np.isnan(z), np.nan, z)\n",
    "    return np.maximum(z, 0.0)  # only right tail counts as anomalous\n",
    "\n",
    "\n",
    "def _percentile01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map to [0,1] by robust percentiles (2â€“98). Values outside clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    lo = np.nanpercentile(x, 2)\n",
    "    hi = np.nanpercentile(x, 98)\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    y = (x - lo) / rng\n",
    "    return np.clip(y, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hybrid_score_on_mask(df: pd.DataFrame, cfg: dict, mask_idx) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute hybrid only on valid rows (mask_idx). Returns array the size of df,\n",
    "    NaN elsewhere. Requires >= min_components present.\n",
    "    \"\"\"\n",
    "    out = np.full(len(df), np.nan)\n",
    "    if not cfg.get(\"hybrid\", {}).get(\"enabled\", False):\n",
    "        return out\n",
    "\n",
    "    # Weights guard\n",
    "    wmap = cfg.get(\"hybrid\", {}).get(\"weights\")\n",
    "    if not isinstance(wmap, dict) or not wmap:\n",
    "        wmap = {\"iso_score\": 0.25, \"lof_score\": 0.25, \"ae_error\": 0.25, \"lstm_error\": 0.25}\n",
    "\n",
    "    method = cfg[\"hybrid\"].get(\"method\", \"robust_z\")\n",
    "    min_components = int(cfg[\"hybrid\"].get(\"min_components\", 2))\n",
    "\n",
    "    use = df.loc[mask_idx]  # restrict to valid feature rows\n",
    "\n",
    "    comps = [c for c in [\"iso_score\", \"lof_score\", \"ae_error\", \"lstm_error\"] if c in use.columns and c in wmap]\n",
    "    if not comps:\n",
    "        return out\n",
    "\n",
    "    parts = []\n",
    "    for c in comps:\n",
    "        arr = use[c].to_numpy(dtype=float)\n",
    "        if method == \"robust_z\":\n",
    "            norm = _robust_z_pos(arr)\n",
    "            norm = np.clip(norm, 0, 10.0) / 10.0  # compress extreme tails to ~[0,1]\n",
    "        else:\n",
    "            norm = _percentile01(arr)\n",
    "        parts.append((norm, float(wmap[c])))\n",
    "\n",
    "    num = np.zeros(len(use), dtype=float)\n",
    "    den = np.zeros(len(use), dtype=float)\n",
    "    present = np.zeros(len(use), dtype=int)\n",
    "\n",
    "    for norm, w in parts:\n",
    "        m = ~np.isnan(norm)\n",
    "        num[m] += w * norm[m]\n",
    "        den[m] += w\n",
    "        present[m] += 1\n",
    "\n",
    "    hybrid_local = np.where((den > 0) & (present >= min_components), num / den, np.nan)\n",
    "    out[np.asarray(mask_idx)] = hybrid_local\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Voting, episodes, explanations\n",
    "# =========================================================\n",
    "def generate_votes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"agreement_all_4\"] = (\n",
    "        (df.get(\"ae_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lof_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lstm_is_anomaly\", 0) == 1)\n",
    "    ).astype(int)\n",
    "    df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
    "    df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
    "    return df  # NOTE: vote_any removed\n",
    "\n",
    "\n",
    "def extract_voted_rows(df: pd.DataFrame, rule: str = \"vote_3plus\") -> pd.DataFrame:\n",
    "    if rule == \"vote_3plus\":\n",
    "        mask = df[\"vote_3plus\"] == 1\n",
    "    elif rule == \"agreement_all_4\":\n",
    "        mask = df[\"agreement_all_4\"] == 1\n",
    "    elif rule == \"any\":\n",
    "        mask = (\n",
    "            (df[\"ae_is_anomaly\"] == 1)\n",
    "            | (df[\"is_anomaly\"] == 1)\n",
    "            | (df[\"lof_is_anomaly\"] == 1)\n",
    "            | (df[\"lstm_is_anomaly\"] == 1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown rule: {rule}\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _group_runs(idxs: np.ndarray, min_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "    runs, start, prev = [], int(idxs[0]), int(idxs[0])\n",
    "    for i in idxs[1:]:\n",
    "        if int(i) - prev <= min_gap:\n",
    "            prev = int(i)\n",
    "            continue\n",
    "        runs.append((start, prev))\n",
    "        start = int(i); prev = int(i)\n",
    "    runs.append((start, prev))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def summarize_episodes(voted_df: pd.DataFrame, min_gap: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build episodes PER FILE (avoids cross-file index slices).\n",
    "    \"\"\"\n",
    "    if voted_df.empty:\n",
    "        cols = [\"source_file\", \"start_idx\", \"end_idx\", \"length\", \"n_models_mean\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    if \"source_file\" in voted_df.columns:\n",
    "        groups = voted_df.groupby(\"source_file\")\n",
    "    else:\n",
    "        groups = [(\"\", voted_df)]\n",
    "\n",
    "    for sf, g in groups:\n",
    "        idxs = g.index.to_numpy()\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "        runs = _group_runs(idxs, min_gap=min_gap)\n",
    "        for start, end in runs:\n",
    "            chunk = g.loc[start:end]\n",
    "            row = {\n",
    "                \"source_file\": sf,\n",
    "                \"start_idx\": int(start),\n",
    "                \"end_idx\": int(end),\n",
    "                \"length\": int(end - start + 1),\n",
    "                \"n_models_mean\": float(\n",
    "                    chunk[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1).mean()\n",
    "                ),\n",
    "            }\n",
    "            for c in [\"iso_score\", \"ae_error\", \"lof_score\", \"lstm_error\", \"hybrid_score\"]:\n",
    "                if c in chunk.columns:\n",
    "                    row[f\"{c}_max\"] = float(chunk[c].max())\n",
    "                    row[f\"{c}_mean\"] = float(chunk[c].mean())\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _base_residual_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def _models_string(chunk: pd.DataFrame) -> str:\n",
    "    model_cols = [c for c in [\"is_anomaly\", \"ae_is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\", \"hybrid_is_anomaly\"] if c in chunk.columns]\n",
    "    if not model_cols:\n",
    "        return \"no-model-flags\"\n",
    "    means = chunk[model_cols].mean()\n",
    "    active = [m.replace(\"_is_anomaly\", \"\").upper() for m, v in means.items() if v >= 0.5]\n",
    "    return \", \".join(active) if active else \"weak/isolated flags\"\n",
    "\n",
    "\n",
    "def attach_episode_reasons(\n",
    "    combined_df: pd.DataFrame, episodes_df: pd.DataFrame, top_k: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "\n",
    "    base_res = _base_residual_columns(combined_df)\n",
    "    if not base_res:\n",
    "        episodes_df[\"primary_signal\"] = \"\"\n",
    "        episodes_df[\"reason\"] = \"no residual columns present\"\n",
    "        episodes_df[\"suspected_sensor\"] = \"\"\n",
    "        return episodes_df\n",
    "\n",
    "    out = []\n",
    "    for _, epi in episodes_df.iterrows():\n",
    "        start, end = int(epi[\"start_idx\"]), int(epi[\"end_idx\"])\n",
    "        mask = combined_df[\"source_file\"] == epi[\"source_file\"] if \"source_file\" in combined_df.columns else slice(None)\n",
    "        chunk = combined_df.loc[mask].loc[start:end]\n",
    "\n",
    "        if chunk.empty:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"empty slice\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats = []\n",
    "        for col in base_res:\n",
    "            if col in chunk.columns:\n",
    "                stats.append((col, float(chunk[col].abs().max())))\n",
    "        if not stats:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"no residual stats\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_signal, primary_val = stats[:top_k][0]\n",
    "        models_str = _models_string(chunk)\n",
    "        measured_col = primary_signal.replace(\"Residual\", \"Measured\")\n",
    "        suspected = measured_col if (measured_col in combined_df.columns) else \"unknown-measured-sensor\"\n",
    "\n",
    "        epi[\"primary_signal\"] = primary_signal\n",
    "        epi[\"reason\"] = f\"max |{primary_signal}| = {primary_val:.3f}; models: {models_str}\"\n",
    "        epi[\"suspected_sensor\"] = suspected\n",
    "        out.append(epi)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hardware mapping + root cause scoring\n",
    "# =========================================================\n",
    "HARDWARE_MAP = [\n",
    "    (\"Force_\",         \"Actuator/LoadCell\",  \"Force didnâ€™t follow demand â†’ friction/lag/saturation/load-cell drift likely\"),\n",
    "    (\"Encoder_\",       \"Encoders/Alignment\", \"Pose/velocity mismatch â†’ quantization/missing counts/misalignment\"),\n",
    "    (\"Accelerometer_\", \"IMU/Accelerometer\",  \"Vibration bursts â†’ mounting/looseness/thermal drift\"),\n",
    "    (\"State_\",         \"Control/Timing\",     \"Requested vs achieved state diverged â†’ scheduler limits/controller windup\"),\n",
    "]\n",
    "\n",
    "def map_signal_to_hardware(primary_signal: str):\n",
    "    for needle, hw, why in HARDWARE_MAP:\n",
    "        if needle in primary_signal:\n",
    "            return hw, why\n",
    "    return \"Unknown\", \"No mapping rule matched\"\n",
    "\n",
    "\n",
    "def enrich_hardware_mapping(episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    episodes_df = episodes_df.copy()\n",
    "    episodes_df[\"hardware_class\"] = \"\"\n",
    "    episodes_df[\"hardware_why\"] = \"\"\n",
    "    for i, r in episodes_df.iterrows():\n",
    "        hw, why = map_signal_to_hardware(r.get(\"primary_signal\", \"\"))\n",
    "        episodes_df.at[i, \"hardware_class\"] = hw\n",
    "        episodes_df.at[i, \"hardware_why\"]   = why\n",
    "    return episodes_df\n",
    "\n",
    "\n",
    "def _paired_columns(primary_signal: str, cfg: dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "    resid_tok  = cfg[\"signals\"][\"residual_token\"]\n",
    "    demand_tok = cfg[\"signals\"][\"demand_token\"]\n",
    "    measured_tok = cfg[\"signals\"][\"measured_token\"]\n",
    "    if resid_tok not in primary_signal:\n",
    "        return None, None\n",
    "    demand_col   = primary_signal.replace(resid_tok, demand_tok)\n",
    "    measured_col = primary_signal.replace(resid_tok, measured_tok)\n",
    "    return demand_col, measured_col\n",
    "\n",
    "\n",
    "def _nan_ok(arr: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_correlation_lag(x: np.ndarray, y: np.ndarray, sample_rate_hz: Optional[float]) -> Tuple[float, int]:\n",
    "    x = _nan_ok(x); y = _nan_ok(y)\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return (np.nan, 0)\n",
    "    x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    x = np.nan_to_num(x);  y = np.nan_to_num(y)\n",
    "    corr = np.correlate(x, y, mode=\"full\")\n",
    "    lags = np.arange(-len(x)+1, len(x))\n",
    "    k = int(np.argmax(corr))\n",
    "    lag_samples = int(lags[k])\n",
    "    lag_seconds = lag_samples / sample_rate_hz if sample_rate_hz and sample_rate_hz > 0 else np.nan\n",
    "    return (lag_seconds, lag_samples)\n",
    "\n",
    "\n",
    "def _saturation_score(demand: np.ndarray, residual: np.ndarray, cfg: dict) -> float:\n",
    "    if len(demand) == 0 or len(residual) == 0:\n",
    "        return 0.0\n",
    "    p_dem = np.nanpercentile(demand, cfg[\"scores\"][\"saturation_pct\"])\n",
    "    p_res = np.nanpercentile(np.abs(residual), cfg[\"scores\"][\"resid_prominence_pct\"])\n",
    "    near_limit = demand >= p_dem\n",
    "    large_res  = np.abs(residual) >= p_res\n",
    "    both = np.logical_and(near_limit, large_res)\n",
    "    return float(np.nansum(both)) / max(1, len(demand))\n",
    "\n",
    "\n",
    "def _drift_score(residual: np.ndarray) -> float:\n",
    "    residual = _nan_ok(residual)\n",
    "    mu = float(np.nanmean(residual))\n",
    "    sd = float(np.nanstd(residual)) + 1e-9\n",
    "    return abs(mu) / sd\n",
    "\n",
    "\n",
    "def _vibration_score(signal: np.ndarray, sample_rate_hz: Optional[float]) -> float:\n",
    "    if not sample_rate_hz or sample_rate_hz <= 0 or len(signal) < 8:\n",
    "        return np.nan\n",
    "    sig = np.nan_to_num(signal - np.nanmean(signal))\n",
    "    fft = np.fft.rfft(sig)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1.0 / sample_rate_hz)\n",
    "    if len(freqs) == 0:\n",
    "        return np.nan\n",
    "    cutoff = 0.25 * (sample_rate_hz / 2.0)\n",
    "    mask_hi = freqs >= cutoff\n",
    "    num = float(np.nansum(power[mask_hi]))\n",
    "    den = float(np.nansum(power) + 1e-12)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def score_episodes(combined_df: pd.DataFrame, episodes_df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    out = episodes_df.copy()\n",
    "    sr = cfg[\"signals\"][\"sample_rate_hz\"]\n",
    "    min_len = cfg[\"scores\"][\"min_window_len\"]\n",
    "\n",
    "    if \"primary_signal\" not in out.columns:\n",
    "        out[\"primary_signal\"] = \"\"\n",
    "\n",
    "    for i, r in out.iterrows():\n",
    "        start, end = int(r[\"start_idx\"]), int(r[\"end_idx\"])\n",
    "        if end - start + 1 < min_len:\n",
    "            out.at[i, \"lag_seconds\"] = np.nan\n",
    "            out.at[i, \"lag_samples\"] = 0\n",
    "            out.at[i, \"saturation_score\"] = 0.0\n",
    "            out.at[i, \"drift_score\"] = 0.0\n",
    "            out.at[i, \"vibe_score\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        if \"source_file\" in combined_df.columns and \"source_file\" in out.columns and \"source_file\" in r:\n",
    "            chunk = combined_df.loc[(combined_df[\"source_file\"] == r[\"source_file\"])].loc[start:end]\n",
    "        else:\n",
    "            chunk = combined_df.loc[start:end]\n",
    "\n",
    "        primary = r.get(\"primary_signal\", \"\")\n",
    "        demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "\n",
    "        resid = chunk[primary].values if (primary in chunk.columns) else np.array([])\n",
    "        dem   = chunk[demand_col].values if (demand_col and demand_col in chunk.columns) else np.array([])\n",
    "        meas  = chunk[measured_col].values if (measured_col and measured_col in chunk.columns) else np.array([])\n",
    "\n",
    "        lag_s, lag_k = _cross_correlation_lag(dem, meas, sr) if (len(dem) and len(meas)) else (np.nan, 0)\n",
    "        sat_sc = _saturation_score(dem, resid, cfg) if (len(dem) and len(resid)) else 0.0\n",
    "        dr_sc  = _drift_score(resid) if len(resid) else 0.0\n",
    "        if \"Accelerometer_\" in primary and primary in chunk.columns:\n",
    "            vibe_sc = _vibration_score(chunk[primary].values, sr)\n",
    "        else:\n",
    "            vibe_sc = _vibration_score(resid, sr)\n",
    "\n",
    "        out.at[i, \"lag_seconds\"]       = lag_s\n",
    "        out.at[i, \"lag_samples\"]       = int(lag_k)\n",
    "        out.at[i, \"saturation_score\"]  = float(sat_sc)\n",
    "        out.at[i, \"drift_score\"]       = float(dr_sc)\n",
    "        out.at[i, \"vibe_score\"]        = float(vibe_sc) if vibe_sc == vibe_sc else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Plotting helpers (per-file voted overlays)\n",
    "# =========================================================\n",
    "def _pick_residual(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand = [c for c in df.columns if \"Residual\" in c and not any(t in c for t in [\"_delta\", \"_rolling_\"])]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def _slice_by_global_index(sub: pd.DataFrame, start: int, end: int, pad: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust slice of `sub` (filtered to one file) using global indices [start, end],\n",
    "    expanded by `pad` points on both sides. Works even when index has gaps.\n",
    "    \"\"\"\n",
    "    if sub.empty:\n",
    "        return sub\n",
    "    idx = sub.index.to_numpy()\n",
    "    i0 = np.searchsorted(idx, start, side=\"left\")\n",
    "    i1 = np.searchsorted(idx, end,   side=\"right\")\n",
    "    i0 = max(i0 - pad, 0)\n",
    "    i1 = min(i1 + pad, len(idx))\n",
    "    return sub.iloc[i0:i1]\n",
    "\n",
    "\n",
    "def plot_voted_for_file(\n",
    "    df_file: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    rule: str,\n",
    "    min_gap: int,\n",
    "    figsize: Tuple[int, int] = (12, 5),\n",
    ") -> Optional[str]:\n",
    "    ensure_dir(out_dir)\n",
    "    residual_col = _pick_residual(df_file)\n",
    "    if residual_col is None:\n",
    "        print(\"âš ï¸ No residual column to plot.\")\n",
    "        return None\n",
    "\n",
    "    voted_rows = extract_voted_rows(df_file, rule=rule)\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=min_gap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_file.index, df_file[residual_col], label=residual_col, alpha=0.85)\n",
    "\n",
    "    if not voted_rows.empty:\n",
    "        plt.scatter(voted_rows.index, voted_rows[residual_col], s=12, label=f\"Voted anomalies ({rule})\")\n",
    "\n",
    "    if not episodes.empty:\n",
    "        for _, r in episodes.iterrows():\n",
    "            plt.axvspan(r[\"start_idx\"], r[\"end_idx\"], alpha=0.15, label=\"Episode\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uniq, seen = [], set()\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in seen:\n",
    "                uniq.append((h, l)); seen.add(l)\n",
    "        handles, labels = zip(*uniq)\n",
    "        plt.legend(handles, labels)\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    sf = df_file[\"source_file\"].iloc[0] if \"source_file\" in df_file.columns else \"file\"\n",
    "    plt.title(f\"{sf} â€” Residual with voted anomalies & episodes\")\n",
    "    plt.xlabel(\"Index\"); plt.ylabel(residual_col)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"voted_plot_{safe_name(sf)}.png\")\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_all_files(combined_df: pd.DataFrame, out_dir: str, rule: str, min_gap: int, max_files: Optional[int] = None):\n",
    "    paths = []\n",
    "    if \"source_file\" not in combined_df.columns:\n",
    "        print(\"âš ï¸ combined_df missing 'source_file'.\")\n",
    "        return paths\n",
    "    groups = list(combined_df.groupby(\"source_file\"))\n",
    "    if max_files is not None:\n",
    "        groups = groups[:max_files]\n",
    "    for fname, df_file in groups:\n",
    "        p = plot_voted_for_file(df_file, out_dir=out_dir, rule=rule, min_gap=min_gap)\n",
    "        if p:\n",
    "            paths.append(p); print(f\"ðŸ–¼ï¸ Saved: {p}\")\n",
    "    if not paths:\n",
    "        print(\"âš ï¸ No plots produced.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Sensor attribution, clustering & heatmap\n",
    "# =========================================================\n",
    "def _residual_cols_base(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_sensor_table(\n",
    "    combined: pd.DataFrame,\n",
    "    voted_rows: pd.DataFrame,\n",
    "    episodes_with_reasons: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    base_res = _residual_cols_base(combined)\n",
    "    if not base_res:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(combined)\n",
    "    voted_mask = pd.Series(False, index=combined.index)\n",
    "    if not voted_rows.empty:\n",
    "        voted_mask.loc[voted_rows.index] = True\n",
    "\n",
    "    rows = []\n",
    "    expected_keys = [\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "        \"episodes_as_primary\",\n",
    "    ]\n",
    "\n",
    "    for col in base_res:\n",
    "        stats = {\"sensor\": col}\n",
    "\n",
    "        stats[\"anomaly_rate_is\"]   = float(combined[\"is_anomaly\"].sum())   / max(total_rows, 1) if \"is_anomaly\"   in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_ae\"]   = float(combined[\"ae_is_anomaly\"].sum())/ max(total_rows, 1) if \"ae_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lof\"]  = float(combined[\"lof_is_anomaly\"].sum())/max(total_rows, 1) if \"lof_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lstm\"] = float(combined[\"lstm_is_anomaly\"].fillna(0).sum())/max(total_rows, 1) if \"lstm_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_hybrid\"] = float(combined[\"hybrid_is_anomaly\"].sum())/max(total_rows, 1) if \"hybrid_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote3p\"] = float(combined[\"vote_3plus\"].sum())/max(total_rows, 1) if \"vote_3plus\" in combined.columns else 0.0\n",
    "\n",
    "        if col in combined.columns and voted_mask.any():\n",
    "            vals = combined.loc[voted_mask, col].abs()\n",
    "            stats[\"mean_abs_resid_voted\"] = float(vals.mean()) if not vals.empty else 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = float(vals.max())  if not vals.empty else 0.0\n",
    "        else:\n",
    "            stats[\"mean_abs_resid_voted\"] = 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = 0.0\n",
    "\n",
    "        if episodes_with_reasons is not None and not episodes_with_reasons.empty and \"primary_signal\" in episodes_with_reasons.columns:\n",
    "            stats[\"episodes_as_primary\"] = int((episodes_with_reasons[\"primary_signal\"] == col).sum())\n",
    "        else:\n",
    "            stats[\"episodes_as_primary\"] = 0\n",
    "\n",
    "        for k in expected_keys:\n",
    "            stats.setdefault(k, 0.0)\n",
    "\n",
    "        rows.append(stats)\n",
    "\n",
    "    sensor_df = pd.DataFrame(rows)\n",
    "    for c in sensor_df.columns:\n",
    "        if c != \"sensor\":\n",
    "            sensor_df[c] = sensor_df[c].fillna(0.0)\n",
    "    return sensor_df\n",
    "\n",
    "\n",
    "def cluster_sensors(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    n_clusters: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    if sensor_df.empty or \"sensor\" not in sensor_df.columns:\n",
    "        return sensor_df, np.empty((0, 2)), np.empty((0, 2))\n",
    "\n",
    "    features = sensor_df.drop(columns=[\"sensor\"]).to_numpy(dtype=np.float32)\n",
    "    if features.shape[0] < n_clusters:\n",
    "        n_clusters = max(1, features.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(features)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(Z)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    Z2 = pca.fit_transform(Z)\n",
    "    centers2 = pca.transform(km.cluster_centers_)\n",
    "\n",
    "    out = sensor_df.copy()\n",
    "    out[\"cluster\"] = labels\n",
    "\n",
    "    return out, Z2, centers2\n",
    "\n",
    "\n",
    "def plot_sensor_bar_top(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metric: str = \"episodes_as_primary\",\n",
    "    top_n: int = 15,\n",
    "    title: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty or metric not in sensor_df.columns:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    df = sensor_df.sort_values(metric, ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(df)), df[metric])\n",
    "    plt.xticks(range(len(df)), [s.replace(\"Force_\", \"F_\") for s in df[\"sensor\"]], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title or f\"Top {top_n} sensors by {metric}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"top_sensors_{metric}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_clusters_scatter(\n",
    "    sensor_df_with_cluster: pd.DataFrame,\n",
    "    Z2: np.ndarray,\n",
    "    centers2: np.ndarray,\n",
    "    out_dir: str,\n",
    "    title: str = \"Sensor clusters (PCA of features)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df_with_cluster.empty or Z2.size == 0:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    clusters = sorted(sensor_df_with_cluster[\"cluster\"].unique().tolist())\n",
    "    for cl in clusters:\n",
    "        mask = sensor_df_with_cluster[\"cluster\"] == cl\n",
    "        pts = Z2[mask.values]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"cluster {cl}\", alpha=0.8, s=36)\n",
    "\n",
    "    if centers2.size:\n",
    "        plt.scatter(centers2[:, 0], centers2[:, 1], marker=\"X\", s=120, label=\"centers\")\n",
    "\n",
    "    try:\n",
    "        top_lab = sensor_df_with_cluster.sort_values(\"episodes_as_primary\", ascending=False).head(10).index\n",
    "        for idx in top_lab:\n",
    "            plt.text(Z2[idx, 0], Z2[idx, 1], sensor_df_with_cluster.loc[idx, \"sensor\"], fontsize=8)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_clusters_pca.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_heatmap(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    title: str = \"Sensor anomaly fingerprint (rates & magnitudes)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty:\n",
    "        return None\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    desired = [\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "    ]\n",
    "    if metrics is None:\n",
    "        metrics = desired\n",
    "\n",
    "    available = [m for m in metrics if m in sensor_df.columns]\n",
    "    if not available:\n",
    "        print(\"âš ï¸ No requested heatmap metrics are present in sensor_df. Skipping heatmap.\")\n",
    "        return None\n",
    "    if len(available) < len(metrics):\n",
    "        missing = [m for m in metrics if m not in sensor_df.columns]\n",
    "        print(f\"â„¹ï¸ Skipping missing metrics in heatmap: {missing}\")\n",
    "    metrics = available\n",
    "\n",
    "    key_rank = \"episodes_as_primary\" if \"episodes_as_primary\" in sensor_df.columns else metrics[0]\n",
    "    keep = sensor_df.sort_values(key_rank, ascending=False).head(25)\n",
    "\n",
    "    M = keep[metrics].to_numpy(dtype=np.float32)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(M, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.yticks(range(len(keep)), keep[\"sensor\"])\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_fingerprint_heatmap.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Report (robust overlays with padding)\n",
    "# =========================================================\n",
    "def _overlay_episode_plot(df: pd.DataFrame, episode_row: pd.Series, cfg: dict, ax=None):\n",
    "    start, end = int(episode_row[\"start_idx\"]), int(episode_row[\"end_idx\"])\n",
    "    primary = episode_row.get(\"primary_signal\", \"\")\n",
    "    demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "    pad = int(cfg.get(\"report\", {}).get(\"pad_points\", 100))\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # If df contains multiple files, filter to the right one first\n",
    "    if \"source_file\" in df.columns and \"source_file\" in episode_row:\n",
    "        df = df.loc[df[\"source_file\"] == episode_row[\"source_file\"]]\n",
    "\n",
    "    # Robust slice by global indices + padding\n",
    "    window = _slice_by_global_index(df, start, end, pad=pad)\n",
    "    if window.empty:\n",
    "        ax.set_title(f\"Episode {start}â€“{end} (EMPTY SLICE)\")\n",
    "        return\n",
    "\n",
    "    t = window.index.to_numpy()\n",
    "\n",
    "    if primary in window.columns:\n",
    "        ax.plot(t, window[primary].values, label=f\"{primary}\", alpha=0.9)\n",
    "    if demand_col and demand_col in window.columns:\n",
    "        ax.plot(t, window[demand_col].values, label=f\"{demand_col}\", alpha=0.8)\n",
    "    if measured_col and measured_col in window.columns:\n",
    "        ax.plot(t, window[measured_col].values, label=f\"{measured_col}\", alpha=0.8)\n",
    "\n",
    "    # Shade the exact episode span inside the padded window\n",
    "    ax.axvspan(start, end, alpha=0.15, label=\"episode window\")\n",
    "\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_title(f\"Episode {start}â€“{end}\\nprimary={primary}\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def build_ops_report(\n",
    "    combined: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    sensor_df: pd.DataFrame,\n",
    "    episodes_scored: pd.DataFrame,\n",
    "    cfg: dict,\n",
    "    out_pdf_path: str\n",
    "):\n",
    "    ensure_dir(os.path.dirname(out_pdf_path))\n",
    "    with PdfPages(out_pdf_path) as pdf:\n",
    "\n",
    "        # Page 1 â€” Anomalies counts by model (dynamic columns, no vote_any)\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "        summary[plot_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomalies per Model per File\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 2 â€” Top sensors by episodes_as_primary\n",
    "        p1 = plot_sensor_bar_top(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"], metric=\"episodes_as_primary\", top_n=15,\n",
    "                                 title=\"Top sensors by episodes_as_primary\")\n",
    "        if p1 and os.path.exists(p1):\n",
    "            img = plt.imread(p1)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 3 â€” Sensor heatmap (if created)\n",
    "        p2 = plot_sensor_heatmap(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"])\n",
    "        if p2 and os.path.exists(p2):\n",
    "            img = plt.imread(p2)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Pages 4+ â€” Episode overlays (per-file selection with caps)\n",
    "        if not episodes_scored.empty:\n",
    "            candidates = episodes_scored.copy()\n",
    "            sort_keys = [c for c in [\"n_models_mean\", \"hybrid_score_mean\", \"iso_score_mean\", \"ae_error_mean\"] if c in candidates.columns]\n",
    "            if sort_keys:\n",
    "                candidates = candidates.sort_values(sort_keys, ascending=False)\n",
    "\n",
    "            n_per_file = int(cfg.get(\"report\", {}).get(\"top_n_per_file\", 2))\n",
    "            max_pages  = int(cfg.get(\"report\", {}).get(\"max_pages\", 12))\n",
    "\n",
    "            pages = 0\n",
    "            for sf, grp in candidates.groupby(\"source_file\"):\n",
    "                for _, epi in grp.head(n_per_file).iterrows():\n",
    "                    if pages >= max_pages:\n",
    "                        break\n",
    "                    plt.figure(figsize=(11, 5))\n",
    "                    _overlay_episode_plot(combined, epi, cfg, ax=plt.gca())\n",
    "                    hw = epi.get(\"hardware_class\", \"Unknown\")\n",
    "                    why = epi.get(\"hardware_why\", \"\")\n",
    "                    lag_s = epi.get(\"lag_seconds\", np.nan)\n",
    "                    sat   = epi.get(\"saturation_score\", np.nan)\n",
    "                    drift = epi.get(\"drift_score\", np.nan)\n",
    "                    vibe  = epi.get(\"vibe_score\", np.nan)\n",
    "                    txt = (\n",
    "                        f\"hardware: {hw}\\n\"\n",
    "                        f\"why: {why}\\n\"\n",
    "                        f\"lag_seconds: {lag_s:.4f}  |  saturation: {sat:.3f}  |  drift: {drift:.3f}  |  vibe: {vibe:.3f}\"\n",
    "                    )\n",
    "                    plt.gcf().text(0.02, 0.02, txt, ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(); plt.close()\n",
    "                    pages += 1\n",
    "                if pages >= max_pages:\n",
    "                    break\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Config (defaults or JSON)\n",
    "# =========================================================\n",
    "def default_config() -> dict:\n",
    "    return {\n",
    "        \"io\": {\n",
    "            \"input_folder\": \"./Datasets/Datasets\",\n",
    "            \"residual_folder\": \"./Anomaly_detection/residual_created/\",\n",
    "            \"output_folder\": \"./Anomaly_detection/code/outputs/\"\n",
    "        },\n",
    "        \"residuals\": {\n",
    "            \"enabled\": True,\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\",\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"suffix\": \"_residual\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"window\": 5,\n",
    "            \"max_features\": 500\n",
    "        },\n",
    "        \"threshold\": {\n",
    "            \"k\": 3.5\n",
    "        },\n",
    "        \"ae\": {\n",
    "            \"epochs\": 50,\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"seq_len\": 5,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"patience\": 5,\n",
    "            \"max_sequences\": 3000,\n",
    "            \"downsample\": 5\n",
    "        },\n",
    "        \"lof\": {\n",
    "            \"n_neighbors\": 20\n",
    "        },\n",
    "        \"hybrid\": {                     # Hybrid scoring config\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"robust_z\",      # \"robust_z\" | \"percentile\"\n",
    "            \"min_components\": 2,       # require at least N model scores present\n",
    "            \"weights\": {               # relative importance (doesn't need to sum to 1)\n",
    "                \"iso_score\": 0.20,\n",
    "                \"lof_score\": 0.20,\n",
    "                \"ae_error\": 0.30,\n",
    "                \"lstm_error\": 0.30\n",
    "            }\n",
    "        },\n",
    "        \"hybrid_threshold\": {          # How to threshold hybrid_score\n",
    "            \"mode\": \"quantile\",        # \"mad\" or \"quantile\"\n",
    "            \"k\": 3.5,                  # used only if mode=\"mad\"\n",
    "            \"quantile\": 0.99           # top 1% as anomalies (fallback if MAD degenerates)\n",
    "        },\n",
    "        \"voting\": {\n",
    "            \"rule\": \"vote_3plus\",    # \"vote_3plus\" | \"agreement_all_4\" | \"any\"\n",
    "            \"min_gap\": 1\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_files\": None,\n",
    "            \"emit_rate_plot\": True     # also write an anomaly RATE bar chart (% rows)\n",
    "        },\n",
    "        \"runtime\": {\n",
    "            \"use_float32\": True\n",
    "        },\n",
    "        \"signals\": {\n",
    "            \"sample_rate_hz\": 100.0,     # set None if unknown\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\"\n",
    "        },\n",
    "        \"scores\": {\n",
    "            \"saturation_pct\": 95.0,\n",
    "            \"resid_prominence_pct\": 95.0,\n",
    "            \"min_window_len\": 5\n",
    "        },\n",
    "        \"report\": {\n",
    "            \"enabled\": True,\n",
    "            \"top_n_episodes\": 3,     # still used elsewhere; harmless\n",
    "            \"top_n_per_file\": 2,     # NEW: how many episodes per file to show\n",
    "            \"max_pages\": 12,         # NEW: cap to avoid huge PDFs\n",
    "            \"pad_points\": 100        # NEW: context on each side of an episode in plots\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_config_from_path_or_default(path: Optional[str]) -> dict:\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    print(\"â„¹ï¸  No --config provided or not found. Using in-memory default config.\")\n",
    "    return default_config()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Per-file processing & Pipeline\n",
    "# =========================================================\n",
    "def process_file(file_path: str, cfg: Dict, logger=print) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "\n",
    "    residual_cols = [c for c in df.columns if \"Residual\" in c]\n",
    "    if not residual_cols:\n",
    "        logger(f\"âŒ Skipped {file_name}: No residuals found.\")\n",
    "        return None\n",
    "\n",
    "    X, feature_cols, fe_stats = prepare_features(\n",
    "        df, residual_cols,\n",
    "        window=cfg[\"features\"][\"window\"],\n",
    "        max_features=cfg[\"features\"][\"max_features\"],\n",
    "        logger=logger,\n",
    "    )\n",
    "    if X is None or len(feature_cols) == 0 or X.empty:\n",
    "        logger(f\"âŒ Skipped {file_name}: invalid or empty features\")\n",
    "        return None\n",
    "\n",
    "    _, X_scaled, X_tensor = scale_features(X, use_float32=cfg[\"runtime\"][\"use_float32\"])\n",
    "\n",
    "    iso_labels, iso_scores, iso_thr = isolation_forest_detect(X_scaled, k=cfg[\"threshold\"][\"k\"])\n",
    "    df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
    "    df.loc[X.index, \"iso_score\"] = iso_scores\n",
    "    df.loc[X.index, \"iso_thr\"] = iso_thr\n",
    "\n",
    "    ae_labels, ae_errors, ae_thr = dense_autoencoder_detect(\n",
    "        X_tensor, k=cfg[\"threshold\"][\"k\"], ae_epochs=cfg[\"ae\"][\"epochs\"], ae_lr=cfg[\"ae\"][\"lr\"]\n",
    "    )\n",
    "    df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
    "    df.loc[X.index, \"ae_error\"] = ae_errors\n",
    "    df.loc[X.index, \"ae_thr\"] = ae_thr\n",
    "\n",
    "    lof_labels, lof_scores, lof_thr = lof_detect(\n",
    "        X_scaled, k=cfg[\"threshold\"][\"k\"], n_neighbors=cfg[\"lof\"][\"n_neighbors\"]\n",
    "    )\n",
    "    df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
    "    df.loc[X.index, \"lof_score\"] = lof_scores\n",
    "    df.loc[X.index, \"lof_thr\"] = lof_thr\n",
    "\n",
    "    lstm_labels, lstm_errors, lstm_idx, lstm_thr = lstm_autoencoder_detect(\n",
    "        X_scaled,\n",
    "        k=cfg[\"threshold\"][\"k\"],\n",
    "        seq_len=cfg[\"lstm\"][\"seq_len\"],\n",
    "        hidden_dim=cfg[\"lstm\"][\"hidden_dim\"],\n",
    "        patience=cfg[\"lstm\"][\"patience\"],\n",
    "        max_sequences=cfg[\"lstm\"][\"max_sequences\"],\n",
    "        downsample=cfg[\"lstm\"][\"downsample\"],\n",
    "    )\n",
    "    if len(lstm_idx) > 0:\n",
    "        df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
    "        df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
    "        df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
    "    else:\n",
    "        df[\"lstm_is_anomaly\"] = 0\n",
    "        df[\"lstm_error\"] = np.nan\n",
    "        df[\"lstm_thr\"] = np.nan\n",
    "\n",
    "    # --- Hybrid score (weighted fusion on valid rows)\n",
    "    mask_idx = X.index\n",
    "    df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
    "\n",
    "    hs = df.loc[mask_idx, \"hybrid_score\"].to_numpy()\n",
    "    if np.isnan(hs).all():\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = 0\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = np.nan\n",
    "    else:\n",
    "        mode = cfg.get(\"hybrid_threshold\", {}).get(\"mode\", \"mad\")\n",
    "        if mode == \"quantile\":\n",
    "            q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "            thr = np.nanpercentile(hs, 100 * q)\n",
    "            labels = (hs > thr).astype(int)\n",
    "        else:\n",
    "            thr, labels = robust_threshold(hs, k=cfg[\"hybrid_threshold\"].get(\"k\", 3.5), tail=\"high\")\n",
    "            # Fallback if too many positives (MAD degenerate)\n",
    "            if np.nanmean(labels) > 0.5:\n",
    "                q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "                thr = np.nanpercentile(hs, 100 * q)\n",
    "                labels = (hs > thr).astype(int)\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
    "\n",
    "    df = generate_votes(df)  # adds vote_3plus (no vote_any)\n",
    "    df[\"source_file\"] = file_name\n",
    "    df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
    "    df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n",
    "\n",
    "    logger(\n",
    "        f\"[{file_name}] iso={int(df['is_anomaly'].sum())} | \"\n",
    "        f\"ae={int(df['ae_is_anomaly'].sum())} | \"\n",
    "        f\"lof={int(df['lof_is_anomaly'].sum())} | \"\n",
    "        f\"lstm={int(df['lstm_is_anomaly'].fillna(0).sum())} | \"\n",
    "        f\"hyb={int(df['hybrid_is_anomaly'].sum())} | \"\n",
    "        f\"vote3+={int(df['vote_3plus'].sum())}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Dict):\n",
    "    logger = print\n",
    "\n",
    "    # A) residuals (optional)\n",
    "    if cfg[\"residuals\"][\"enabled\"]:\n",
    "        logger(\"ðŸ”§ Creating residuals...\")\n",
    "        create_residuals_for_folder(\n",
    "            in_folder=cfg[\"io\"][\"input_folder\"],\n",
    "            out_folder=cfg[\"io\"][\"residual_folder\"],\n",
    "            demand_token=cfg[\"residuals\"][\"demand_token\"],\n",
    "            measured_token=cfg[\"residuals\"][\"measured_token\"],\n",
    "            residual_token=cfg[\"residuals\"][\"residual_token\"],\n",
    "            skip_if_exists=True,\n",
    "            suffix=cfg[\"residuals\"][\"suffix\"],\n",
    "            logger=logger,\n",
    "        )\n",
    "        data_folder = cfg[\"io\"][\"residual_folder\"]\n",
    "    else:\n",
    "        data_folder = cfg[\"io\"][\"input_folder\"]\n",
    "\n",
    "    # B) per-file\n",
    "    all_dfs = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            out = process_file(os.path.join(data_folder, file), cfg, logger=logger)\n",
    "            if out is not None:\n",
    "                all_dfs.append(out)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger(\"âŒ No files processed.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    ensure_dir(cfg[\"io\"][\"output_folder\"])\n",
    "\n",
    "    combined_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"combined_anomaly_results.csv\")\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "\n",
    "    # Summary (counts) â€” no vote_any\n",
    "    cols = [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"]\n",
    "    cols = [c for c in cols if c in combined.columns]\n",
    "    summary = combined.groupby(\"source_file\")[cols].sum()\n",
    "    summary[\"total_anomalies\"] = summary.sum(axis=1)\n",
    "    summary_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_summary.csv\")\n",
    "    summary.to_csv(summary_path)\n",
    "\n",
    "    logger(f\"âœ… Saved row-level: {combined_path}\")\n",
    "    logger(f\"âœ… Saved summary:   {summary_path}\")\n",
    "\n",
    "    # C) Counts plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "    summary[plot_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "    plt.title(\"Anomalies per Model per File\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_plot.png\")\n",
    "    plt.savefig(bar_path); plt.close()\n",
    "    logger(f\"ðŸ–¼ï¸ Saved: {bar_path}\")\n",
    "\n",
    "    # C2) Rate plot (% rows)\n",
    "    if cfg.get(\"plots\", {}).get(\"emit_rate_plot\", True):\n",
    "        sizes = combined.groupby(\"source_file\").size().rename(\"n_rows\")\n",
    "        summary_rates = summary.div(sizes, axis=0) * 100.0\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        rate_cols = [c for c in plot_cols if c in summary_rates.columns]\n",
    "        summary_rates[rate_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "        plt.title(\"Anomaly RATE per Model per File (%)\")\n",
    "        plt.ylabel(\"Percent of rows (%)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        rate_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_rate_plot.png\")\n",
    "        plt.savefig(rate_path); plt.close()\n",
    "        logger(f\"ðŸ–¼ï¸ Saved: {rate_path}\")\n",
    "\n",
    "    # D) Voted rows + episodes + reasons\n",
    "    voted_rows = extract_voted_rows(combined, rule=cfg[\"voting\"][\"rule\"])\n",
    "    voted_dir = os.path.join(cfg[\"io\"][\"output_folder\"], \"voted_outputs\")\n",
    "    ensure_dir(voted_dir)\n",
    "    voted_rows_path = os.path.join(voted_dir, \"voted_anomalies_rows.csv\")\n",
    "    voted_rows.to_csv(voted_rows_path, index=False)\n",
    "\n",
    "    episodes = summarize_episodes(voted_rows, min_gap=cfg[\"voting\"][\"min_gap\"])\n",
    "    episodes_path = os.path.join(voted_dir, \"voted_anomaly_episodes.csv\")\n",
    "    episodes.to_csv(episodes_path, index=False)\n",
    "\n",
    "    episodes_with_reasons = attach_episode_reasons(combined, episodes, top_k=1)\n",
    "    episodes_with_reasons = enrich_hardware_mapping(episodes_with_reasons)\n",
    "    episodes_scored = score_episodes(combined, episodes_with_reasons, cfg)\n",
    "\n",
    "    episodes_reason_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons.csv\")\n",
    "    episodes_scored_path = os.path.join(voted_dir, \"voted_anomaly_episodes_with_reasons_and_scores.csv\")\n",
    "    episodes_with_reasons.to_csv(episodes_reason_path, index=False)\n",
    "    episodes_scored.to_csv(episodes_scored_path, index=False)\n",
    "    logger(f\"âœ… Saved episodes+reason: {episodes_reason_path}\")\n",
    "    logger(f\"âœ… Saved episodes+scores: {episodes_scored_path}\")\n",
    "\n",
    "    # Debug: how many episodes per file\n",
    "    print(\"\\nEPISODES PER FILE (after voting & scoring):\")\n",
    "    if not episodes_scored.empty:\n",
    "        print(episodes_scored.groupby(\"source_file\").size().to_string())\n",
    "    else:\n",
    "        print(\"No episodes found under current voting rule.\")\n",
    "\n",
    "    # E) Per-file plots with voted overlays (optional)\n",
    "    if cfg[\"plots\"][\"enabled\"]:\n",
    "        _ = plot_all_files(\n",
    "            combined_df=combined,\n",
    "            out_dir=voted_dir,\n",
    "            rule=cfg[\"voting\"][\"rule\"],\n",
    "            min_gap=cfg[\"voting\"][\"min_gap\"],\n",
    "            max_files=cfg[\"plots\"][\"max_files\"],\n",
    "        )\n",
    "\n",
    "    # F) Sensor table + clustering visuals\n",
    "    sensor_df = build_sensor_table(combined, voted_rows, episodes_with_reasons=episodes_with_reasons)\n",
    "    sensor_df_path = os.path.join(voted_dir, \"sensor_table.csv\")\n",
    "    sensor_df.to_csv(sensor_df_path, index=False)\n",
    "    logger(f\"âœ… Saved sensor table: {sensor_df_path}\")\n",
    "\n",
    "    clustered, Z2, centers2 = cluster_sensors(sensor_df, n_clusters=3, random_state=42)\n",
    "    _ = plot_sensor_clusters_scatter(clustered, Z2, centers2, out_dir=voted_dir)\n",
    "    _ = plot_sensor_heatmap(sensor_df, out_dir=voted_dir)\n",
    "    _ = plot_sensor_bar_top(sensor_df, out_dir=voted_dir, metric=\"episodes_as_primary\", top_n=15)\n",
    "\n",
    "    # G) PDF report\n",
    "    if cfg.get(\"report\", {}).get(\"enabled\", True):\n",
    "        pdf_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"ops_report.pdf\")\n",
    "        build_ops_report(\n",
    "            combined=combined,\n",
    "            summary=summary,\n",
    "            sensor_df=sensor_df,\n",
    "            episodes_scored=episodes_scored,\n",
    "            cfg=cfg,\n",
    "            out_pdf_path=pdf_path\n",
    "        )\n",
    "        logger(f\"ðŸ“„ Ops report saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Entrypoint\n",
    "# =========================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Anomaly Detection Product\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to config JSON\")\n",
    "    args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "    cfg = load_config_from_path_or_default(args.config)\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd79840-b0db-4ddd-8367-301f3f598c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No --config provided or not found. Using in-memory default config.\n",
      "ðŸ”§ Creating residuals...\n",
      "â†©ï¸  Skip residual (exists): Dataset01_Ski_CrossbeamYawNotPerforming_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset02_Matrix_Rocker4EncoderNotWorking_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset03_Wushu_YawTrapezoidNormal_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset04_Wushu_YawWaveletSqueak_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset05_Wushu_LaneChanges_ModelBump_residual.csv\n",
      "âŒ Failed to read Dataset07_Demo_Spa_GT.csv: No columns to parse from file\n",
      "â†©ï¸  Skip residual (exists): Dataset08_Demo_Jiggler_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset09_Demo_VerticalChirp_residual.csv\n",
      "âŒ Failed to read Dataset10_Demo_MillbrookHills.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1288: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1289: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset01_Ski_CrossbeamYawNotPerforming_residual] iso=6081 | ae=10734 | lof=1724 | lstm=217 | hyb=1802 | vote3+=217\n",
      "âŒ Skipped Dataset02_Matrix_Rocker4EncoderNotWorking_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1288: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1289: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset03_Wushu_YawTrapezoidNormal_residual] iso=4353 | ae=13404 | lof=2735 | lstm=141 | hyb=3815 | vote3+=368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1288: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1289: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset04_Wushu_YawWaveletSqueak_residual] iso=5912 | ae=5598 | lof=1179 | lstm=721 | hyb=190 | vote3+=662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:395: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:402: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1288: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\571867561.py:1289: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset05_Wushu_LaneChanges_ModelBump_residual] iso=7951 | ae=8071 | lof=1177 | lstm=330 | hyb=1312 | vote3+=118\n",
      "âŒ Skipped Dataset08_Demo_Jiggler_residual: No residuals found.\n",
      "âŒ Skipped Dataset09_Demo_VerticalChirp_residual: No residuals found.\n",
      "âœ… Saved row-level: ./Anomaly_detection/hybrid/outputs/combined_anomaly_results.csv\n",
      "âœ… Saved summary:   ./Anomaly_detection/hybrid/outputs/model_comparison_summary.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/hybrid/outputs/model_comparison_plot.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/hybrid/outputs/model_comparison_rate_plot.png\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.91 MiB for an array with shape (381474, 1) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1456\u001b[0m\n\u001b[0;32m   1452\u001b[0m     run_pipeline(cfg)\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1456\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 1452\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1449\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()  \u001b[38;5;66;03m# allows notebook execution\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m cfg \u001b[38;5;241m=\u001b[39m load_config_from_path_or_default(args\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m-> 1452\u001b[0m run_pipeline(cfg)\n",
      "Cell \u001b[1;32mIn[8], line 1392\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m   1390\u001b[0m episodes_with_reasons \u001b[38;5;241m=\u001b[39m attach_episode_reasons(combined, episodes, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1391\u001b[0m episodes_with_reasons \u001b[38;5;241m=\u001b[39m enrich_hardware_mapping(episodes_with_reasons)\n\u001b[1;32m-> 1392\u001b[0m episodes_scored \u001b[38;5;241m=\u001b[39m score_episodes(combined, episodes_with_reasons, cfg)\n\u001b[0;32m   1394\u001b[0m episodes_reason_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sel_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes_with_reasons_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselection_rule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1395\u001b[0m episodes_scored_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sel_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes_with_reasons_and_scores_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselection_rule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 666\u001b[0m, in \u001b[0;36mscore_episodes\u001b[1;34m(combined_df, episodes_df, cfg)\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r:\n\u001b[1;32m--> 666\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mloc[(combined_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\u001b[38;5;241m.\u001b[39mloc[start:end]\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mloc[start:end]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1413\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1211\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1209\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[0;32m   1210\u001b[0m inds \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.91 MiB for an array with shape (381474, 1) and data type float64"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Anomaly Detection Product (single script) â€” Hybrid-Driven Episodes (no vote_any)\n",
    "- Residual creation (Demand - Measured -> Residual)\n",
    "- Feature engineering (reuse if already present)\n",
    "- Scaling once -> shared across models\n",
    "- Models: IsolationForest, LOF, Dense AE, LSTM AE\n",
    "- Dynamic thresholds (MAD); Hybrid: robust fusion + MAD/quantile with fallback\n",
    "- Hybrid-DRIVEN selection for episodes (default), with flexible selection rules\n",
    "- Robust overlay plots with context padding (fixes blank episode pages)\n",
    "- Episode explanations + hardware mapping + root-cause scoring\n",
    "- Sensor ranking, clustering & heatmap\n",
    "- Multi-page PDF Ops Report (includes hybrid)\n",
    "- Emits a RATE plot (% rows) for fair cross-file comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utils\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in str(name))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Residual creation (optional)\n",
    "# =========================================================\n",
    "def create_residuals_for_folder(\n",
    "    in_folder: str,\n",
    "    out_folder: str,\n",
    "    demand_token: str = \"Demand\",\n",
    "    measured_token: str = \"Measured\",\n",
    "    residual_token: str = \"Residual\",\n",
    "    skip_if_exists: bool = True,\n",
    "    suffix: str = \"_residual\",\n",
    "    logger=print,\n",
    ") -> None:\n",
    "    ensure_dir(out_folder)\n",
    "    for file in os.listdir(in_folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(in_folder, file)\n",
    "        out_name = file.replace(\".csv\", f\"{suffix}.csv\")\n",
    "        out_path = os.path.join(out_folder, out_name)\n",
    "\n",
    "        if skip_if_exists and os.path.exists(out_path):\n",
    "            logger(f\"â†©ï¸  Skip residual (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(in_path)\n",
    "        except Exception as e:\n",
    "            logger(f\"âŒ Failed to read {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        made_any = False\n",
    "        for col in cols:\n",
    "            if demand_token in col:\n",
    "                measured_col = col.replace(demand_token, measured_token)\n",
    "                if measured_col in df.columns:\n",
    "                    residual_col = col.replace(demand_token, residual_token)\n",
    "                    df[residual_col] = df[col] - df[measured_col]\n",
    "                    made_any = True\n",
    "\n",
    "        if not made_any:\n",
    "            logger(f\"âš ï¸  No Demand/Measured pairs found in {file}.\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        logger(f\"âœ… Residual CSV saved: {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Scaling + robust threshold (MAD)\n",
    "# =========================================================\n",
    "def scale_features(X: pd.DataFrame, use_float32: bool = True):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    if use_float32:\n",
    "        X_scaled = X_scaled.astype(\"float32\")\n",
    "    X_tensor = torch.from_numpy(X_scaled)\n",
    "    return scaler, X_scaled, X_tensor\n",
    "\n",
    "\n",
    "def robust_threshold(\n",
    "    values: np.ndarray,\n",
    "    k: float = 3.5,\n",
    "    tail: str = \"high\",\n",
    "    min_anoms: int = 5,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    v = np.asarray(values)\n",
    "    mask = ~np.isnan(v)\n",
    "    v = v[mask]\n",
    "    if v.size == 0:\n",
    "        return (np.inf if tail == \"high\" else -np.inf), np.zeros_like(values, dtype=int)\n",
    "\n",
    "    med = np.median(v)\n",
    "    mad = np.median(np.abs(v - med)) + 1e-12\n",
    "    if tail == \"high\":\n",
    "        thr = med + k * 1.4826 * mad\n",
    "        labels = (values > thr).astype(int)\n",
    "    else:\n",
    "        thr = med - k * 1.4826 * mad\n",
    "        labels = (values < thr).astype(int)\n",
    "\n",
    "    # relax if too strict on large arrays\n",
    "    if labels.sum() < min_anoms and v.size >= 100:\n",
    "        for k_relax in (3.0, 2.5, 2.0):\n",
    "            if tail == \"high\":\n",
    "                thr = med + k_relax * 1.4826 * mad\n",
    "                labels = (values > thr).astype(int)\n",
    "            else:\n",
    "                thr = med - k_relax * 1.4826 * mad\n",
    "                labels = (values < thr).astype(int)\n",
    "            if labels.sum() >= min_anoms:\n",
    "                break\n",
    "\n",
    "    return thr, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Feature Engineering\n",
    "# =========================================================\n",
    "def prepare_features(\n",
    "    df: pd.DataFrame,\n",
    "    residual_cols: List[str],\n",
    "    window: int = 5,\n",
    "    max_features: int = 500,\n",
    "    logger=print,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:\n",
    "    already_done = any(f\"{residual_cols[0]}_delta\" in df.columns for _ in residual_cols)\n",
    "    stats = {\"reused\": 0, \"generated\": 0}\n",
    "\n",
    "    if already_done:\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if any(k in c for k in [\"Residual\", \"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "        ]\n",
    "        X = df[feature_cols].dropna()\n",
    "        stats[\"reused\"] = len(feature_cols)\n",
    "        logger(f\"ðŸ” Reusing {len(feature_cols)} engineered features.\")\n",
    "        return X, feature_cols, stats\n",
    "\n",
    "    # Generate\n",
    "    for col in residual_cols:\n",
    "        df[f\"{col}_delta\"] = df[col].diff()\n",
    "        df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
    "        df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std()\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in residual_cols:\n",
    "        feature_cols += [\n",
    "            col,\n",
    "            f\"{col}_delta\",\n",
    "            f\"{col}_rolling_mean_{window}\",\n",
    "            f\"{col}_rolling_std_{window}\",\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].dropna()\n",
    "    stats[\"generated\"] = len(feature_cols)\n",
    "    logger(f\"ðŸ› ï¸  Generated {len(feature_cols)} features (window={window}).\")\n",
    "\n",
    "    if X.shape[1] > max_features:\n",
    "        logger(f\"âŒ Too many features ({X.shape[1]} > {max_features}). Skipping file.\")\n",
    "        return pd.DataFrame(), [], stats\n",
    "\n",
    "    return X, feature_cols, stats\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Models\n",
    "# =========================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 32), nn.ReLU(), nn.Linear(32, 8))\n",
    "        self.decoder = nn.Sequential(nn.Linear(8, 32), nn.ReLU(), nn.Linear(32, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def dense_autoencoder_detect(\n",
    "    X_tensor: torch.Tensor, k: float, ae_epochs: int, ae_lr: float\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    model = Autoencoder(X_tensor.shape[1])\n",
    "    opt = optim.Adam(model.parameters(), lr=ae_lr)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    for _ in range(ae_epochs):\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = crit(out, X_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec = model(X_tensor)\n",
    "        errors = torch.mean((X_tensor - rec) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "    return labels.astype(int), errors, thr\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.encoder(x)  # [1, B, H]\n",
    "        repeated = h.repeat(x.size(1), 1, 1).transpose(0, 1)  # [B, T, H]\n",
    "        decoded, _ = self.decoder(repeated)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def make_sequences(X: np.ndarray, seq_len: int) -> Tuple[np.ndarray, List[int]]:\n",
    "    seqs, idxs = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seqs.append(X[i:i+seq_len])\n",
    "        idxs.append(i + seq_len - 1)\n",
    "    return np.array(seqs), idxs\n",
    "\n",
    "\n",
    "def lstm_autoencoder_detect(\n",
    "    X_scaled: np.ndarray,\n",
    "    k: float,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int,\n",
    "    patience: int,\n",
    "    max_sequences: int,\n",
    "    downsample: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[int], float]:\n",
    "    try:\n",
    "        Xds = X_scaled[::downsample]\n",
    "        if len(Xds) < seq_len:\n",
    "            return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "        Xseq, idxs = make_sequences(Xds, seq_len)\n",
    "        if len(Xseq) > max_sequences:\n",
    "            Xseq, idxs = Xseq[:max_sequences], idxs[:max_sequences]\n",
    "\n",
    "        Xt = torch.tensor(Xseq, dtype=torch.float32)\n",
    "        model = LSTMAutoencoder(Xt.shape[2], hidden_dim)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best, wait = float(\"inf\"), 0\n",
    "        for _ in range(100):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(Xt)\n",
    "            loss = crit(out, Xt)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if loss.item() < best:\n",
    "                best, wait = loss.item(), 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(Xt)\n",
    "            errors = torch.mean((Xt - out) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "        thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "        return labels.astype(int), errors, idxs, thr\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ LSTM memory error: {e}\")\n",
    "        return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "\n",
    "def isolation_forest_detect(X_scaled: np.ndarray, k: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    iso = IsolationForest(contamination=\"auto\", n_estimators=300, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "def lof_detect(X_scaled: np.ndarray, k: float, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=\"auto\")\n",
    "    _ = lof.fit_predict(X_scaled)  # populates negative_outlier_factor_\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hybrid scoring utilities\n",
    "# =========================================================\n",
    "def _robust_z_pos(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Right-tail robust z-score (>=0 when above median).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    z = np.where(np.isnan(z), np.nan, z)\n",
    "    return np.maximum(z, 0.0)  # only right tail counts as anomalous\n",
    "\n",
    "\n",
    "def _percentile01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map to [0,1] by robust percentiles (2â€“98). Values outside clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    lo = np.nanpercentile(x, 2)\n",
    "    hi = np.nanpercentile(x, 98)\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    y = (x - lo) / rng\n",
    "    return np.clip(y, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hybrid_score_on_mask(df: pd.DataFrame, cfg: dict, mask_idx) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute hybrid only on valid rows (mask_idx). Returns array the size of df,\n",
    "    NaN elsewhere. Requires >= min_components present.\n",
    "    \"\"\"\n",
    "    out = np.full(len(df), np.nan)\n",
    "    if not cfg.get(\"hybrid\", {}).get(\"enabled\", False):\n",
    "        return out\n",
    "\n",
    "    # Weights guard\n",
    "    wmap = cfg.get(\"hybrid\", {}).get(\"weights\")\n",
    "    if not isinstance(wmap, dict) or not wmap:\n",
    "        wmap = {\"iso_score\": 0.25, \"lof_score\": 0.25, \"ae_error\": 0.25, \"lstm_error\": 0.25}\n",
    "\n",
    "    method = cfg[\"hybrid\"].get(\"method\", \"robust_z\")\n",
    "    min_components = int(cfg[\"hybrid\"].get(\"min_components\", 2))\n",
    "\n",
    "    use = df.loc[mask_idx]  # restrict to valid feature rows\n",
    "\n",
    "    comps = [c for c in [\"iso_score\", \"lof_score\", \"ae_error\", \"lstm_error\"] if c in use.columns and c in wmap]\n",
    "    if not comps:\n",
    "        return out\n",
    "\n",
    "    parts = []\n",
    "    for c in comps:\n",
    "        arr = use[c].to_numpy(dtype=float)\n",
    "        if method == \"robust_z\":\n",
    "            norm = _robust_z_pos(arr)\n",
    "            norm = np.clip(norm, 0, 10.0) / 10.0  # compress extreme tails to ~[0,1]\n",
    "        else:\n",
    "            norm = _percentile01(arr)\n",
    "        parts.append((norm, float(wmap[c])))\n",
    "\n",
    "    num = np.zeros(len(use), dtype=float)\n",
    "    den = np.zeros(len(use), dtype=float)\n",
    "    present = np.zeros(len(use), dtype=int)\n",
    "\n",
    "    for norm, w in parts:\n",
    "        m = ~np.isnan(norm)\n",
    "        num[m] += w * norm[m]\n",
    "        den[m] += w\n",
    "        present[m] += 1\n",
    "\n",
    "    hybrid_local = np.where((den > 0) & (present >= min_components), num / den, np.nan)\n",
    "    out[np.asarray(mask_idx)] = hybrid_local\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Voting (for reference only), selection, episodes, explanations\n",
    "# =========================================================\n",
    "def generate_votes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"agreement_all_4\"] = (\n",
    "        (df.get(\"ae_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lof_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lstm_is_anomaly\", 0) == 1)\n",
    "    ).astype(int)\n",
    "    df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
    "    df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
    "    return df  # NOTE: vote_any removed\n",
    "\n",
    "\n",
    "def extract_selected_rows(df: pd.DataFrame, rule: str = \"hybrid\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selection rules that drive downstream episodes & reasoning.\n",
    "    Supported:\n",
    "      - \"hybrid\": hybrid_is_anomaly == 1\n",
    "      - \"vote_3plus\": â‰¥3/4 models (conservative)\n",
    "      - \"agreement_all_4\": all four agree (very strict)\n",
    "      - \"hybrid_or_vote3p\": union of hybrid and vote_3plus\n",
    "      - \"hybrid_and_vote3p\": intersection of hybrid and vote_3plus\n",
    "    \"\"\"\n",
    "    rule = (rule or \"hybrid\").lower()\n",
    "    if rule == \"hybrid\":\n",
    "        mask = df.get(\"hybrid_is_anomaly\", 0) == 1\n",
    "    elif rule == \"vote_3plus\":\n",
    "        mask = df.get(\"vote_3plus\", 0) == 1\n",
    "    elif rule == \"agreement_all_4\":\n",
    "        mask = df.get(\"agreement_all_4\", 0) == 1\n",
    "    elif rule == \"hybrid_or_vote3p\":\n",
    "        mask = ((df.get(\"hybrid_is_anomaly\", 0) == 1) | (df.get(\"vote_3plus\", 0) == 1))\n",
    "    elif rule == \"hybrid_and_vote3p\":\n",
    "        mask = ((df.get(\"hybrid_is_anomaly\", 0) == 1) & (df.get(\"vote_3plus\", 0) == 1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown selection rule: {rule}\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _group_runs(idxs: np.ndarray, min_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "    runs, start, prev = [], int(idxs[0]), int(idxs[0])\n",
    "    for i in idxs[1:]:\n",
    "        if int(i) - prev <= min_gap:\n",
    "            prev = int(i)\n",
    "            continue\n",
    "        runs.append((start, prev))\n",
    "        start = int(i); prev = int(i)\n",
    "    runs.append((start, prev))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def summarize_episodes(selected_df: pd.DataFrame, min_gap: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build episodes PER FILE from 'selected' rows (e.g., hybrid-driven).\n",
    "    \"\"\"\n",
    "    if selected_df.empty:\n",
    "        cols = [\"source_file\", \"start_idx\", \"end_idx\", \"length\", \"n_models_mean\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    if \"source_file\" in selected_df.columns:\n",
    "        groups = selected_df.groupby(\"source_file\")\n",
    "    else:\n",
    "        groups = [(\"\", selected_df)]\n",
    "\n",
    "    for sf, g in groups:\n",
    "        idxs = g.index.to_numpy()\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "        runs = _group_runs(idxs, min_gap=min_gap)\n",
    "        for start, end in runs:\n",
    "            chunk = g.loc[start:end]\n",
    "            row = {\n",
    "                \"source_file\": sf,\n",
    "                \"start_idx\": int(start),\n",
    "                \"end_idx\": int(end),\n",
    "                \"length\": int(end - start + 1),\n",
    "                \"n_models_mean\": float(\n",
    "                    chunk[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1).mean()\n",
    "                ),\n",
    "            }\n",
    "            for c in [\"iso_score\", \"ae_error\", \"lof_score\", \"lstm_error\", \"hybrid_score\"]:\n",
    "                if c in chunk.columns:\n",
    "                    row[f\"{c}_max\"] = float(chunk[c].max())\n",
    "                    row[f\"{c}_mean\"] = float(chunk[c].mean())\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _base_residual_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def _models_string(chunk: pd.DataFrame) -> str:\n",
    "    model_cols = [c for c in [\"is_anomaly\", \"ae_is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\", \"hybrid_is_anomaly\"] if c in chunk.columns]\n",
    "    if not model_cols:\n",
    "        return \"no-model-flags\"\n",
    "    means = chunk[model_cols].mean()\n",
    "    active = [m.replace(\"_is_anomaly\", \"\").upper() for m, v in means.items() if v >= 0.5]\n",
    "    return \", \".join(active) if active else \"weak/isolated flags\"\n",
    "\n",
    "\n",
    "def attach_episode_reasons(\n",
    "    combined_df: pd.DataFrame, episodes_df: pd.DataFrame, top_k: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "\n",
    "    base_res = _base_residual_columns(combined_df)\n",
    "    if not base_res:\n",
    "        episodes_df[\"primary_signal\"] = \"\"\n",
    "        episodes_df[\"reason\"] = \"no residual columns present\"\n",
    "        episodes_df[\"suspected_sensor\"] = \"\"\n",
    "        return episodes_df\n",
    "\n",
    "    out = []\n",
    "    for _, epi in episodes_df.iterrows():\n",
    "        start, end = int(epi[\"start_idx\"]), int(epi[\"end_idx\"])\n",
    "        mask = combined_df[\"source_file\"] == epi[\"source_file\"] if \"source_file\" in combined_df.columns else slice(None)\n",
    "        chunk = combined_df.loc[mask].loc[start:end]\n",
    "\n",
    "        if chunk.empty:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"empty slice\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats = []\n",
    "        for col in base_res:\n",
    "            if col in chunk.columns:\n",
    "                stats.append((col, float(chunk[col].abs().max())))\n",
    "        if not stats:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"no residual stats\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_signal, primary_val = stats[:top_k][0]\n",
    "        models_str = _models_string(chunk)\n",
    "        measured_col = primary_signal.replace(\"Residual\", \"Measured\")\n",
    "        suspected = measured_col if (measured_col in combined_df.columns) else \"unknown-measured-sensor\"\n",
    "\n",
    "        epi[\"primary_signal\"] = primary_signal\n",
    "        epi[\"reason\"] = f\"max |{primary_signal}| = {primary_val:.3f}; models: {models_str}\"\n",
    "        epi[\"suspected_sensor\"] = suspected\n",
    "        out.append(epi)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hardware mapping + root cause scoring\n",
    "# =========================================================\n",
    "HARDWARE_MAP = [\n",
    "    (\"Force_\",         \"Actuator/LoadCell\",  \"Force didnâ€™t follow demand â†’ friction/lag/saturation/load-cell drift likely\"),\n",
    "    (\"Encoder_\",       \"Encoders/Alignment\", \"Pose/velocity mismatch â†’ quantization/missing counts/misalignment\"),\n",
    "    (\"Accelerometer_\", \"IMU/Accelerometer\",  \"Vibration bursts â†’ mounting/looseness/thermal drift\"),\n",
    "    (\"State_\",         \"Control/Timing\",     \"Requested vs achieved state diverged â†’ scheduler limits/controller windup\"),\n",
    "]\n",
    "\n",
    "def map_signal_to_hardware(primary_signal: str):\n",
    "    for needle, hw, why in HARDWARE_MAP:\n",
    "        if needle in primary_signal:\n",
    "            return hw, why\n",
    "    return \"Unknown\", \"No mapping rule matched\"\n",
    "\n",
    "\n",
    "def enrich_hardware_mapping(episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    episodes_df = episodes_df.copy()\n",
    "    episodes_df[\"hardware_class\"] = \"\"\n",
    "    episodes_df[\"hardware_why\"] = \"\"\n",
    "    for i, r in episodes_df.iterrows():\n",
    "        hw, why = map_signal_to_hardware(r.get(\"primary_signal\", \"\"))\n",
    "        episodes_df.at[i, \"hardware_class\"] = hw\n",
    "        episodes_df.at[i, \"hardware_why\"]   = why\n",
    "    return episodes_df\n",
    "\n",
    "\n",
    "def _paired_columns(primary_signal: str, cfg: dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "    resid_tok  = cfg[\"signals\"][\"residual_token\"]\n",
    "    demand_tok = cfg[\"signals\"][\"demand_token\"]\n",
    "    measured_tok = cfg[\"signals\"][\"measured_token\"]\n",
    "    if resid_tok not in primary_signal:\n",
    "        return None, None\n",
    "    demand_col   = primary_signal.replace(resid_tok, demand_tok)\n",
    "    measured_col = primary_signal.replace(resid_tok, measured_tok)\n",
    "    return demand_col, measured_col\n",
    "\n",
    "\n",
    "def _nan_ok(arr: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_correlation_lag(x: np.ndarray, y: np.ndarray, sample_rate_hz: Optional[float]) -> Tuple[float, int]:\n",
    "    x = _nan_ok(x); y = _nan_ok(y)\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return (np.nan, 0)\n",
    "    x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    x = np.nan_to_num(x);  y = np.nan_to_num(y)\n",
    "    corr = np.correlate(x, y, mode=\"full\")\n",
    "    lags = np.arange(-len(x)+1, len(x))\n",
    "    k = int(np.argmax(corr))\n",
    "    lag_samples = int(lags[k])\n",
    "    lag_seconds = lag_samples / sample_rate_hz if sample_rate_hz and sample_rate_hz > 0 else np.nan\n",
    "    return (lag_seconds, lag_samples)\n",
    "\n",
    "\n",
    "def _saturation_score(demand: np.ndarray, residual: np.ndarray, cfg: dict) -> float:\n",
    "    if len(demand) == 0 or len(residual) == 0:\n",
    "        return 0.0\n",
    "    p_dem = np.nanpercentile(demand, cfg[\"scores\"][\"saturation_pct\"])\n",
    "    p_res = np.nanpercentile(np.abs(residual), cfg[\"scores\"][\"resid_prominence_pct\"])\n",
    "    near_limit = demand >= p_dem\n",
    "    large_res  = np.abs(residual) >= p_res\n",
    "    both = np.logical_and(near_limit, large_res)\n",
    "    return float(np.nansum(both)) / max(1, len(demand))\n",
    "\n",
    "\n",
    "def _drift_score(residual: np.ndarray) -> float:\n",
    "    residual = _nan_ok(residual)\n",
    "    mu = float(np.nanmean(residual))\n",
    "    sd = float(np.nanstd(residual)) + 1e-9\n",
    "    return abs(mu) / sd\n",
    "\n",
    "\n",
    "def _vibration_score(signal: np.ndarray, sample_rate_hz: Optional[float]) -> float:\n",
    "    if not sample_rate_hz or sample_rate_hz <= 0 or len(signal) < 8:\n",
    "        return np.nan\n",
    "    sig = np.nan_to_num(signal - np.nanmean(signal))\n",
    "    fft = np.fft.rfft(sig)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1.0 / sample_rate_hz)\n",
    "    if len(freqs) == 0:\n",
    "        return np.nan\n",
    "    cutoff = 0.25 * (sample_rate_hz / 2.0)\n",
    "    mask_hi = freqs >= cutoff\n",
    "    num = float(np.nansum(power[mask_hi]))\n",
    "    den = float(np.nansum(power) + 1e-12)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def score_episodes(combined_df: pd.DataFrame, episodes_df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    out = episodes_df.copy()\n",
    "    sr = cfg[\"signals\"][\"sample_rate_hz\"]\n",
    "    min_len = cfg[\"scores\"][\"min_window_len\"]\n",
    "\n",
    "    if \"primary_signal\" not in out.columns:\n",
    "        out[\"primary_signal\"] = \"\"\n",
    "\n",
    "    for i, r in out.iterrows():\n",
    "        start, end = int(r[\"start_idx\"]), int(r[\"end_idx\"])\n",
    "        if end - start + 1 < min_len:\n",
    "            out.at[i, \"lag_seconds\"] = np.nan\n",
    "            out.at[i, \"lag_samples\"] = 0\n",
    "            out.at[i, \"saturation_score\"] = 0.0\n",
    "            out.at[i, \"drift_score\"] = 0.0\n",
    "            out.at[i, \"vibe_score\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        if \"source_file\" in combined_df.columns and \"source_file\" in out.columns and \"source_file\" in r:\n",
    "            chunk = combined_df.loc[(combined_df[\"source_file\"] == r[\"source_file\"])].loc[start:end]\n",
    "        else:\n",
    "            chunk = combined_df.loc[start:end]\n",
    "\n",
    "        primary = r.get(\"primary_signal\", \"\")\n",
    "        demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "\n",
    "        resid = chunk[primary].values if (primary in chunk.columns) else np.array([])\n",
    "        dem   = chunk[demand_col].values if (demand_col and demand_col in chunk.columns) else np.array([])\n",
    "        meas  = chunk[measured_col].values if (measured_col and measured_col in chunk.columns) else np.array([])\n",
    "\n",
    "        lag_s, lag_k = _cross_correlation_lag(dem, meas, sr) if (len(dem) and len(meas)) else (np.nan, 0)\n",
    "        sat_sc = _saturation_score(dem, resid, cfg) if (len(dem) and len(resid)) else 0.0\n",
    "        dr_sc  = _drift_score(resid) if len(resid) else 0.0\n",
    "        if \"Accelerometer_\" in primary and primary in chunk.columns:\n",
    "            vibe_sc = _vibration_score(chunk[primary].values, sr)\n",
    "        else:\n",
    "            vibe_sc = _vibration_score(resid, sr)\n",
    "\n",
    "        out.at[i, \"lag_seconds\"]       = lag_s\n",
    "        out.at[i, \"lag_samples\"]       = int(lag_k)\n",
    "        out.at[i, \"saturation_score\"]  = float(sat_sc)\n",
    "        out.at[i, \"drift_score\"]       = float(dr_sc)\n",
    "        out.at[i, \"vibe_score\"]        = float(vibe_sc) if vibe_sc == vibe_sc else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Plotting helpers (per-file selected overlays)\n",
    "# =========================================================\n",
    "def _pick_residual(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand = [c for c in df.columns if \"Residual\" in c and not any(t in c for t in [\"_delta\", \"_rolling_\"])]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def _slice_by_global_index(sub: pd.DataFrame, start: int, end: int, pad: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust slice of `sub` (filtered to one file) using global indices [start, end],\n",
    "    expanded by `pad` points on both sides. Works even when index has gaps.\n",
    "    \"\"\"\n",
    "    if sub.empty:\n",
    "        return sub\n",
    "    idx = sub.index.to_numpy()\n",
    "    i0 = np.searchsorted(idx, start, side=\"left\")\n",
    "    i1 = np.searchsorted(idx, end,   side=\"right\")\n",
    "    i0 = max(i0 - pad, 0)\n",
    "    i1 = min(i1 + pad, len(idx))\n",
    "    return sub.iloc[i0:i1]\n",
    "\n",
    "\n",
    "def plot_selected_for_file(\n",
    "    df_file: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    rule: str,\n",
    "    min_gap: int,\n",
    "    figsize: Tuple[int, int] = (12, 5),\n",
    ") -> Optional[str]:\n",
    "    ensure_dir(out_dir)\n",
    "    residual_col = _pick_residual(df_file)\n",
    "    if residual_col is None:\n",
    "        print(\"âš ï¸ No residual column to plot.\")\n",
    "        return None\n",
    "\n",
    "    selected_rows = extract_selected_rows(df_file, rule=rule)\n",
    "    episodes = summarize_episodes(selected_rows, min_gap=min_gap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_file.index, df_file[residual_col], label=residual_col, alpha=0.85)\n",
    "\n",
    "    if not selected_rows.empty:\n",
    "        plt.scatter(selected_rows.index, selected_rows[residual_col], s=12, label=f\"Selected anomalies ({rule})\")\n",
    "\n",
    "    if not episodes.empty:\n",
    "        for _, r in episodes.iterrows():\n",
    "            plt.axvspan(r[\"start_idx\"], r[\"end_idx\"], alpha=0.15, label=\"Episode\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uniq, seen = [], set()\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in seen:\n",
    "                uniq.append((h, l)); seen.add(l)\n",
    "        handles, labels = zip(*uniq)\n",
    "        plt.legend(handles, labels)\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    sf = df_file[\"source_file\"].iloc[0] if \"source_file\" in df_file.columns else \"file\"\n",
    "    plt.title(f\"{sf} â€” Residual with selected anomalies & episodes ({rule})\")\n",
    "    plt.xlabel(\"Index\"); plt.ylabel(residual_col)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"selected_plot_{safe_name(sf)}.png\")\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_all_files(combined_df: pd.DataFrame, out_dir: str, rule: str, min_gap: int, max_files: Optional[int] = None):\n",
    "    paths = []\n",
    "    if \"source_file\" not in combined_df.columns:\n",
    "        print(\"âš ï¸ combined_df missing 'source_file'.\")\n",
    "        return paths\n",
    "    groups = list(combined_df.groupby(\"source_file\"))\n",
    "    if max_files is not None:\n",
    "        groups = groups[:max_files]\n",
    "    for fname, df_file in groups:\n",
    "        p = plot_selected_for_file(df_file, out_dir=out_dir, rule=rule, min_gap=min_gap)\n",
    "        if p:\n",
    "            paths.append(p); print(f\"ðŸ–¼ï¸ Saved: {p}\")\n",
    "    if not paths:\n",
    "        print(\"âš ï¸ No plots produced.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Sensor attribution, clustering & heatmap\n",
    "# =========================================================\n",
    "def _residual_cols_base(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_sensor_table(\n",
    "    combined: pd.DataFrame,\n",
    "    selected_rows: pd.DataFrame,\n",
    "    episodes_with_reasons: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    base_res = _residual_cols_base(combined)\n",
    "    if not base_res:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(combined)\n",
    "    selected_mask = pd.Series(False, index=combined.index)\n",
    "    if not selected_rows.empty:\n",
    "        selected_mask.loc[selected_rows.index] = True\n",
    "\n",
    "    rows = []\n",
    "    expected_keys = [\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"mean_abs_resid_selected\",\n",
    "        \"max_abs_resid_selected\",\n",
    "        \"episodes_as_primary\",\n",
    "    ]\n",
    "\n",
    "    for col in base_res:\n",
    "        stats = {\"sensor\": col}\n",
    "\n",
    "        stats[\"anomaly_rate_is\"]   = float(combined[\"is_anomaly\"].sum())   / max(total_rows, 1) if \"is_anomaly\"   in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_ae\"]   = float(combined[\"ae_is_anomaly\"].sum())/ max(total_rows, 1) if \"ae_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lof\"]  = float(combined[\"lof_is_anomaly\"].sum())/max(total_rows, 1) if \"lof_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lstm\"] = float(combined[\"lstm_is_anomaly\"].fillna(0).sum())/max(total_rows, 1) if \"lstm_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_hybrid\"] = float(combined[\"hybrid_is_anomaly\"].sum())/max(total_rows, 1) if \"hybrid_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote3p\"] = float(combined[\"vote_3plus\"].sum())/max(total_rows, 1) if \"vote_3plus\" in combined.columns else 0.0\n",
    "\n",
    "        if col in combined.columns and selected_mask.any():\n",
    "            vals = combined.loc[selected_mask, col].abs()\n",
    "            stats[\"mean_abs_resid_selected\"] = float(vals.mean()) if not vals.empty else 0.0\n",
    "            stats[\"max_abs_resid_selected\"]  = float(vals.max())  if not vals.empty else 0.0\n",
    "        else:\n",
    "            stats[\"mean_abs_resid_selected\"] = 0.0\n",
    "            stats[\"max_abs_resid_selected\"]  = 0.0\n",
    "\n",
    "        if episodes_with_reasons is not None and not episodes_with_reasons.empty and \"primary_signal\" in episodes_with_reasons.columns:\n",
    "            stats[\"episodes_as_primary\"] = int((episodes_with_reasons[\"primary_signal\"] == col).sum())\n",
    "        else:\n",
    "            stats[\"episodes_as_primary\"] = 0\n",
    "\n",
    "        for k in expected_keys:\n",
    "            stats.setdefault(k, 0.0)\n",
    "\n",
    "        rows.append(stats)\n",
    "\n",
    "    sensor_df = pd.DataFrame(rows)\n",
    "    for c in sensor_df.columns:\n",
    "        if c != \"sensor\":\n",
    "            sensor_df[c] = sensor_df[c].fillna(0.0)\n",
    "    return sensor_df\n",
    "\n",
    "\n",
    "def cluster_sensors(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    n_clusters: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    if sensor_df.empty or \"sensor\" not in sensor_df.columns:\n",
    "        return sensor_df, np.empty((0, 2)), np.empty((0, 2))\n",
    "\n",
    "    features = sensor_df.drop(columns=[\"sensor\"]).to_numpy(dtype=np.float32)\n",
    "    if features.shape[0] < n_clusters:\n",
    "        n_clusters = max(1, features.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(features)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(Z)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    Z2 = pca.fit_transform(Z)\n",
    "    centers2 = pca.transform(km.cluster_centers_)\n",
    "\n",
    "    out = sensor_df.copy()\n",
    "    out[\"cluster\"] = labels\n",
    "\n",
    "    return out, Z2, centers2\n",
    "\n",
    "\n",
    "def plot_sensor_bar_top(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metric: str = \"episodes_as_primary\",\n",
    "    top_n: int = 15,\n",
    "    title: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty or metric not in sensor_df.columns:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    df = sensor_df.sort_values(metric, ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(df)), df[metric])\n",
    "    plt.xticks(range(len(df)), [s.replace(\"Force_\", \"F_\") for s in df[\"sensor\"]], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title or f\"Top {top_n} sensors by {metric}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"top_sensors_{metric}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_clusters_scatter(\n",
    "    sensor_df_with_cluster: pd.DataFrame,\n",
    "    Z2: np.ndarray,\n",
    "    centers2: np.ndarray,\n",
    "    out_dir: str,\n",
    "    title: str = \"Sensor clusters (PCA of features)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df_with_cluster.empty or Z2.size == 0:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    clusters = sorted(sensor_df_with_cluster[\"cluster\"].unique().tolist())\n",
    "    for cl in clusters:\n",
    "        mask = sensor_df_with_cluster[\"cluster\"] == cl\n",
    "        pts = Z2[mask.values]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"cluster {cl}\", alpha=0.8, s=36)\n",
    "\n",
    "    if centers2.size:\n",
    "        plt.scatter(centers2[:, 0], centers2[:, 1], marker=\"X\", s=120, label=\"centers\")\n",
    "\n",
    "    try:\n",
    "        top_lab = sensor_df_with_cluster.sort_values(\"episodes_as_primary\", ascending=False).head(10).index\n",
    "        for idx in top_lab:\n",
    "            plt.text(Z2[idx, 0], Z2[idx, 1], sensor_df_with_cluster.loc[idx, \"sensor\"], fontsize=8)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_clusters_pca.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_heatmap(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    title: str = \"Sensor anomaly fingerprint (rates & magnitudes)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty:\n",
    "        return None\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    desired = [\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"mean_abs_resid_selected\",\n",
    "        \"max_abs_resid_selected\",\n",
    "    ]\n",
    "    if metrics is None:\n",
    "        metrics = desired\n",
    "\n",
    "    available = [m for m in metrics if m in sensor_df.columns]\n",
    "    if not available:\n",
    "        print(\"âš ï¸ No requested heatmap metrics are present in sensor_df. Skipping heatmap.\")\n",
    "        return None\n",
    "    if len(available) < len(metrics):\n",
    "        missing = [m for m in metrics if m not in sensor_df.columns]\n",
    "        print(f\"â„¹ï¸ Skipping missing metrics in heatmap: {missing}\")\n",
    "    metrics = available\n",
    "\n",
    "    key_rank = \"episodes_as_primary\" if \"episodes_as_primary\" in sensor_df.columns else metrics[0]\n",
    "    keep = sensor_df.sort_values(key_rank, ascending=False).head(25)\n",
    "\n",
    "    M = keep[metrics].to_numpy(dtype=np.float32)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(M, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.yticks(range(len(keep)), keep[\"sensor\"])\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_fingerprint_heatmap.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Report (robust overlays with padding)\n",
    "# =========================================================\n",
    "def _overlay_episode_plot(df: pd.DataFrame, episode_row: pd.Series, cfg: dict, ax=None):\n",
    "    start, end = int(episode_row[\"start_idx\"]), int(episode_row[\"end_idx\"])\n",
    "    primary = episode_row.get(\"primary_signal\", \"\")\n",
    "    demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "    pad = int(cfg.get(\"report\", {}).get(\"pad_points\", 100))\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # If df contains multiple files, filter to the right one first\n",
    "    if \"source_file\" in df.columns and \"source_file\" in episode_row:\n",
    "        df = df.loc[df[\"source_file\"] == episode_row[\"source_file\"]]\n",
    "\n",
    "    # Robust slice by global indices + padding\n",
    "    window = _slice_by_global_index(df, start, end, pad=pad)\n",
    "    if window.empty:\n",
    "        ax.set_title(f\"Episode {start}â€“{end} (EMPTY SLICE)\")\n",
    "        return\n",
    "\n",
    "    t = window.index.to_numpy()\n",
    "\n",
    "    if primary in window.columns:\n",
    "        ax.plot(t, window[primary].values, label=f\"{primary}\", alpha=0.9)\n",
    "    if demand_col and demand_col in window.columns:\n",
    "        ax.plot(t, window[demand_col].values, label=f\"{demand_col}\", alpha=0.8)\n",
    "    if measured_col and measured_col in window.columns:\n",
    "        ax.plot(t, window[measured_col].values, label=f\"{measured_col}\", alpha=0.8)\n",
    "\n",
    "    # Shade the exact episode span inside the padded window\n",
    "    ax.axvspan(start, end, alpha=0.15, label=\"episode window\")\n",
    "\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_title(f\"Episode {start}â€“{end}\\nprimary={primary}\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def build_ops_report(\n",
    "    combined: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    sensor_df: pd.DataFrame,\n",
    "    episodes_scored: pd.DataFrame,\n",
    "    cfg: dict,\n",
    "    out_pdf_path: str\n",
    "):\n",
    "    ensure_dir(os.path.dirname(out_pdf_path))\n",
    "    with PdfPages(out_pdf_path) as pdf:\n",
    "\n",
    "        # Page 1 â€” Anomalies counts by model (dynamic columns)\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "        summary[plot_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomalies per Model per File\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 2 â€” Top sensors by episodes_as_primary\n",
    "        p1 = plot_sensor_bar_top(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"], metric=\"episodes_as_primary\", top_n=15,\n",
    "                                 title=\"Top sensors by episodes_as_primary\")\n",
    "        if p1 and os.path.exists(p1):\n",
    "            img = plt.imread(p1)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 3 â€” Sensor heatmap (if created)\n",
    "        p2 = plot_sensor_heatmap(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"])\n",
    "        if p2 and os.path.exists(p2):\n",
    "            img = plt.imread(p2)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Pages 4+ â€” Episode overlays (per-file selection with caps)\n",
    "        if not episodes_scored.empty:\n",
    "            candidates = episodes_scored.copy()\n",
    "            sort_keys = [c for c in [\"n_models_mean\", \"hybrid_score_mean\", \"iso_score_mean\", \"ae_error_mean\"] if c in candidates.columns]\n",
    "            if sort_keys:\n",
    "                candidates = candidates.sort_values(sort_keys, ascending=False)\n",
    "\n",
    "            n_per_file = int(cfg.get(\"report\", {}).get(\"top_n_per_file\", 2))\n",
    "            max_pages  = int(cfg.get(\"report\", {}).get(\"max_pages\", 12))\n",
    "\n",
    "            pages = 0\n",
    "            for sf, grp in candidates.groupby(\"source_file\"):\n",
    "                for _, epi in grp.head(n_per_file).iterrows():\n",
    "                    if pages >= max_pages:\n",
    "                        break\n",
    "                    plt.figure(figsize=(11, 5))\n",
    "                    _overlay_episode_plot(combined, epi, cfg, ax=plt.gca())\n",
    "                    hw = epi.get(\"hardware_class\", \"Unknown\")\n",
    "                    why = epi.get(\"hardware_why\", \"\")\n",
    "                    lag_s = epi.get(\"lag_seconds\", np.nan)\n",
    "                    sat   = epi.get(\"saturation_score\", np.nan)\n",
    "                    drift = epi.get(\"drift_score\", np.nan)\n",
    "                    vibe  = epi.get(\"vibe_score\", np.nan)\n",
    "                    txt = (\n",
    "                        f\"hardware: {hw}\\n\"\n",
    "                        f\"why: {why}\\n\"\n",
    "                        f\"lag_seconds: {lag_s:.4f}  |  saturation: {sat:.3f}  |  drift: {drift:.3f}  |  vibe: {vibe:.3f}\"\n",
    "                    )\n",
    "                    plt.gcf().text(0.02, 0.02, txt, ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(); plt.close()\n",
    "                    pages += 1\n",
    "                if pages >= max_pages:\n",
    "                    break\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Config (defaults or JSON)\n",
    "# =========================================================\n",
    "def default_config() -> dict:\n",
    "    return {\n",
    "        \"io\": {\n",
    "            \"input_folder\": \"./Datasets/Datasets\",\n",
    "            \"residual_folder\": \"./Anomaly_detection/residual_created/\",\n",
    "            \"output_folder\": \"./Anomaly_detection/hybrid/outputs/\"\n",
    "        },\n",
    "        \"residuals\": {\n",
    "            \"enabled\": True,\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\",\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"suffix\": \"_residual\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"window\": 5,\n",
    "            \"max_features\": 500\n",
    "        },\n",
    "        \"threshold\": {\n",
    "            \"k\": 3.5\n",
    "        },\n",
    "        \"ae\": {\n",
    "            \"epochs\": 50,\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"seq_len\": 5,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"patience\": 5,\n",
    "            \"max_sequences\": 3000,\n",
    "            \"downsample\": 5\n",
    "        },\n",
    "        \"lof\": {\n",
    "            \"n_neighbors\": 20\n",
    "        },\n",
    "        \"hybrid\": {                     # Hybrid scoring config\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"robust_z\",      # \"robust_z\" | \"percentile\"\n",
    "            \"min_components\": 2,       # require at least N model scores present\n",
    "            \"weights\": {               # relative importance (doesn't need to sum to 1)\n",
    "                \"iso_score\": 0.20,\n",
    "                \"lof_score\": 0.20,\n",
    "                \"ae_error\": 0.30,\n",
    "                \"lstm_error\": 0.30\n",
    "            }\n",
    "        },\n",
    "        \"hybrid_threshold\": {          # How to threshold hybrid_score\n",
    "            \"mode\": \"quantile\",        # \"mad\" or \"quantile\"\n",
    "            \"k\": 3.5,                  # used only if mode=\"mad\"\n",
    "            \"quantile\": 0.99           # top 1% as anomalies (fallback if MAD degenerates)\n",
    "        },\n",
    "        \"voting\": {                    # kept for reference plots; not used for selection unless you choose it\n",
    "            \"rule\": \"vote_3plus\",\n",
    "            \"min_gap\": 1\n",
    "        },\n",
    "        \"selection\": {                 # NEW: drives episodes & downstream logic\n",
    "            \"rule\": \"hybrid\"           # \"hybrid\" | \"vote_3plus\" | \"agreement_all_4\" | \"hybrid_or_vote3p\" | \"hybrid_and_vote3p\"\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_files\": None,\n",
    "            \"emit_rate_plot\": True     # also write an anomaly RATE bar chart (% rows)\n",
    "        },\n",
    "        \"runtime\": {\n",
    "            \"use_float32\": True\n",
    "        },\n",
    "        \"signals\": {\n",
    "            \"sample_rate_hz\": 100.0,     # set None if unknown\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\"\n",
    "        },\n",
    "        \"scores\": {\n",
    "            \"saturation_pct\": 95.0,\n",
    "            \"resid_prominence_pct\": 95.0,\n",
    "            \"min_window_len\": 5\n",
    "        },\n",
    "        \"report\": {\n",
    "            \"enabled\": True,\n",
    "            \"top_n_episodes\": 3,     # still used elsewhere; harmless\n",
    "            \"top_n_per_file\": 2,     # how many episodes per file to show\n",
    "            \"max_pages\": 12,         # cap to avoid huge PDFs\n",
    "            \"pad_points\": 100        # context on each side of an episode in plots\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_config_from_path_or_default(path: Optional[str]) -> dict:\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    print(\"â„¹ï¸  No --config provided or not found. Using in-memory default config.\")\n",
    "    return default_config()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Per-file processing & Pipeline\n",
    "# =========================================================\n",
    "def process_file(file_path: str, cfg: Dict, logger=print) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "\n",
    "    residual_cols = [c for c in df.columns if \"Residual\" in c]\n",
    "    if not residual_cols:\n",
    "        logger(f\"âŒ Skipped {file_name}: No residuals found.\")\n",
    "        return None\n",
    "\n",
    "    X, feature_cols, fe_stats = prepare_features(\n",
    "        df, residual_cols,\n",
    "        window=cfg[\"features\"][\"window\"],\n",
    "        max_features=cfg[\"features\"][\"max_features\"],\n",
    "        logger=logger,\n",
    "    )\n",
    "    if X is None or len(feature_cols) == 0 or X.empty:\n",
    "        logger(f\"âŒ Skipped {file_name}: invalid or empty features\")\n",
    "        return None\n",
    "\n",
    "    _, X_scaled, X_tensor = scale_features(X, use_float32=cfg[\"runtime\"][\"use_float32\"])\n",
    "\n",
    "    iso_labels, iso_scores, iso_thr = isolation_forest_detect(X_scaled, k=cfg[\"threshold\"][\"k\"])\n",
    "    df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
    "    df.loc[X.index, \"iso_score\"] = iso_scores\n",
    "    df.loc[X.index, \"iso_thr\"] = iso_thr\n",
    "\n",
    "    ae_labels, ae_errors, ae_thr = dense_autoencoder_detect(\n",
    "        X_tensor, k=cfg[\"threshold\"][\"k\"], ae_epochs=cfg[\"ae\"][\"epochs\"], ae_lr=cfg[\"ae\"][\"lr\"]\n",
    "    )\n",
    "    df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
    "    df.loc[X.index, \"ae_error\"] = ae_errors\n",
    "    df.loc[X.index, \"ae_thr\"] = ae_thr\n",
    "\n",
    "    lof_labels, lof_scores, lof_thr = lof_detect(\n",
    "        X_scaled, k=cfg[\"threshold\"][\"k\"], n_neighbors=cfg[\"lof\"][\"n_neighbors\"]\n",
    "    )\n",
    "    df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
    "    df.loc[X.index, \"lof_score\"] = lof_scores\n",
    "    df.loc[X.index, \"lof_thr\"] = lof_thr\n",
    "\n",
    "    lstm_labels, lstm_errors, lstm_idx, lstm_thr = lstm_autoencoder_detect(\n",
    "        X_scaled,\n",
    "        k=cfg[\"threshold\"][\"k\"],\n",
    "        seq_len=cfg[\"lstm\"][\"seq_len\"],\n",
    "        hidden_dim=cfg[\"lstm\"][\"hidden_dim\"],\n",
    "        patience=cfg[\"lstm\"][\"patience\"],\n",
    "        max_sequences=cfg[\"lstm\"][\"max_sequences\"],\n",
    "        downsample=cfg[\"lstm\"][\"downsample\"],\n",
    "    )\n",
    "    if len(lstm_idx) > 0:\n",
    "        df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
    "        df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
    "        df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
    "    else:\n",
    "        df[\"lstm_is_anomaly\"] = 0\n",
    "        df[\"lstm_error\"] = np.nan\n",
    "        df[\"lstm_thr\"] = np.nan\n",
    "\n",
    "    # --- Hybrid score (weighted fusion on valid rows)\n",
    "    mask_idx = X.index\n",
    "    df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx)\n",
    "\n",
    "    hs = df.loc[mask_idx, \"hybrid_score\"].to_numpy()\n",
    "    if np.isnan(hs).all():\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = 0\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = np.nan\n",
    "    else:\n",
    "        mode = cfg.get(\"hybrid_threshold\", {}).get(\"mode\", \"mad\")\n",
    "        if mode == \"quantile\":\n",
    "            q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "            thr = np.nanpercentile(hs, 100 * q)\n",
    "            labels = (hs > thr).astype(int)\n",
    "        else:\n",
    "            thr, labels = robust_threshold(hs, k=cfg[\"hybrid_threshold\"].get(\"k\", 3.5), tail=\"high\")\n",
    "            # Fallback if too many positives (MAD degenerate)\n",
    "            if np.nanmean(labels) > 0.5:\n",
    "                q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "                thr = np.nanpercentile(hs, 100 * q)\n",
    "                labels = (hs > thr).astype(int)\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
    "\n",
    "    # Votes (kept for reference/plots)\n",
    "    df = generate_votes(df)\n",
    "\n",
    "    df[\"source_file\"] = file_name\n",
    "    df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
    "    df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n",
    "\n",
    "    logger(\n",
    "        f\"[{file_name}] iso={int(df['is_anomaly'].sum())} | \"\n",
    "        f\"ae={int(df['ae_is_anomaly'].sum())} | \"\n",
    "        f\"lof={int(df['lof_is_anomaly'].sum())} | \"\n",
    "        f\"lstm={int(df['lstm_is_anomaly'].fillna(0).sum())} | \"\n",
    "        f\"hyb={int(df['hybrid_is_anomaly'].sum())} | \"\n",
    "        f\"vote3+={int(df['vote_3plus'].sum())}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Dict):\n",
    "    logger = print\n",
    "\n",
    "    # A) residuals (optional)\n",
    "    if cfg[\"residuals\"][\"enabled\"]:\n",
    "        logger(\"ðŸ”§ Creating residuals...\")\n",
    "        create_residuals_for_folder(\n",
    "            in_folder=cfg[\"io\"][\"input_folder\"],\n",
    "            out_folder=cfg[\"io\"][\"residual_folder\"],\n",
    "            demand_token=cfg[\"residuals\"][\"demand_token\"],\n",
    "            measured_token=cfg[\"residuals\"][\"measured_token\"],\n",
    "            residual_token=cfg[\"residuals\"][\"residual_token\"],\n",
    "            skip_if_exists=True,\n",
    "            suffix=cfg[\"residuals\"][\"suffix\"],\n",
    "            logger=logger,\n",
    "        )\n",
    "        data_folder = cfg[\"io\"][\"residual_folder\"]\n",
    "    else:\n",
    "        data_folder = cfg[\"io\"][\"input_folder\"]\n",
    "\n",
    "    # B) per-file\n",
    "    all_dfs = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            out = process_file(os.path.join(data_folder, file), cfg, logger=logger)\n",
    "            if out is not None:\n",
    "                all_dfs.append(out)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger(\"âŒ No files processed.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    ensure_dir(cfg[\"io\"][\"output_folder\"])\n",
    "\n",
    "    combined_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"combined_anomaly_results.csv\")\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "\n",
    "    # Summary (counts)\n",
    "    cols = [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"]\n",
    "    cols = [c for c in cols if c in combined.columns]\n",
    "    summary = combined.groupby(\"source_file\")[cols].sum()\n",
    "    summary[\"total_anomalies\"] = summary.sum(axis=1)\n",
    "    summary_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_summary.csv\")\n",
    "    summary.to_csv(summary_path)\n",
    "\n",
    "    logger(f\"âœ… Saved row-level: {combined_path}\")\n",
    "    logger(f\"âœ… Saved summary:   {summary_path}\")\n",
    "\n",
    "    # C) Counts plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "    summary[plot_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "    plt.title(\"Anomalies per Model per File\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_plot.png\")\n",
    "    plt.savefig(bar_path); plt.close()\n",
    "    logger(f\"ðŸ–¼ï¸ Saved: {bar_path}\")\n",
    "\n",
    "    # C2) Rate plot (% rows)\n",
    "    if cfg.get(\"plots\", {}).get(\"emit_rate_plot\", True):\n",
    "        sizes = combined.groupby(\"source_file\").size().rename(\"n_rows\")\n",
    "        summary_rates = summary.div(sizes, axis=0) * 100.0\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        rate_cols = [c for c in plot_cols if c in summary_rates.columns]\n",
    "        summary_rates[rate_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "        plt.title(\"Anomaly RATE per Model per File (%)\")\n",
    "        plt.ylabel(\"Percent of rows (%)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        rate_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_rate_plot.png\")\n",
    "        plt.savefig(rate_path); plt.close()\n",
    "        logger(f\"ðŸ–¼ï¸ Saved: {rate_path}\")\n",
    "\n",
    "    # D) Selection (HYBRID by default) + episodes + reasons\n",
    "    selection_rule = cfg.get(\"selection\", {}).get(\"rule\", \"hybrid\")\n",
    "    selected_rows = extract_selected_rows(combined, rule=selection_rule)\n",
    "    sel_dir = os.path.join(cfg[\"io\"][\"output_folder\"], \"selected_outputs\")\n",
    "    ensure_dir(sel_dir)\n",
    "    selected_rows_path = os.path.join(sel_dir, f\"selected_rows_{selection_rule}.csv\")\n",
    "    selected_rows.to_csv(selected_rows_path, index=False)\n",
    "\n",
    "    episodes = summarize_episodes(selected_rows, min_gap=cfg[\"voting\"][\"min_gap\"])\n",
    "    episodes_path = os.path.join(sel_dir, f\"episodes_{selection_rule}.csv\")\n",
    "    episodes.to_csv(episodes_path, index=False)\n",
    "\n",
    "    episodes_with_reasons = attach_episode_reasons(combined, episodes, top_k=1)\n",
    "    episodes_with_reasons = enrich_hardware_mapping(episodes_with_reasons)\n",
    "    episodes_scored = score_episodes(combined, episodes_with_reasons, cfg)\n",
    "\n",
    "    episodes_reason_path = os.path.join(sel_dir, f\"episodes_with_reasons_{selection_rule}.csv\")\n",
    "    episodes_scored_path = os.path.join(sel_dir, f\"episodes_with_reasons_and_scores_{selection_rule}.csv\")\n",
    "    episodes_with_reasons.to_csv(episodes_reason_path, index=False)\n",
    "    episodes_scored.to_csv(episodes_scored_path, index=False)\n",
    "    logger(f\"âœ… Saved episodes+reason: {episodes_reason_path}\")\n",
    "    logger(f\"âœ… Saved episodes+scores: {episodes_scored_path}\")\n",
    "\n",
    "    # Debug: how many episodes per file\n",
    "    print(f\"\\nEPISODES PER FILE (selection rule = {selection_rule}):\")\n",
    "    if not episodes_scored.empty:\n",
    "        print(episodes_scored.groupby(\"source_file\").size().to_string())\n",
    "    else:\n",
    "        print(\"No episodes found under current selection rule.\")\n",
    "\n",
    "    # E) Per-file plots with selected overlays (optional)\n",
    "    if cfg[\"plots\"][\"enabled\"]:\n",
    "        _ = plot_all_files(\n",
    "            combined_df=combined,\n",
    "            out_dir=sel_dir,\n",
    "            rule=selection_rule,\n",
    "            min_gap=cfg[\"voting\"][\"min_gap\"],\n",
    "            max_files=cfg[\"plots\"][\"max_files\"],\n",
    "        )\n",
    "\n",
    "    # F) Sensor table + clustering visuals (using SELECTED mask)\n",
    "    sensor_df = build_sensor_table(combined, selected_rows, episodes_with_reasons=episodes_with_reasons)\n",
    "    sensor_df_path = os.path.join(sel_dir, \"sensor_table.csv\")\n",
    "    sensor_df.to_csv(sensor_df_path, index=False)\n",
    "    logger(f\"âœ… Saved sensor table: {sensor_df_path}\")\n",
    "\n",
    "    clustered, Z2, centers2 = cluster_sensors(sensor_df, n_clusters=3, random_state=42)\n",
    "    _ = plot_sensor_clusters_scatter(clustered, Z2, centers2, out_dir=sel_dir)\n",
    "    _ = plot_sensor_heatmap(sensor_df, out_dir=sel_dir)\n",
    "    _ = plot_sensor_bar_top(sensor_df, out_dir=sel_dir, metric=\"episodes_as_primary\", top_n=15)\n",
    "\n",
    "    # G) PDF report\n",
    "    if cfg.get(\"report\", {}).get(\"enabled\", True):\n",
    "        pdf_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"ops_report.pdf\")\n",
    "        build_ops_report(\n",
    "            combined=combined,\n",
    "            summary=summary,\n",
    "            sensor_df=sensor_df,\n",
    "            episodes_scored=episodes_scored,\n",
    "            cfg=cfg,\n",
    "            out_pdf_path=pdf_path\n",
    "        )\n",
    "        logger(f\"ðŸ“„ Ops report saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Entrypoint\n",
    "# =========================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Anomaly Detection Product\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to config JSON\")\n",
    "    args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "    cfg = load_config_from_path_or_default(args.config)\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa24024-910d-4ef5-863d-d67bddb4e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No --config provided or not found. Using in-memory default config.\n",
      "ðŸ”§ Creating residuals...\n",
      "â†©ï¸  Skip residual (exists): Dataset01_Ski_CrossbeamYawNotPerforming_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset02_Matrix_Rocker4EncoderNotWorking_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset03_Wushu_YawTrapezoidNormal_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset04_Wushu_YawWaveletSqueak_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset05_Wushu_LaneChanges_ModelBump_residual.csv\n",
      "âŒ Failed to read Dataset07_Demo_Spa_GT.csv: No columns to parse from file\n",
      "â†©ï¸  Skip residual (exists): Dataset08_Demo_Jiggler_residual.csv\n",
      "â†©ï¸  Skip residual (exists): Dataset09_Demo_VerticalChirp_residual.csv\n",
      "âŒ Failed to read Dataset10_Demo_MillbrookHills.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1283: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1293: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1319: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1320: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1321: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1350: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_error\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_thr\"] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ LOF skipped (n_rows=180200 > limit).\n",
      "â„¹ï¸ LSTM-AE skipped (n_rows=180200 > limit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:352: RuntimeWarning: All-NaN slice encountered\n",
      "  med = np.nanmedian(x).astype(np.float32)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:353: RuntimeWarning: All-NaN slice encountered\n",
      "  mad = np.nanmedian(np.abs(x - med)).astype(np.float32) + 1e-12\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1356: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx).astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1376: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:423: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:430: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1382: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset01_Ski_CrossbeamYawNotPerforming_residual] iso=6081 | ae=11394 | lof=0 | lstm=0 | hyb=1802 | vote3+=0\n",
      "âŒ Skipped Dataset02_Matrix_Rocker4EncoderNotWorking_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1283: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1293: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1319: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1320: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ LOF skipped (n_rows=381474 > limit).\n",
      "â„¹ï¸ LSTM-AE skipped (n_rows=381474 > limit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1321: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1350: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_error\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_thr\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:352: RuntimeWarning: All-NaN slice encountered\n",
      "  med = np.nanmedian(x).astype(np.float32)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:353: RuntimeWarning: All-NaN slice encountered\n",
      "  mad = np.nanmedian(np.abs(x - med)).astype(np.float32) + 1e-12\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1356: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx).astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1376: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:423: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:430: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1382: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset03_Wushu_YawTrapezoidNormal_residual] iso=4543 | ae=15349 | lof=0 | lstm=0 | hyb=3815 | vote3+=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1283: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1293: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1309: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1310: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = lof_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = lof_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1336: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1337: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1338: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1356: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx).astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1376: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:423: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:430: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1382: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset04_Wushu_YawWaveletSqueak_residual] iso=5912 | ae=5632 | lof=1179 | lstm=610 | hyb=190 | vote3+=663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Generated 168 features (window=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1281: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_score\"] = iso_scores\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1283: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"iso_thr\"] = iso_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_error\"] = ae_errors\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1293: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"ae_thr\"] = ae_thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1319: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1320: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_score\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1321: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[X.index, \"lof_thr\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1350: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_is_anomaly\"] = 0\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_error\"] = np.nan\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"lstm_thr\"] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ LOF skipped (n_rows=131200 > limit).\n",
      "â„¹ï¸ LSTM-AE skipped (n_rows=131200 > limit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:352: RuntimeWarning: All-NaN slice encountered\n",
      "  med = np.nanmedian(x).astype(np.float32)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:353: RuntimeWarning: All-NaN slice encountered\n",
      "  mad = np.nanmedian(np.abs(x - med)).astype(np.float32) + 1e-12\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1356: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx).astype(\"float32\")\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1376: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:423: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"agreement_all_4\"] = (\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:430: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"source_file\"] = file_name\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1382: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset05_Wushu_LaneChanges_ModelBump_residual] iso=7951 | ae=7437 | lof=0 | lstm=0 | hyb=1312 | vote3+=0\n",
      "âŒ Skipped Dataset08_Demo_Jiggler_residual: No residuals found.\n",
      "âŒ Skipped Dataset09_Demo_VerticalChirp_residual: No residuals found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\AppData\\Local\\Temp\\ipykernel_41488\\2716021884.py:1432: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved row-level: ./Anomaly_detection/code/outputs/combined_anomaly_results.csv\n",
      "âœ… Saved summary:   ./Anomaly_detection/code/outputs/model_comparison_summary.csv\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_plot.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/model_comparison_rate_plot.png\n",
      "âœ… Saved episodes+reason: ./Anomaly_detection/code/outputs/selected_outputs\\selected_anomaly_episodes_with_reasons_hybrid.csv\n",
      "âœ… Saved episodes+scores: ./Anomaly_detection/code/outputs/selected_outputs\\selected_anomaly_episodes_with_reasons_and_scores_hybrid.csv\n",
      "\n",
      "EPISODES PER FILE (after selection & scoring):\n",
      "source_file\n",
      "Dataset01_Ski_CrossbeamYawNotPerforming_residual     392\n",
      "Dataset03_Wushu_YawTrapezoidNormal_residual         1015\n",
      "Dataset04_Wushu_YawWaveletSqueak_residual             89\n",
      "Dataset05_Wushu_LaneChanges_ModelBump_residual       622\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/selected_outputs\\selected_plot_Dataset01_Ski_CrossbeamYawNotPerforming_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/selected_outputs\\selected_plot_Dataset03_Wushu_YawTrapezoidNormal_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/selected_outputs\\selected_plot_Dataset04_Wushu_YawWaveletSqueak_residual.png\n",
      "ðŸ–¼ï¸ Saved: ./Anomaly_detection/code/outputs/selected_outputs\\selected_plot_Dataset05_Wushu_LaneChanges_ModelBump_residual.png\n",
      "âœ… Saved sensor table: ./Anomaly_detection/code/outputs/selected_outputs\\sensor_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sugal\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Ops report saved: ./Anomaly_detection/code/outputs/ops_report.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Anomaly Detection Product (single script) â€” Hybrid-driven + Memory-safe\n",
    "- Residual creation (Demand - Measured -> Residual)\n",
    "- Feature engineering (reuse if already present) with float32 downcasting\n",
    "- Scaling once -> shared across models\n",
    "- Models: IsolationForest, LOF, Dense AE, LSTM AE (each can be skipped on huge files)\n",
    "- Dynamic thresholds (MAD) with quantile fallback when needed\n",
    "- HYBRID scoring (weighted fusion) drives selection & episodes by default\n",
    "- Voting (3+) retained for diagnostics; vote_any removed\n",
    "- Episodes per file, robust overlays with context padding\n",
    "- Episode explanations + hardware mapping + root-cause scoring (lag/sat/drift/vibe)\n",
    "- Sensor ranking, clustering & heatmap\n",
    "- Multi-page PDF Ops Report: counts, RATE (%), hybrid histogram, sensors, heatmap, episodes\n",
    "- Memory guards: float32, dataframe downcast, model limits, early gc\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Utils\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in str(name))\n",
    "\n",
    "\n",
    "def downcast_df_inplace(df: pd.DataFrame, prefer_float32: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numeric dtypes to save memory.\"\"\"\n",
    "    for c in df.columns:\n",
    "        col = df[c]\n",
    "        if pd.api.types.is_float_dtype(col) and prefer_float32:\n",
    "            df[c] = pd.to_numeric(col, downcast=\"float\")\n",
    "        elif pd.api.types.is_integer_dtype(col):\n",
    "            df[c] = pd.to_numeric(col, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Residual creation (optional)\n",
    "# =========================================================\n",
    "def create_residuals_for_folder(\n",
    "    in_folder: str,\n",
    "    out_folder: str,\n",
    "    demand_token: str = \"Demand\",\n",
    "    measured_token: str = \"Measured\",\n",
    "    residual_token: str = \"Residual\",\n",
    "    skip_if_exists: bool = True,\n",
    "    suffix: str = \"_residual\",\n",
    "    logger=print,\n",
    ") -> None:\n",
    "    ensure_dir(out_folder)\n",
    "    for file in os.listdir(in_folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(in_folder, file)\n",
    "        out_name = file.replace(\".csv\", f\"{suffix}.csv\")\n",
    "        out_path = os.path.join(out_folder, out_name)\n",
    "\n",
    "        if skip_if_exists and os.path.exists(out_path):\n",
    "            logger(f\"â†©ï¸  Skip residual (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(in_path)\n",
    "        except Exception as e:\n",
    "            logger(f\"âŒ Failed to read {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        made_any = False\n",
    "        for col in cols:\n",
    "            if demand_token in col:\n",
    "                measured_col = col.replace(demand_token, measured_token)\n",
    "                if measured_col in df.columns:\n",
    "                    residual_col = col.replace(demand_token, residual_token)\n",
    "                    df[residual_col] = df[col] - df[measured_col]\n",
    "                    made_any = True\n",
    "\n",
    "        if not made_any:\n",
    "            logger(f\"âš ï¸  No Demand/Measured pairs found in {file}.\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        logger(f\"âœ… Residual CSV saved: {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Scaling + robust threshold (MAD)\n",
    "# =========================================================\n",
    "def scale_features(X: pd.DataFrame, use_float32: bool = True):\n",
    "    \"\"\"\n",
    "    Standardize features once and share across models.\n",
    "    Returns (scaler, X_scaled np.array, X_tensor torch.tensor)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    if use_float32:\n",
    "        X_scaled = X_scaled.astype(\"float32\")\n",
    "    X_tensor = torch.from_numpy(X_scaled)\n",
    "    return scaler, X_scaled, X_tensor\n",
    "\n",
    "\n",
    "def robust_threshold(\n",
    "    values: np.ndarray,\n",
    "    k: float = 3.5,\n",
    "    tail: str = \"high\",\n",
    "    min_anoms: int = 5,\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    MAD-based threshold: median Â± k * 1.4826 * MAD\n",
    "    tail = 'high' (right tail) or 'low' (left tail)\n",
    "    Returns: (threshold, labels) labels aligned to 'values' (1=anomaly)\n",
    "    \"\"\"\n",
    "    v = np.asarray(values)\n",
    "    mask = ~np.isnan(v)\n",
    "    v = v[mask]\n",
    "    if v.size == 0:\n",
    "        return (np.inf if tail == \"high\" else -np.inf), np.zeros_like(values, dtype=int)\n",
    "\n",
    "    med = np.median(v)\n",
    "    mad = np.median(np.abs(v - med)) + 1e-12\n",
    "    if tail == \"high\":\n",
    "        thr = med + k * 1.4826 * mad\n",
    "        labels = (values > thr).astype(int)\n",
    "    else:\n",
    "        thr = med - k * 1.4826 * mad\n",
    "        labels = (values < thr).astype(int)\n",
    "\n",
    "    # relax if too strict on large arrays\n",
    "    if labels.sum() < min_anoms and v.size >= 100:\n",
    "        for k_relax in (3.0, 2.5, 2.0):\n",
    "            if tail == \"high\":\n",
    "                thr = med + k_relax * 1.4826 * mad\n",
    "                labels = (values > thr).astype(int)\n",
    "            else:\n",
    "                thr = med - k_relax * 1.4826 * mad\n",
    "                labels = (values < thr).astype(int)\n",
    "            if labels.sum() >= min_anoms:\n",
    "                break\n",
    "\n",
    "    return thr, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Feature Engineering\n",
    "# =========================================================\n",
    "def prepare_features(\n",
    "    df: pd.DataFrame,\n",
    "    residual_cols: List[str],\n",
    "    window: int = 5,\n",
    "    max_features: int = 500,\n",
    "    logger=print,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Create or reuse features: residual, delta, rolling mean/std\n",
    "    Returns: X, feature_cols, stats (reused vs generated)\n",
    "    \"\"\"\n",
    "    already_done = any(f\"{residual_cols[0]}_delta\" in df.columns for _ in residual_cols)\n",
    "    stats = {\"reused\": 0, \"generated\": 0}\n",
    "\n",
    "    if already_done:\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if any(k in c for k in [\"Residual\", \"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "        ]\n",
    "        X = df[feature_cols].dropna().astype(\"float32\")\n",
    "        stats[\"reused\"] = len(feature_cols)\n",
    "        logger(f\"ðŸ” Reusing {len(feature_cols)} engineered features.\")\n",
    "        return X, feature_cols, stats\n",
    "\n",
    "    # Generate\n",
    "    for col in residual_cols:\n",
    "        df[f\"{col}_delta\"] = df[col].diff().astype(\"float32\")\n",
    "        df[f\"{col}_rolling_mean_{window}\"] = df[col].rolling(window=window).mean().astype(\"float32\")\n",
    "        df[f\"{col}_rolling_std_{window}\"] = df[col].rolling(window=window).std().astype(\"float32\")\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in residual_cols:\n",
    "        feature_cols += [\n",
    "            col,\n",
    "            f\"{col}_delta\",\n",
    "            f\"{col}_rolling_mean_{window}\",\n",
    "            f\"{col}_rolling_std_{window}\",\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].dropna().astype(\"float32\")\n",
    "    stats[\"generated\"] = len(feature_cols)\n",
    "    logger(f\"ðŸ› ï¸  Generated {len(feature_cols)} features (window={window}).\")\n",
    "\n",
    "    if X.shape[1] > max_features:\n",
    "        logger(f\"âŒ Too many features ({X.shape[1]} > {max_features}). Skipping file.\")\n",
    "        return pd.DataFrame(), [], stats\n",
    "\n",
    "    return X, feature_cols, stats\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Models\n",
    "# =========================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 32), nn.ReLU(), nn.Linear(32, 8))\n",
    "        self.decoder = nn.Sequential(nn.Linear(8, 32), nn.ReLU(), nn.Linear(32, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def dense_autoencoder_detect(\n",
    "    X_tensor: torch.Tensor, k: float, ae_epochs: int, ae_lr: float\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    model = Autoencoder(X_tensor.shape[1])\n",
    "    opt = optim.Adam(model.parameters(), lr=ae_lr)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    for _ in range(ae_epochs):\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = crit(out, X_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec = model(X_tensor)\n",
    "        errors = torch.mean((X_tensor - rec) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "    return labels.astype(int), errors, thr\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.encoder(x)  # [1, B, H]\n",
    "        repeated = h.repeat(x.size(1), 1, 1).transpose(0, 1)  # [B, T, H]\n",
    "        decoded, _ = self.decoder(repeated)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def make_sequences(X: np.ndarray, seq_len: int) -> Tuple[np.ndarray, List[int]]:\n",
    "    seqs, idxs = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seqs.append(X[i:i+seq_len])\n",
    "        idxs.append(i + seq_len - 1)\n",
    "    return np.array(seqs), idxs\n",
    "\n",
    "\n",
    "def lstm_autoencoder_detect(\n",
    "    X_scaled: np.ndarray,\n",
    "    k: float,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int,\n",
    "    patience: int,\n",
    "    max_sequences: int,\n",
    "    downsample: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[int], float]:\n",
    "    try:\n",
    "        Xds = X_scaled[::downsample]\n",
    "        if len(Xds) < seq_len:\n",
    "            return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "        Xseq, idxs = make_sequences(Xds, seq_len)\n",
    "        if len(Xseq) > max_sequences:\n",
    "            Xseq, idxs = Xseq[:max_sequences], idxs[:max_sequences]\n",
    "\n",
    "        Xt = torch.tensor(Xseq, dtype=torch.float32)\n",
    "        model = LSTMAutoencoder(Xt.shape[2], hidden_dim)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best, wait = float(\"inf\"), 0\n",
    "        for _ in range(100):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(Xt)\n",
    "            loss = crit(out, Xt)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if loss.item() < best:\n",
    "                best, wait = loss.item(), 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(Xt)\n",
    "            errors = torch.mean((Xt - out) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "        thr, labels = robust_threshold(errors, k=k, tail=\"high\")\n",
    "        return labels.astype(int), errors, idxs, thr\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ LSTM memory error: {e}\")\n",
    "        return np.array([]), np.array([]), [], np.nan\n",
    "\n",
    "\n",
    "def isolation_forest_detect(X_scaled: np.ndarray, k: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    iso = IsolationForest(contamination=\"auto\", n_estimators=300, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    scores = -iso.decision_function(X_scaled)  # higher = more anomalous\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "def lof_detect(X_scaled: np.ndarray, k: float, n_neighbors: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=\"auto\")\n",
    "    _ = lof.fit_predict(X_scaled)  # populates negative_outlier_factor_\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "    thr, labels = robust_threshold(scores, k=k, tail=\"high\")\n",
    "    return labels.astype(int), scores, thr\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hybrid scoring utilities\n",
    "# =========================================================\n",
    "def _robust_z_pos(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Right-tail robust z-score (>=0 when above median).\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    med = np.nanmedian(x).astype(np.float32)\n",
    "    mad = np.nanmedian(np.abs(x - med)).astype(np.float32) + 1e-12\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    return np.maximum(z, 0.0).astype(np.float32)  # only right tail\n",
    "\n",
    "\n",
    "def _percentile01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map to [0,1] by robust percentiles (2â€“98). Values outside clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    lo = np.nanpercentile(x, 2).astype(np.float32)\n",
    "    hi = np.nanpercentile(x, 98).astype(np.float32)\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    y = (x - lo) / rng\n",
    "    return np.clip(y, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_hybrid_score_on_mask(df: pd.DataFrame, cfg: dict, mask_idx) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute hybrid only on valid rows (mask_idx). Returns array the size of df,\n",
    "    NaN elsewhere. Requires >= min_components present.\n",
    "    \"\"\"\n",
    "    out = np.full(len(df), np.nan, dtype=np.float32)\n",
    "    if not cfg.get(\"hybrid\", {}).get(\"enabled\", False):\n",
    "        return out\n",
    "\n",
    "    # Weights guard\n",
    "    wmap = cfg.get(\"hybrid\", {}).get(\"weights\")\n",
    "    if not isinstance(wmap, dict) or not wmap:\n",
    "        wmap = {\"iso_score\": 0.25, \"lof_score\": 0.25, \"ae_error\": 0.25, \"lstm_error\": 0.25}\n",
    "\n",
    "    method = cfg[\"hybrid\"].get(\"method\", \"robust_z\")\n",
    "    min_components = int(cfg[\"hybrid\"].get(\"min_components\", 2))\n",
    "\n",
    "    use = df.loc[mask_idx]  # restrict to valid feature rows\n",
    "\n",
    "    comps = [c for c in [\"iso_score\", \"lof_score\", \"ae_error\", \"lstm_error\"] if c in use.columns and c in wmap]\n",
    "    if not comps:\n",
    "        return out\n",
    "\n",
    "    parts = []\n",
    "    for c in comps:\n",
    "        arr = use[c].to_numpy(dtype=np.float32)\n",
    "        if method == \"robust_z\":\n",
    "            norm = _robust_z_pos(arr)\n",
    "            norm = np.clip(norm, 0, 10.0).astype(np.float32) / 10.0  # ~[0,1]\n",
    "        else:\n",
    "            norm = _percentile01(arr)\n",
    "        parts.append((norm, float(wmap[c])))\n",
    "\n",
    "    num = np.zeros(len(use), dtype=np.float32)\n",
    "    den = np.zeros(len(use), dtype=np.float32)\n",
    "    present = np.zeros(len(use), dtype=np.int32)\n",
    "\n",
    "    for norm, w in parts:\n",
    "        m = ~np.isnan(norm)\n",
    "        num[m] += w * norm[m]\n",
    "        den[m] += w\n",
    "        present[m] += 1\n",
    "\n",
    "    ok = (den > 0) & (present >= min_components)\n",
    "    hybrid_local = np.full(len(use), np.nan, dtype=np.float32)\n",
    "    hybrid_local[ok] = (num[ok] / den[ok]).astype(np.float32)\n",
    "\n",
    "    out[np.asarray(mask_idx)] = hybrid_local\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Voting, selection rules, episodes, explanations\n",
    "# =========================================================\n",
    "def generate_votes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"agreement_all_4\"] = (\n",
    "        (df.get(\"ae_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lof_is_anomaly\", 0) == 1)\n",
    "        & (df.get(\"lstm_is_anomaly\", 0) == 1)\n",
    "    ).astype(int)\n",
    "    df[\"num_votes\"] = df[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1)\n",
    "    df[\"vote_3plus\"] = (df[\"num_votes\"] >= 3).astype(int)\n",
    "    return df  # note: vote-any intentionally removed\n",
    "\n",
    "\n",
    "def extract_selected_rows(df: pd.DataFrame, rule: str = \"hybrid\") -> pd.DataFrame:\n",
    "    if rule == \"hybrid\":\n",
    "        mask = df.get(\"hybrid_is_anomaly\", 0) == 1\n",
    "    elif rule == \"vote_3plus\":\n",
    "        mask = df.get(\"vote_3plus\", 0) == 1\n",
    "    elif rule == \"agreement_all_4\":\n",
    "        mask = df.get(\"agreement_all_4\", 0) == 1\n",
    "    elif rule == \"hybrid_or_vote3p\":\n",
    "        mask = (df.get(\"hybrid_is_anomaly\", 0) == 1) | (df.get(\"vote_3plus\", 0) == 1)\n",
    "    elif rule == \"hybrid_and_vote3p\":\n",
    "        mask = (df.get(\"hybrid_is_anomaly\", 0) == 1) & (df.get(\"vote_3plus\", 0) == 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown selection rule: {rule}\")\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _group_runs(idxs: np.ndarray, min_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "    runs, start, prev = [], int(idxs[0]), int(idxs[0])\n",
    "    for i in idxs[1:]:\n",
    "        if int(i) - prev <= min_gap:\n",
    "            prev = int(i)\n",
    "            continue\n",
    "        runs.append((start, prev))\n",
    "        start = int(i); prev = int(i)\n",
    "    runs.append((start, prev))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def summarize_episodes(selected_df: pd.DataFrame, min_gap: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build episodes PER FILE from the selected rows (hybrid/vote rule).\n",
    "    \"\"\"\n",
    "    if selected_df.empty:\n",
    "        cols = [\"source_file\", \"start_idx\", \"end_idx\", \"length\", \"n_models_mean\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    groups = selected_df.groupby(\"source_file\") if \"source_file\" in selected_df.columns else [(\"\", selected_df)]\n",
    "\n",
    "    for sf, g in groups:\n",
    "        idxs = g.index.to_numpy()\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "        runs = _group_runs(idxs, min_gap=min_gap)\n",
    "        for start, end in runs:\n",
    "            chunk = g.loc[start:end]\n",
    "            row = {\n",
    "                \"source_file\": sf,\n",
    "                \"start_idx\": int(start),\n",
    "                \"end_idx\": int(end),\n",
    "                \"length\": int(end - start + 1),\n",
    "                \"n_models_mean\": float(\n",
    "                    chunk[[\"ae_is_anomaly\", \"is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\"]].sum(axis=1).mean()\n",
    "                ),\n",
    "            }\n",
    "            for c in [\"iso_score\", \"ae_error\", \"lof_score\", \"lstm_error\", \"hybrid_score\"]:\n",
    "                if c in chunk.columns:\n",
    "                    row[f\"{c}_max\"] = float(chunk[c].max())\n",
    "                    row[f\"{c}_mean\"] = float(chunk[c].mean())\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _base_residual_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def _models_string(chunk: pd.DataFrame) -> str:\n",
    "    model_cols = [c for c in [\"is_anomaly\", \"ae_is_anomaly\", \"lof_is_anomaly\", \"lstm_is_anomaly\", \"hybrid_is_anomaly\"] if c in chunk.columns]\n",
    "    if not model_cols:\n",
    "        return \"no-model-flags\"\n",
    "    means = chunk[model_cols].mean()\n",
    "    active = [m.replace(\"_is_anomaly\", \"\").upper() for m, v in means.items() if v >= 0.5]\n",
    "    return \", \".join(active) if active else \"weak/isolated flags\"\n",
    "\n",
    "\n",
    "def attach_episode_reasons(\n",
    "    combined_df: pd.DataFrame, episodes_df: pd.DataFrame, top_k: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "\n",
    "    base_res = _base_residual_columns(combined_df)\n",
    "    if not base_res:\n",
    "        episodes_df[\"primary_signal\"] = \"\"\n",
    "        episodes_df[\"reason\"] = \"no residual columns present\"\n",
    "        episodes_df[\"suspected_sensor\"] = \"\"\n",
    "        return episodes_df\n",
    "\n",
    "    out = []\n",
    "    for _, epi in episodes_df.iterrows():\n",
    "        start, end = int(epi[\"start_idx\"]), int(epi[\"end_idx\"])\n",
    "        mask = combined_df[\"source_file\"] == epi[\"source_file\"] if \"source_file\" in combined_df.columns else slice(None)\n",
    "        chunk = combined_df.loc[mask].loc[start:end]\n",
    "\n",
    "        if chunk.empty:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"empty slice\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats = []\n",
    "        for col in base_res:\n",
    "            if col in chunk.columns:\n",
    "                stats.append((col, float(chunk[col].abs().max())))\n",
    "        if not stats:\n",
    "            epi[\"primary_signal\"] = \"\"\n",
    "            epi[\"reason\"] = \"no residual stats\"\n",
    "            epi[\"suspected_sensor\"] = \"\"\n",
    "            out.append(epi)\n",
    "            continue\n",
    "\n",
    "        stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_signal, primary_val = stats[:top_k][0]\n",
    "        models_str = _models_string(chunk)\n",
    "        measured_col = primary_signal.replace(\"Residual\", \"Measured\")\n",
    "        suspected = measured_col if (measured_col in combined_df.columns) else \"unknown-measured-sensor\"\n",
    "\n",
    "        epi[\"primary_signal\"] = primary_signal\n",
    "        epi[\"reason\"] = f\"max |{primary_signal}| = {primary_val:.3f}; models: {models_str}\"\n",
    "        epi[\"suspected_sensor\"] = suspected\n",
    "        out.append(epi)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Hardware mapping + root cause scoring\n",
    "# =========================================================\n",
    "HARDWARE_MAP = [\n",
    "    (\"Force_\",         \"Actuator/LoadCell\",  \"Force didnâ€™t follow demand â†’ friction/lag/saturation/load-cell drift likely\"),\n",
    "    (\"Encoder_\",       \"Encoders/Alignment\", \"Pose/velocity mismatch â†’ quantization/missing counts/misalignment\"),\n",
    "    (\"Accelerometer_\", \"IMU/Accelerometer\",  \"Vibration bursts â†’ mounting/looseness/thermal drift\"),\n",
    "    (\"State_\",         \"Control/Timing\",     \"Requested vs achieved state diverged â†’ scheduler limits/controller windup\"),\n",
    "]\n",
    "\n",
    "def map_signal_to_hardware(primary_signal: str):\n",
    "    for needle, hw, why in HARDWARE_MAP:\n",
    "        if needle in primary_signal:\n",
    "            return hw, why\n",
    "    return \"Unknown\", \"No mapping rule matched\"\n",
    "\n",
    "\n",
    "def enrich_hardware_mapping(episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    episodes_df = episodes_df.copy()\n",
    "    episodes_df[\"hardware_class\"] = \"\"\n",
    "    episodes_df[\"hardware_why\"] = \"\"\n",
    "    for i, r in episodes_df.iterrows():\n",
    "        hw, why = map_signal_to_hardware(r.get(\"primary_signal\", \"\"))\n",
    "        episodes_df.at[i, \"hardware_class\"] = hw\n",
    "        episodes_df.at[i, \"hardware_why\"]   = why\n",
    "    return episodes_df\n",
    "\n",
    "\n",
    "def _paired_columns(primary_signal: str, cfg: dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "    resid_tok  = cfg[\"signals\"][\"residual_token\"]\n",
    "    demand_tok = cfg[\"signals\"][\"demand_token\"]\n",
    "    measured_tok = cfg[\"signals\"][\"measured_token\"]\n",
    "    if resid_tok not in primary_signal:\n",
    "        return None, None\n",
    "    demand_col   = primary_signal.replace(resid_tok, demand_tok)\n",
    "    measured_col = primary_signal.replace(resid_tok, measured_tok)\n",
    "    return demand_col, measured_col\n",
    "\n",
    "\n",
    "def _nan_ok(arr: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(arr, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_correlation_lag(x: np.ndarray, y: np.ndarray, sample_rate_hz: Optional[float]) -> Tuple[float, int]:\n",
    "    x = _nan_ok(x); y = _nan_ok(y)\n",
    "    if len(x) != len(y) or len(x) == 0:\n",
    "        return (np.nan, 0)\n",
    "    x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    x = np.nan_to_num(x);  y = np.nan_to_num(y)\n",
    "    corr = np.correlate(x, y, mode=\"full\")\n",
    "    lags = np.arange(-len(x)+1, len(x))\n",
    "    k = int(np.argmax(corr))\n",
    "    lag_samples = int(lags[k])\n",
    "    lag_seconds = lag_samples / sample_rate_hz if sample_rate_hz and sample_rate_hz > 0 else np.nan\n",
    "    return (lag_seconds, lag_samples)\n",
    "\n",
    "\n",
    "def _saturation_score(demand: np.ndarray, residual: np.ndarray, cfg: dict) -> float:\n",
    "    if len(demand) == 0 or len(residual) == 0:\n",
    "        return 0.0\n",
    "    p_dem = np.nanpercentile(demand, cfg[\"scores\"][\"saturation_pct\"])\n",
    "    p_res = np.nanpercentile(np.abs(residual), cfg[\"scores\"][\"resid_prominence_pct\"])\n",
    "    near_limit = demand >= p_dem\n",
    "    large_res  = np.abs(residual) >= p_res\n",
    "    both = np.logical_and(near_limit, large_res)\n",
    "    return float(np.nansum(both)) / max(1, len(demand))\n",
    "\n",
    "\n",
    "def _drift_score(residual: np.ndarray) -> float:\n",
    "    residual = _nan_ok(residual)\n",
    "    mu = float(np.nanmean(residual))\n",
    "    sd = float(np.nanstd(residual)) + 1e-9\n",
    "    return abs(mu) / sd\n",
    "\n",
    "\n",
    "def _vibration_score(signal: np.ndarray, sample_rate_hz: Optional[float]) -> float:\n",
    "    if not sample_rate_hz or sample_rate_hz <= 0 or len(signal) < 8:\n",
    "        return np.nan\n",
    "    sig = np.nan_to_num(signal - np.nanmean(signal))\n",
    "    fft = np.fft.rfft(sig)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), d=1.0 / sample_rate_hz)\n",
    "    if len(freqs) == 0:\n",
    "        return np.nan\n",
    "    cutoff = 0.25 * (sample_rate_hz / 2.0)  # > Nyquist/4\n",
    "    mask_hi = freqs >= cutoff\n",
    "    num = float(np.nansum(power[mask_hi]))\n",
    "    den = float(np.nansum(power) + 1e-12)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def score_episodes(combined_df: pd.DataFrame, episodes_df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds: lag_seconds, lag_samples, saturation_score, drift_score, vibe_score\n",
    "    \"\"\"\n",
    "    if episodes_df.empty:\n",
    "        return episodes_df\n",
    "    out = episodes_df.copy()\n",
    "    sr = cfg[\"signals\"][\"sample_rate_hz\"]\n",
    "    min_len = cfg[\"scores\"][\"min_window_len\"]\n",
    "\n",
    "    if \"primary_signal\" not in out.columns:\n",
    "        out[\"primary_signal\"] = \"\"\n",
    "\n",
    "    for i, r in out.iterrows():\n",
    "        start, end = int(r[\"start_idx\"]), int(r[\"end_idx\"])\n",
    "        if end - start + 1 < min_len:\n",
    "            out.at[i, \"lag_seconds\"] = np.nan\n",
    "            out.at[i, \"lag_samples\"] = 0\n",
    "            out.at[i, \"saturation_score\"] = 0.0\n",
    "            out.at[i, \"drift_score\"] = 0.0\n",
    "            out.at[i, \"vibe_score\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        if \"source_file\" in combined_df.columns and \"source_file\" in out.columns and \"source_file\" in r:\n",
    "            chunk = combined_df.loc[(combined_df[\"source_file\"] == r[\"source_file\"])].loc[start:end]\n",
    "        else:\n",
    "            chunk = combined_df.loc[start:end]\n",
    "\n",
    "        primary = r.get(\"primary_signal\", \"\")\n",
    "        demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "\n",
    "        resid = chunk[primary].values if (primary in chunk.columns) else np.array([])\n",
    "        dem   = chunk[demand_col].values if (demand_col and demand_col in chunk.columns) else np.array([])\n",
    "        meas  = chunk[measured_col].values if (measured_col and measured_col in chunk.columns) else np.array([])\n",
    "\n",
    "        lag_s, lag_k = _cross_correlation_lag(dem, meas, sr) if (len(dem) and len(meas)) else (np.nan, 0)\n",
    "        sat_sc = _saturation_score(dem, resid, cfg) if (len(dem) and len(resid)) else 0.0\n",
    "        dr_sc  = _drift_score(resid) if len(resid) else 0.0\n",
    "        if \"Accelerometer_\" in primary and primary in chunk.columns:\n",
    "            vibe_sc = _vibration_score(chunk[primary].values, sr)\n",
    "        else:\n",
    "            vibe_sc = _vibration_score(resid, sr)\n",
    "\n",
    "        out.at[i, \"lag_seconds\"]       = lag_s\n",
    "        out.at[i, \"lag_samples\"]       = int(lag_k)\n",
    "        out.at[i, \"saturation_score\"]  = float(sat_sc)\n",
    "        out.at[i, \"drift_score\"]       = float(dr_sc)\n",
    "        out.at[i, \"vibe_score\"]        = float(vibe_sc) if vibe_sc == vibe_sc else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Plotting helpers (per-file overlays for selected rows)\n",
    "# =========================================================\n",
    "def _pick_residual(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand = [c for c in df.columns if \"Residual\" in c and not any(t in c for t in [\"_delta\", \"_rolling_\"])]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def _slice_by_global_index(sub: pd.DataFrame, start: int, end: int, pad: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust slice of `sub` (filtered to one file) using global indices [start, end],\n",
    "    expanded by `pad` points on both sides. Works even when index has gaps.\n",
    "    \"\"\"\n",
    "    if sub.empty:\n",
    "        return sub\n",
    "    idx = sub.index.to_numpy()\n",
    "    i0 = np.searchsorted(idx, start, side=\"left\")\n",
    "    i1 = np.searchsorted(idx, end,   side=\"right\")\n",
    "    i0 = max(i0 - pad, 0)\n",
    "    i1 = min(i1 + pad, len(idx))\n",
    "    return sub.iloc[i0:i1]\n",
    "\n",
    "\n",
    "def plot_selected_for_file(\n",
    "    df_file: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    rule: str,\n",
    "    min_gap: int,\n",
    "    figsize: Tuple[int, int] = (12, 5),\n",
    ") -> Optional[str]:\n",
    "    ensure_dir(out_dir)\n",
    "    residual_col = _pick_residual(df_file)\n",
    "    if residual_col is None:\n",
    "        print(\"âš ï¸ No residual column to plot.\")\n",
    "        return None\n",
    "\n",
    "    selected_rows = extract_selected_rows(df_file, rule=rule)\n",
    "    episodes = summarize_episodes(selected_rows, min_gap=min_gap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_file.index, df_file[residual_col], label=residual_col, alpha=0.85)\n",
    "\n",
    "    if not selected_rows.empty:\n",
    "        plt.scatter(selected_rows.index, selected_rows[residual_col], s=12, label=f\"Selected anomalies ({rule})\")\n",
    "\n",
    "    if not episodes.empty:\n",
    "        for _, r in episodes.iterrows():\n",
    "            plt.axvspan(r[\"start_idx\"], r[\"end_idx\"], alpha=0.15, label=\"Episode\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uniq, seen = [], set()\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in seen:\n",
    "                uniq.append((h, l)); seen.add(l)\n",
    "        handles, labels = zip(*uniq)\n",
    "        plt.legend(handles, labels)\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    sf = df_file[\"source_file\"].iloc[0] if \"source_file\" in df_file.columns else \"file\"\n",
    "    plt.title(f\"{sf} â€” Residual with selected anomalies & episodes\")\n",
    "    plt.xlabel(\"Index\"); plt.ylabel(residual_col)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"selected_plot_{safe_name(sf)}.png\")\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_all_files(combined_df: pd.DataFrame, out_dir: str, rule: str, min_gap: int, max_files: Optional[int] = None):\n",
    "    paths = []\n",
    "    if \"source_file\" not in combined_df.columns:\n",
    "        print(\"âš ï¸ combined_df missing 'source_file'.\")\n",
    "        return paths\n",
    "    groups = list(combined_df.groupby(\"source_file\"))\n",
    "    if max_files is not None:\n",
    "        groups = groups[:max_files]\n",
    "    for fname, df_file in groups:\n",
    "        p = plot_selected_for_file(df_file, out_dir=out_dir, rule=rule, min_gap=min_gap)\n",
    "        if p:\n",
    "            paths.append(p); print(f\"ðŸ–¼ï¸ Saved: {p}\")\n",
    "    if not paths:\n",
    "        print(\"âš ï¸ No plots produced.\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Sensor attribution, clustering & heatmap\n",
    "# =========================================================\n",
    "def _residual_cols_base(df: pd.DataFrame) -> List[str]:\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if (\"Residual\" in c) and not any(tag in c for tag in [\"_delta\", \"_rolling_mean\", \"_rolling_std\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_sensor_table(\n",
    "    combined: pd.DataFrame,\n",
    "    selected_rows: pd.DataFrame,\n",
    "    episodes_with_reasons: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    base_res = _residual_cols_base(combined)\n",
    "    if not base_res:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_rows = len(combined)\n",
    "    sel_mask = pd.Series(False, index=combined.index)\n",
    "    if not selected_rows.empty:\n",
    "        sel_mask.loc[selected_rows.index] = True\n",
    "\n",
    "    rows = []\n",
    "    expected_keys = [\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "        \"episodes_as_primary\",\n",
    "    ]\n",
    "\n",
    "    for col in base_res:\n",
    "        stats = {\"sensor\": col}\n",
    "\n",
    "        stats[\"anomaly_rate_is\"]   = float(combined[\"is_anomaly\"].sum())   / max(total_rows, 1) if \"is_anomaly\"   in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_ae\"]   = float(combined[\"ae_is_anomaly\"].sum())/ max(total_rows, 1) if \"ae_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lof\"]  = float(combined[\"lof_is_anomaly\"].sum())/max(total_rows, 1) if \"lof_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_lstm\"] = float(combined[\"lstm_is_anomaly\"].fillna(0).sum())/max(total_rows, 1) if \"lstm_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_hybrid\"] = float(combined[\"hybrid_is_anomaly\"].sum())/max(total_rows, 1) if \"hybrid_is_anomaly\" in combined.columns else 0.0\n",
    "        stats[\"anomaly_rate_vote3p\"] = float(combined[\"vote_3plus\"].sum())/max(total_rows, 1) if \"vote_3plus\" in combined.columns else 0.0\n",
    "\n",
    "        if col in combined.columns and sel_mask.any():\n",
    "            vals = combined.loc[sel_mask, col].abs()\n",
    "            stats[\"mean_abs_resid_voted\"] = float(vals.mean()) if not vals.empty else 0.0  # (name kept for compatibility)\n",
    "            stats[\"max_abs_resid_voted\"]  = float(vals.max())  if not vals.empty else 0.0\n",
    "        else:\n",
    "            stats[\"mean_abs_resid_voted\"] = 0.0\n",
    "            stats[\"max_abs_resid_voted\"]  = 0.0\n",
    "\n",
    "        if episodes_with_reasons is not None and not episodes_with_reasons.empty and \"primary_signal\" in episodes_with_reasons.columns:\n",
    "            stats[\"episodes_as_primary\"] = int((episodes_with_reasons[\"primary_signal\"] == col).sum())\n",
    "        else:\n",
    "            stats[\"episodes_as_primary\"] = 0\n",
    "\n",
    "        for k in expected_keys:\n",
    "            stats.setdefault(k, 0.0)\n",
    "\n",
    "        rows.append(stats)\n",
    "\n",
    "    sensor_df = pd.DataFrame(rows)\n",
    "    for c in sensor_df.columns:\n",
    "        if c != \"sensor\":\n",
    "            sensor_df[c] = sensor_df[c].fillna(0.0)\n",
    "    return sensor_df\n",
    "\n",
    "\n",
    "def cluster_sensors(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    n_clusters: int = 3,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    if sensor_df.empty or \"sensor\" not in sensor_df.columns:\n",
    "        return sensor_df, np.empty((0, 2)), np.empty((0, 2))\n",
    "\n",
    "    features = sensor_df.drop(columns=[\"sensor\"]).to_numpy(dtype=np.float32)\n",
    "    if features.shape[0] < n_clusters:\n",
    "        n_clusters = max(1, features.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(features)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(Z)\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    Z2 = pca.fit_transform(Z)\n",
    "    centers2 = pca.transform(km.cluster_centers_)\n",
    "\n",
    "    out = sensor_df.copy()\n",
    "    out[\"cluster\"] = labels\n",
    "\n",
    "    return out, Z2, centers2\n",
    "\n",
    "\n",
    "def plot_sensor_bar_top(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metric: str = \"episodes_as_primary\",\n",
    "    top_n: int = 15,\n",
    "    title: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty or metric not in sensor_df.columns:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    df = sensor_df.sort_values(metric, ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(df)), df[metric])\n",
    "    plt.xticks(range(len(df)), [s.replace(\"Force_\", \"F_\") for s in df[\"sensor\"]], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title or f\"Top {top_n} sensors by {metric}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"top_sensors_{metric}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_clusters_scatter(\n",
    "    sensor_df_with_cluster: pd.DataFrame,\n",
    "    Z2: np.ndarray,\n",
    "    centers2: np.ndarray,\n",
    "    out_dir: str,\n",
    "    title: str = \"Sensor clusters (PCA of features)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df_with_cluster.empty or Z2.size == 0:\n",
    "        return None\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    clusters = sorted(sensor_df_with_cluster[\"cluster\"].unique().tolist())\n",
    "    for cl in clusters:\n",
    "        mask = sensor_df_with_cluster[\"cluster\"] == cl\n",
    "        pts = Z2[mask.values]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"cluster {cl}\", alpha=0.8, s=36)\n",
    "\n",
    "    if centers2.size:\n",
    "        plt.scatter(centers2[:, 0], centers2[:, 1], marker=\"X\", s=120, label=\"centers\")\n",
    "\n",
    "    try:\n",
    "        top_lab = sensor_df_with_cluster.sort_values(\"episodes_as_primary\", ascending=False).head(10).index\n",
    "        for idx in top_lab:\n",
    "            plt.text(Z2[idx, 0], Z2[idx, 1], sensor_df_with_cluster.loc[idx, \"sensor\"], fontsize=8)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_clusters_pca.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_sensor_heatmap(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    title: str = \"Sensor anomaly fingerprint (rates & magnitudes)\",\n",
    ") -> Optional[str]:\n",
    "    if sensor_df.empty:\n",
    "        return None\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    desired = [\n",
    "        \"anomaly_rate_vote3p\",\n",
    "        \"anomaly_rate_hybrid\",\n",
    "        \"anomaly_rate_ae\",\n",
    "        \"anomaly_rate_is\",\n",
    "        \"anomaly_rate_lof\",\n",
    "        \"anomaly_rate_lstm\",\n",
    "        \"mean_abs_resid_voted\",\n",
    "        \"max_abs_resid_voted\",\n",
    "    ]\n",
    "    if metrics is None:\n",
    "        metrics = desired\n",
    "\n",
    "    available = [m for m in metrics if m in sensor_df.columns]\n",
    "    if not available:\n",
    "        print(\"âš ï¸ No requested heatmap metrics are present in sensor_df. Skipping heatmap.\")\n",
    "        return None\n",
    "    if len(available) < len(metrics):\n",
    "        missing = [m for m in metrics if m not in sensor_df.columns]\n",
    "        print(f\"â„¹ï¸ Skipping missing metrics in heatmap: {missing}\")\n",
    "    metrics = available\n",
    "\n",
    "    key_rank = \"episodes_as_primary\" if \"episodes_as_primary\" in sensor_df.columns else metrics[0]\n",
    "    keep = sensor_df.sort_values(key_rank, ascending=False).head(25)\n",
    "\n",
    "    M = keep[metrics].to_numpy(dtype=np.float32)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(M, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.yticks(range(len(keep)), keep[\"sensor\"])\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, \"sensor_fingerprint_heatmap.png\")\n",
    "    plt.savefig(path, dpi=160); plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Report (adds hybrid histogram page)\n",
    "# =========================================================\n",
    "def _overlay_episode_plot(df: pd.DataFrame, episode_row: pd.Series, cfg: dict, ax=None):\n",
    "    start, end = int(episode_row[\"start_idx\"]), int(episode_row[\"end_idx\"])\n",
    "    primary = episode_row.get(\"primary_signal\", \"\")\n",
    "    demand_col, measured_col = _paired_columns(primary, cfg)\n",
    "    pad = int(cfg.get(\"report\", {}).get(\"pad_points\", 100))\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Filter to the right file first\n",
    "    if \"source_file\" in df.columns and \"source_file\" in episode_row:\n",
    "        df = df.loc[df[\"source_file\"] == episode_row[\"source_file\"]]\n",
    "\n",
    "    window = _slice_by_global_index(df, start, end, pad=pad)\n",
    "    if window.empty:\n",
    "        ax.set_title(f\"Episode {start}â€“{end} (EMPTY SLICE)\")\n",
    "        return\n",
    "\n",
    "    t = window.index.to_numpy()\n",
    "\n",
    "    if primary in window.columns:\n",
    "        ax.plot(t, window[primary].values, label=f\"{primary}\", alpha=0.9)\n",
    "    if demand_col and demand_col in window.columns:\n",
    "        ax.plot(t, window[demand_col].values, label=f\"{demand_col}\", alpha=0.8)\n",
    "    if measured_col and measured_col in window.columns:\n",
    "        ax.plot(t, window[measured_col].values, label=f\"{measured_col}\", alpha=0.8)\n",
    "\n",
    "    ax.axvspan(start, end, alpha=0.15, label=\"episode window\")\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_title(f\"Episode {start}â€“{end}\\nprimary={primary}\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def build_ops_report(\n",
    "    combined: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    sensor_df: pd.DataFrame,\n",
    "    episodes_scored: pd.DataFrame,\n",
    "    cfg: dict,\n",
    "    out_pdf_path: str\n",
    "):\n",
    "    ensure_dir(os.path.dirname(out_pdf_path))\n",
    "    with PdfPages(out_pdf_path) as pdf:\n",
    "\n",
    "        # Page 1 â€” Anomaly counts by model/file\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "        summary[plot_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomalies per Model per File\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 2 â€” Anomaly RATE (%) per model/file\n",
    "        sizes = combined.groupby(\"source_file\").size().rename(\"n_rows\")\n",
    "        summary_rates = summary.div(sizes, axis=0) * 100.0\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        rate_cols = [c for c in plot_cols if c in summary_rates.columns]\n",
    "        summary_rates[rate_cols].plot(kind=\"bar\")\n",
    "        plt.title(\"Anomaly RATE per Model per File (%)\")\n",
    "        plt.ylabel(\"Percent of rows (%)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 3 â€” Hybrid score histogram with threshold\n",
    "        hs = combined.get(\"hybrid_score\", pd.Series([], dtype=\"float32\")).dropna().to_numpy(dtype=np.float32)\n",
    "        if hs.size > 0:\n",
    "            thr_vals = combined.get(\"hybrid_thr\", pd.Series([], dtype=\"float32\")).dropna().to_numpy(dtype=np.float32)\n",
    "            thr = float(np.nanmedian(thr_vals)) if thr_vals.size > 0 else float(np.nanpercentile(hs, 99))\n",
    "            plt.figure(figsize=(11, 6))\n",
    "            plt.hist(hs, bins=60)\n",
    "            plt.axvline(thr, linestyle=\"--\", label=f\"hybrid_thrâ‰ˆ{thr:.3f}\")\n",
    "            plt.title(\"Hybrid score distribution\")\n",
    "            plt.xlabel(\"hybrid_score\"); plt.ylabel(\"count\")\n",
    "            plt.legend(); plt.tight_layout()\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 4 â€” Top sensors by episodes_as_primary\n",
    "        p1 = plot_sensor_bar_top(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"], metric=\"episodes_as_primary\", top_n=15,\n",
    "                                 title=\"Top sensors by episodes_as_primary\")\n",
    "        if p1 and os.path.exists(p1):\n",
    "            img = plt.imread(p1)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Page 5 â€” Sensor heatmap (if created)\n",
    "        p2 = plot_sensor_heatmap(sensor_df, out_dir=cfg[\"io\"][\"output_folder\"])\n",
    "        if p2 and os.path.exists(p2):\n",
    "            img = plt.imread(p2)\n",
    "            plt.figure(figsize=(11, 6)); plt.imshow(img); plt.axis(\"off\")\n",
    "            pdf.savefig(); plt.close()\n",
    "\n",
    "        # Pages â€” Episode overlays (per-file selection with caps)\n",
    "        if not episodes_scored.empty:\n",
    "            candidates = episodes_scored.copy()\n",
    "            sort_keys = [c for c in [\"n_models_mean\", \"hybrid_score_mean\", \"iso_score_mean\", \"ae_error_mean\"] if c in candidates.columns]\n",
    "            if sort_keys:\n",
    "                candidates = candidates.sort_values(sort_keys, ascending=False)\n",
    "\n",
    "            n_per_file = int(cfg.get(\"report\", {}).get(\"top_n_per_file\", 2))\n",
    "            max_pages  = int(cfg.get(\"report\", {}).get(\"max_pages\", 12))\n",
    "\n",
    "            pages = 0\n",
    "            for sf, grp in candidates.groupby(\"source_file\"):\n",
    "                for _, epi in grp.head(n_per_file).iterrows():\n",
    "                    if pages >= max_pages:\n",
    "                        break\n",
    "                    plt.figure(figsize=(11, 5))\n",
    "                    _overlay_episode_plot(combined, epi, cfg, ax=plt.gca())\n",
    "                    hw = epi.get(\"hardware_class\", \"Unknown\")\n",
    "                    why = epi.get(\"hardware_why\", \"\")\n",
    "                    lag_s = epi.get(\"lag_seconds\", np.nan)\n",
    "                    sat   = epi.get(\"saturation_score\", np.nan)\n",
    "                    drift = epi.get(\"drift_score\", np.nan)\n",
    "                    vibe  = epi.get(\"vibe_score\", np.nan)\n",
    "                    txt = (\n",
    "                        f\"hardware: {hw}\\n\"\n",
    "                        f\"why: {why}\\n\"\n",
    "                        f\"lag_seconds: {lag_s:.4f}  |  saturation: {sat:.3f}  |  drift: {drift:.3f}  |  vibe: {vibe:.3f}\"\n",
    "                    )\n",
    "                    plt.gcf().text(0.02, 0.02, txt, ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(); plt.close()\n",
    "                    pages += 1\n",
    "                if pages >= max_pages:\n",
    "                    break\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Config (defaults or JSON)\n",
    "# =========================================================\n",
    "def default_config() -> dict:\n",
    "    return {\n",
    "        \"io\": {\n",
    "            \"input_folder\": \"./Datasets/Datasets\",\n",
    "            \"residual_folder\": \"./Anomaly_detection/residual_created/\",\n",
    "            \"output_folder\": \"./Anomaly_detection/code/outputs/\"\n",
    "        },\n",
    "        \"residuals\": {\n",
    "            \"enabled\": True,\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\",\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"suffix\": \"_residual\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"window\": 5,\n",
    "            \"max_features\": 500\n",
    "        },\n",
    "        \"threshold\": {  # per-model MAD k\n",
    "            \"k\": 3.5\n",
    "        },\n",
    "        \"ae\": {\n",
    "            \"epochs\": 50,\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"seq_len\": 5,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"patience\": 5,\n",
    "            \"max_sequences\": 2000,\n",
    "            \"downsample\": 10\n",
    "        },\n",
    "        \"lof\": {\n",
    "            \"n_neighbors\": 20\n",
    "        },\n",
    "        \"hybrid\": {                     # Hybrid scoring config\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"robust_z\",      # \"robust_z\" | \"percentile\"\n",
    "            \"min_components\": 2,       # require at least N model scores present\n",
    "            \"weights\": {               # relative importance\n",
    "                \"iso_score\": 0.20,\n",
    "                \"lof_score\": 0.20,\n",
    "                \"ae_error\": 0.30,\n",
    "                \"lstm_error\": 0.30\n",
    "            }\n",
    "        },\n",
    "        \"hybrid_threshold\": {          # How to threshold hybrid_score\n",
    "            \"mode\": \"quantile\",        # \"mad\" or \"quantile\"\n",
    "            \"k\": 3.5,                  # used only if mode=\"mad\"\n",
    "            \"quantile\": 0.99           # top 1% as anomalies (fallback if MAD degenerates)\n",
    "        },\n",
    "        \"selection\": {\n",
    "            \"rule\": \"hybrid\"           # \"hybrid\" | \"vote_3plus\" | \"agreement_all_4\" | \"hybrid_or_vote3p\" | \"hybrid_and_vote3p\"\n",
    "        },\n",
    "        \"voting\": {\n",
    "            \"rule\": \"vote_3plus\",      # kept for diagnostics\n",
    "            \"min_gap\": 1\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_files\": None,\n",
    "            \"emit_rate_plot\": True\n",
    "        },\n",
    "        \"runtime\": {\n",
    "            \"use_float32\": True,\n",
    "            \"downcast_dataframe\": True\n",
    "        },\n",
    "        \"limits\": {                    # memory guards (rows in full CSV)\n",
    "            \"max_rows_lof\": 80000,\n",
    "            \"max_rows_lstm\": 120000,\n",
    "            \"max_rows_ae\": 600000\n",
    "        },\n",
    "        \"signals\": {\n",
    "            \"sample_rate_hz\": 100.0,     # set None if unknown\n",
    "            \"residual_token\": \"Residual\",\n",
    "            \"demand_token\": \"Demand\",\n",
    "            \"measured_token\": \"Measured\"\n",
    "        },\n",
    "        \"scores\": {\n",
    "            \"saturation_pct\": 95.0,\n",
    "            \"resid_prominence_pct\": 95.0,\n",
    "            \"min_window_len\": 5\n",
    "        },\n",
    "        \"report\": {\n",
    "            \"enabled\": True,\n",
    "            \"top_n_episodes\": 3,     # legacy; not used directly\n",
    "            \"top_n_per_file\": 2,     # how many episodes per file to show\n",
    "            \"max_pages\": 12,         # cap to avoid huge PDFs\n",
    "            \"pad_points\": 100        # context on each side of an episode in plots\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_config_from_path_or_default(path: Optional[str]) -> dict:\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    print(\"â„¹ï¸  No --config provided or not found. Using in-memory default config.\")\n",
    "    return default_config()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Per-file processing & Pipeline\n",
    "# =========================================================\n",
    "def process_file(file_path: str, cfg: Dict, logger=print) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    if cfg.get(\"runtime\", {}).get(\"downcast_dataframe\", True):\n",
    "        downcast_df_inplace(df, prefer_float32=True)\n",
    "\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "\n",
    "    residual_cols = [c for c in df.columns if \"Residual\" in c]\n",
    "    if not residual_cols:\n",
    "        logger(f\"âŒ Skipped {file_name}: No residuals found.\")\n",
    "        return None\n",
    "\n",
    "    X, feature_cols, fe_stats = prepare_features(\n",
    "        df, residual_cols,\n",
    "        window=cfg[\"features\"][\"window\"],\n",
    "        max_features=cfg[\"features\"][\"max_features\"],\n",
    "        logger=logger,\n",
    "    )\n",
    "    if X is None or len(feature_cols) == 0 or X.empty:\n",
    "        logger(f\"âŒ Skipped {file_name}: invalid or empty features\")\n",
    "        return None\n",
    "\n",
    "    _, X_scaled, X_tensor = scale_features(X, use_float32=cfg[\"runtime\"][\"use_float32\"])\n",
    "\n",
    "    # Memory-aware limits\n",
    "    n_rows = len(df)\n",
    "    limits = cfg.get(\"limits\", {})\n",
    "    skip_lof  = n_rows > int(limits.get(\"max_rows_lof\", 8e4))\n",
    "    skip_lstm = n_rows > int(limits.get(\"max_rows_lstm\", 1.2e5))\n",
    "    skip_ae   = n_rows > int(limits.get(\"max_rows_ae\", 6e5))\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_labels, iso_scores, iso_thr = isolation_forest_detect(X_scaled, k=cfg[\"threshold\"][\"k\"])\n",
    "    df.loc[X.index, \"is_anomaly\"] = iso_labels\n",
    "    df.loc[X.index, \"iso_score\"] = iso_scores\n",
    "    df.loc[X.index, \"iso_thr\"] = iso_thr\n",
    "\n",
    "    # Dense AE (skip if huge)\n",
    "    if not skip_ae:\n",
    "        try:\n",
    "            ae_labels, ae_errors, ae_thr = dense_autoencoder_detect(\n",
    "                X_tensor, k=cfg[\"threshold\"][\"k\"], ae_epochs=cfg[\"ae\"][\"epochs\"], ae_lr=cfg[\"ae\"][\"lr\"]\n",
    "            )\n",
    "            df.loc[X.index, \"ae_is_anomaly\"] = ae_labels\n",
    "            df.loc[X.index, \"ae_error\"] = ae_errors\n",
    "            df.loc[X.index, \"ae_thr\"] = ae_thr\n",
    "        except MemoryError:\n",
    "            print(\"âš ï¸ AE skipped due to memory.\")\n",
    "            df.loc[X.index, \"ae_is_anomaly\"] = 0\n",
    "            df.loc[X.index, \"ae_error\"] = np.nan\n",
    "            df.loc[X.index, \"ae_thr\"] = np.nan\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ AE skipped (n_rows={n_rows} > limit).\")\n",
    "        df.loc[X.index, \"ae_is_anomaly\"] = 0\n",
    "        df.loc[X.index, \"ae_error\"] = np.nan\n",
    "        df.loc[X.index, \"ae_thr\"] = np.nan\n",
    "\n",
    "    # LOF (skip if huge)\n",
    "    if not skip_lof:\n",
    "        try:\n",
    "            lof_labels, lof_scores, lof_thr = lof_detect(X_scaled, k=cfg[\"threshold\"][\"k\"], n_neighbors=cfg[\"lof\"][\"n_neighbors\"])\n",
    "            df.loc[X.index, \"lof_is_anomaly\"] = lof_labels\n",
    "            df.loc[X.index, \"lof_score\"] = lof_scores\n",
    "            df.loc[X.index, \"lof_thr\"] = lof_thr\n",
    "        except MemoryError:\n",
    "            print(\"âš ï¸ LOF skipped due to memory.\")\n",
    "            df.loc[X.index, \"lof_is_anomaly\"] = 0\n",
    "            df.loc[X.index, \"lof_score\"] = np.nan\n",
    "            df.loc[X.index, \"lof_thr\"] = np.nan\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ LOF skipped (n_rows={n_rows} > limit).\")\n",
    "        df.loc[X.index, \"lof_is_anomaly\"] = 0\n",
    "        df.loc[X.index, \"lof_score\"] = np.nan\n",
    "        df.loc[X.index, \"lof_thr\"] = np.nan\n",
    "\n",
    "    # LSTM AE (skip if huge)\n",
    "    if not skip_lstm:\n",
    "        try:\n",
    "            lstm_labels, lstm_errors, lstm_idx, lstm_thr = lstm_autoencoder_detect(\n",
    "                X_scaled,\n",
    "                k=cfg[\"threshold\"][\"k\"],\n",
    "                seq_len=cfg[\"lstm\"][\"seq_len\"],\n",
    "                hidden_dim=cfg[\"lstm\"][\"hidden_dim\"],\n",
    "                patience=cfg[\"lstm\"][\"patience\"],\n",
    "                max_sequences=cfg[\"lstm\"][\"max_sequences\"],\n",
    "                downsample=cfg[\"lstm\"][\"downsample\"],\n",
    "            )\n",
    "            if len(lstm_idx) > 0:\n",
    "                df.loc[df.index[lstm_idx], \"lstm_is_anomaly\"] = lstm_labels\n",
    "                df.loc[df.index[lstm_idx], \"lstm_error\"] = lstm_errors\n",
    "                df.loc[df.index[lstm_idx], \"lstm_thr\"] = lstm_thr\n",
    "            else:\n",
    "                df[\"lstm_is_anomaly\"] = 0\n",
    "                df[\"lstm_error\"] = np.nan\n",
    "                df[\"lstm_thr\"] = np.nan\n",
    "        except MemoryError:\n",
    "            print(\"âš ï¸ LSTM-AE skipped due to memory.\")\n",
    "            df[\"lstm_is_anomaly\"] = 0\n",
    "            df[\"lstm_error\"] = np.nan\n",
    "            df[\"lstm_thr\"] = np.nan\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ LSTM-AE skipped (n_rows={n_rows} > limit).\")\n",
    "        df[\"lstm_is_anomaly\"] = 0\n",
    "        df[\"lstm_error\"] = np.nan\n",
    "        df[\"lstm_thr\"] = np.nan\n",
    "\n",
    "    # --- Hybrid score (weighted fusion on valid rows)\n",
    "    mask_idx = X.index\n",
    "    df[\"hybrid_score\"] = compute_hybrid_score_on_mask(df, cfg, mask_idx).astype(\"float32\")\n",
    "\n",
    "    hs = df.loc[mask_idx, \"hybrid_score\"].to_numpy(dtype=np.float32)\n",
    "    if np.isnan(hs).all():\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = 0\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = np.nan\n",
    "    else:\n",
    "        mode = cfg.get(\"hybrid_threshold\", {}).get(\"mode\", \"mad\")\n",
    "        if mode == \"quantile\":\n",
    "            q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "            thr = np.nanpercentile(hs, 100 * q)\n",
    "            labels = (hs > thr).astype(int)\n",
    "        else:\n",
    "            thr, labels = robust_threshold(hs, k=cfg[\"hybrid_threshold\"].get(\"k\", 3.5), tail=\"high\")\n",
    "            # Fallback if too many positives (MAD degenerate)\n",
    "            if np.nanmean(labels) > 0.5:\n",
    "                q = float(cfg[\"hybrid_threshold\"].get(\"quantile\", 0.98))\n",
    "                thr = np.nanpercentile(hs, 100 * q)\n",
    "                labels = (hs > thr).astype(int)\n",
    "        df.loc[mask_idx, \"hybrid_is_anomaly\"] = labels\n",
    "        df.loc[mask_idx, \"hybrid_thr\"] = thr\n",
    "\n",
    "    # Add votes (for diagnostics)\n",
    "    df = generate_votes(df)\n",
    "\n",
    "    df[\"source_file\"] = file_name\n",
    "    df[\"fe_reused\"] = fe_stats.get(\"reused\", 0)\n",
    "    df[\"fe_generated\"] = fe_stats.get(\"generated\", 0)\n",
    "\n",
    "    # free big buffers early\n",
    "    del X_scaled, X_tensor\n",
    "    gc.collect()\n",
    "\n",
    "    logger(\n",
    "        f\"[{file_name}] iso={int(df['is_anomaly'].sum())} | \"\n",
    "        f\"ae={int(df['ae_is_anomaly'].sum())} | \"\n",
    "        f\"lof={int(df['lof_is_anomaly'].sum())} | \"\n",
    "        f\"lstm={int(df['lstm_is_anomaly'].fillna(0).sum())} | \"\n",
    "        f\"hyb={int(df['hybrid_is_anomaly'].sum())} | \"\n",
    "        f\"vote3+={int(df['vote_3plus'].sum())}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Dict):\n",
    "    logger = print\n",
    "\n",
    "    # A) residuals (optional)\n",
    "    if cfg[\"residuals\"][\"enabled\"]:\n",
    "        logger(\"ðŸ”§ Creating residuals...\")\n",
    "        create_residuals_for_folder(\n",
    "            in_folder=cfg[\"io\"][\"input_folder\"],\n",
    "            out_folder=cfg[\"io\"][\"residual_folder\"],\n",
    "            demand_token=cfg[\"residuals\"][\"demand_token\"],\n",
    "            measured_token=cfg[\"residuals\"][\"measured_token\"],\n",
    "            residual_token=cfg[\"residuals\"][\"residual_token\"],\n",
    "            skip_if_exists=True,\n",
    "            suffix=cfg[\"residuals\"][\"suffix\"],\n",
    "            logger=logger,\n",
    "        )\n",
    "        data_folder = cfg[\"io\"][\"residual_folder\"]\n",
    "    else:\n",
    "        data_folder = cfg[\"io\"][\"input_folder\"]\n",
    "\n",
    "    # B) per-file\n",
    "    all_dfs = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            out = process_file(os.path.join(data_folder, file), cfg, logger=logger)\n",
    "            if out is not None:\n",
    "                all_dfs.append(out)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger(\"âŒ No files processed.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    ensure_dir(cfg[\"io\"][\"output_folder\"])\n",
    "\n",
    "    combined_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"combined_anomaly_results.csv\")\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "\n",
    "    # Summary (counts)\n",
    "    cols = [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"]\n",
    "    cols = [c for c in cols if c in combined.columns]\n",
    "    summary = combined.groupby(\"source_file\")[cols].sum()\n",
    "    summary[\"total_anomalies\"] = summary.sum(axis=1)\n",
    "    summary_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_summary.csv\")\n",
    "    summary.to_csv(summary_path)\n",
    "\n",
    "    logger(f\"âœ… Saved row-level: {combined_path}\")\n",
    "    logger(f\"âœ… Saved summary:   {summary_path}\")\n",
    "\n",
    "    # C) Counts plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_cols = [c for c in [\"is_anomaly\",\"ae_is_anomaly\",\"lof_is_anomaly\",\"lstm_is_anomaly\",\"hybrid_is_anomaly\",\"vote_3plus\"] if c in summary.columns]\n",
    "    summary[plot_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "    plt.title(\"Anomalies per Model per File\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_plot.png\")\n",
    "    plt.savefig(bar_path); plt.close()\n",
    "    logger(f\"ðŸ–¼ï¸ Saved: {bar_path}\")\n",
    "\n",
    "    # C2) Rate plot (% rows)\n",
    "    if cfg.get(\"plots\", {}).get(\"emit_rate_plot\", True):\n",
    "        sizes = combined.groupby(\"source_file\").size().rename(\"n_rows\")\n",
    "        summary_rates = summary.div(sizes, axis=0) * 100.0\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        rate_cols = [c for c in plot_cols if c in summary_rates.columns]\n",
    "        summary_rates[rate_cols].plot(kind=\"bar\", figsize=(12, 6))\n",
    "        plt.title(\"Anomaly RATE per Model per File (%)\")\n",
    "        plt.ylabel(\"Percent of rows (%)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        rate_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"model_comparison_rate_plot.png\")\n",
    "        plt.savefig(rate_path); plt.close()\n",
    "        logger(f\"ðŸ–¼ï¸ Saved: {rate_path}\")\n",
    "\n",
    "    # D) Selection + episodes + reasons (HYBRID drives by default)\n",
    "    selection_rule = cfg.get(\"selection\", {}).get(\"rule\", \"hybrid\")\n",
    "    selected_rows = extract_selected_rows(combined, rule=selection_rule)\n",
    "    out_sel_dir = os.path.join(cfg[\"io\"][\"output_folder\"], \"selected_outputs\")\n",
    "    ensure_dir(out_sel_dir)\n",
    "    selected_rows_path = os.path.join(out_sel_dir, f\"selected_anomalies_rows_{selection_rule}.csv\")\n",
    "    selected_rows.to_csv(selected_rows_path, index=False)\n",
    "\n",
    "    episodes = summarize_episodes(selected_rows, min_gap=cfg[\"voting\"][\"min_gap\"])\n",
    "    episodes_path = os.path.join(out_sel_dir, f\"selected_anomaly_episodes_{selection_rule}.csv\")\n",
    "    episodes.to_csv(episodes_path, index=False)\n",
    "\n",
    "    episodes_with_reasons = attach_episode_reasons(combined, episodes, top_k=1)\n",
    "    episodes_with_reasons = enrich_hardware_mapping(episodes_with_reasons)\n",
    "    episodes_scored = score_episodes(combined, episodes_with_reasons, cfg)\n",
    "\n",
    "    episodes_reason_path = os.path.join(out_sel_dir, f\"selected_anomaly_episodes_with_reasons_{selection_rule}.csv\")\n",
    "    episodes_scored_path = os.path.join(out_sel_dir, f\"selected_anomaly_episodes_with_reasons_and_scores_{selection_rule}.csv\")\n",
    "    episodes_with_reasons.to_csv(episodes_reason_path, index=False)\n",
    "    episodes_scored.to_csv(episodes_scored_path, index=False)\n",
    "    logger(f\"âœ… Saved episodes+reason: {episodes_reason_path}\")\n",
    "    logger(f\"âœ… Saved episodes+scores: {episodes_scored_path}\")\n",
    "\n",
    "    print(\"\\nEPISODES PER FILE (after selection & scoring):\")\n",
    "    if not episodes_scored.empty:\n",
    "        print(episodes_scored.groupby(\"source_file\").size().to_string())\n",
    "    else:\n",
    "        print(\"No episodes found under current selection rule.\")\n",
    "\n",
    "    # E) Per-file plots with selected overlays (optional)\n",
    "    if cfg[\"plots\"][\"enabled\"]:\n",
    "        _ = plot_all_files(\n",
    "            combined_df=combined,\n",
    "            out_dir=out_sel_dir,\n",
    "            rule=selection_rule,\n",
    "            min_gap=cfg[\"voting\"][\"min_gap\"],\n",
    "            max_files=cfg[\"plots\"][\"max_files\"],\n",
    "        )\n",
    "\n",
    "    # F) Sensor table + clustering visuals (use selected rows)\n",
    "    sensor_df = build_sensor_table(combined, selected_rows, episodes_with_reasons=episodes_with_reasons)\n",
    "    sensor_df_path = os.path.join(out_sel_dir, \"sensor_table.csv\")\n",
    "    sensor_df.to_csv(sensor_df_path, index=False)\n",
    "    logger(f\"âœ… Saved sensor table: {sensor_df_path}\")\n",
    "\n",
    "    clustered, Z2, centers2 = cluster_sensors(sensor_df, n_clusters=3, random_state=42)\n",
    "    _ = plot_sensor_clusters_scatter(clustered, Z2, centers2, out_dir=out_sel_dir)\n",
    "    _ = plot_sensor_heatmap(sensor_df, out_dir=out_sel_dir)\n",
    "    _ = plot_sensor_bar_top(sensor_df, out_dir=out_sel_dir, metric=\"episodes_as_primary\", top_n=15)\n",
    "\n",
    "    # G) PDF report\n",
    "    if cfg.get(\"report\", {}).get(\"enabled\", True):\n",
    "        pdf_path = os.path.join(cfg[\"io\"][\"output_folder\"], \"ops_report.pdf\")\n",
    "        build_ops_report(\n",
    "            combined=combined,\n",
    "            summary=summary,\n",
    "            sensor_df=sensor_df,\n",
    "            episodes_scored=episodes_scored,\n",
    "            cfg=cfg,\n",
    "            out_pdf_path=pdf_path\n",
    "        )\n",
    "        logger(f\"ðŸ“„ Ops report saved: {pdf_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Entrypoint\n",
    "# =========================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Anomaly Detection Product\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to config JSON\")\n",
    "    args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "    cfg = load_config_from_path_or_default(args.config)\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670cfc1-3088-4731-9f5a-1771a20d65b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
