
Motion-Simulator Anomaly Detection — Hybrid Fusion Pipeline
==========================================================

Overview
--------
This repository is to support the MSc project for implementation of a robust,
residual-first anomaly-detection pipeline for motion-simulator logs.
It combines Isolation Forest (IF), Local Outlier Factor (LOF), a Dense
Autoencoder (AE), and an LSTM Autoencoder (LSTM-AE). Model scores are
normalized and fused into a hybrid score; rows are selected, merged
into episodes with context, lightly attributed (lag / saturation /
drift / vibration), and summarized into an Ops Report (PDF).

Repository Layout
-----------------
Anomaly_detection/
├─ Config.json                      # Runtime configuration (paths, models, thresholds)
├─ anomaly_detection.ipynb          # End-to-end pipeline (run this)
├─ anomaly_detection_YYYYMMDD_HHMMSS.txt
│                                   # Step log produced by the last run (timestamps)
└─ ops_report.pdf                   # Latest Ops Report generated by the notebook

What each file is for
---------------------
• Config.json
  - Central configuration the notebook reads on startup.
  - You can change input/output folders, sampling rate, model weights,
    thresholds, downsampling, and report limits without editing code.

• anomaly_detection.ipynb
  - The full pipeline: residual creation → feature engineering →
    shared scaling → IF / AE / LOF / LSTM-AE → hybrid fusion →
    selection → episodes → attribution → Ops Report.
  - Also saves row-level CSVs, per-file summaries, and plotting assets.

• anomaly_detection_YYYYMMDD_HHMMSS.txt
  - A concise “step logger” text file created under Anomaly_detection/
    during each run. It records what completed (and when) for traceability.

• ops_report.pdf
  - A multi-page PDF summarizing results (counts, RATE %, hybrid score
    histogram, top sensors & heatmap, and episode overlays with reasons).

Quick Start
-----------
1) Create a Python environment (Python 3.10+ recommended) and install deps:

   pip install numpy pandas matplotlib scikit-learn torch torchvision torchaudio

   (If you plan to export to a script or add extras, also consider:
    'seaborn', though the notebook uses Matplotlib only.)

2) Place your CSV logs in the folder referenced by Config.json
   (see the "io.input_folder" key). If you use Demand/Measured pairs,
   the notebook can auto-create residuals.

3) Open and run: Anomaly_detection/anomaly_detection.ipynb
   - The notebook will try to read ./Config.json by default.
   - If Config.json is missing or malformed, the pipeline falls back
     to a safe in-memory default configuration.

4) Outputs will appear under the configured output folder (see Config.json).
   Typical subfolders created on the first run:
   - outputs/                          (top-level)
     ├─ logs/                          (step logs)
     ├─ selected_outputs/
     │   ├─ selected_anomalies_rows_*.csv
     │   ├─ selected_anomaly_episodes_*.csv
     │   ├─ selected_anomaly_episodes_with_reasons_*.csv
     │   ├─ selected_anomaly_episodes_with_reasons_and_scores_*.csv
     │   └─ per-file overlay images (if enabled)
     ├─ model_comparison_summary.csv
     ├─ model_comparison_plot.png
     ├─ model_comparison_rate_plot.png
     └─ ops_report.pdf                 (final report, also copied to repo root)

Configuration (Config.json)
---------------------------
Key sections you’ll likely edit:

"io":
  - "input_folder":  path to your (raw or residual) CSVs
  - "residual_folder": where residual CSVs are saved if enabled
  - "output_folder":  where results/plots/report are written

"residuals":
  - "enabled": true|false  (create residuals as Demand - Measured)
  - tokens: Demand/Measured/Residual column name parts
  - "suffix": "_residual" (appended to filenames)

"features":
  - "window": rolling mean/std window (e.g., 5)
  - "max_features": guardrail to avoid feature blow-up

"threshold":
  - "k": MAD k-value for per-model thresholds (right-tail)

"ae" / "lstm" / "lof":
  - Model-specific knobs (epochs, hidden_dim, neighbors, downsample, etc.)

"hybrid":
  - "enabled": true|false
  - "method": "robust_z" (or "percentile01") for score normalization
  - "min_components": min number of model scores required
  - "weights": { "iso_score": 0.20, "lof_score": 0.20, "ae_error": 0.30, "lstm_error": 0.30 }

"hybrid_threshold":
  - "mode": "quantile" (default) or "mad"
  - "quantile": 0.99 → sets an alert budget per file
  - "k": fallback MAD k-value

"selection":
  - "rule": "hybrid" (others available: "vote_3plus", etc.)

"signals":
  - "sample_rate_hz": e.g., 100.0
  - column tokens for Demand/Measured/Residual

"report":
  - "top_n_per_file", "max_pages", and "pad_points" for episode figures

Typical Workflow
----------------
1) (Optional) Residual creation
   - If enabled, the notebook generates new CSVs with *_residual columns
     (Demand - Measured) for every Demand/Measured pair found.

2) Modeling
   - Shared StandardScaler over residual-first features.
   - Isolation Forest, Dense AE, LOF (exact or approximate), LSTM-AE
     (with adaptive downsampling) produce raw scores + per-model flags.

3) Fusion & Selection
   - Scores normalized and combined into a hybrid score.
   - A tail-quantile threshold (or MAD fallback) selects rows.
   - Rows merged into episodes with context padding.

4) Attribution
   - Primary residual per episode identified.
   - Lightweight reasons computed (lag, saturation, drift, vibration).
   - Simple mapping to hardware classes (e.g., Actuator/LoadCell).

5) Report & Artifacts
   - Row-level results CSV, per-file counts and RATE (%).
   - Sensor table + optional heatmap and clusters.
   - Ops Report PDF with counts, RATE, hybrid histogram, top sensors,
     and episode overlays including reasons.

Interpreting the Ops Report
---------------------------
• Page 1: Anomalies per model per file (counts)
• Page 2: Anomaly RATE (%) per model per file (length-normalized)
• Page 3: Hybrid score distribution + selected threshold (dashed line)
• Next pages: Top sensors / heatmap (if enabled) and per-episode overlays
  showing Demand/Measured/Residual with context and brief attribution.

Troubleshooting
---------------
• No residuals detected:
  - Ensure your column names contain the configured tokens ("Demand", "Measured").
• Empty hybrid score / no selections:
  - Check that at least two component scores are present per row and that
    thresholds aren’t overly strict. Lower the hybrid quantile temporarily.
• Memory issues on very large logs:
  - The notebook uses batching, subsampling, and downcasting. If needed,
    increase downsample for LSTM, reduce AE batch size, or limit LOF subset.

Reproducibility
---------------
- Random seeds are set where supported (e.g., Isolation Forest, KMeans) to
  reduce run-to-run variance. For exact reproducibility, fix library versions.



Contact
-------
Author: Sugal Vackpathy Saravanan
Questions/Issues: please open a GitHub issue in this repository.
